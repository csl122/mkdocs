# Machine Learning for Imaging

tags:: IC, Course, ML, Uni-S10
alias:: MLI
Ratio: 7:3
Time: æ˜ŸæœŸäº” 14:00 - 16:00

- ## Notes
	- Week2 intro
	  collapsed:: true
		- Lecture
		- Tutorial
		  collapsed:: true
			- åŸºæœ¬çš„æœºå™¨å­¦ä¹ ç±», ä»¥regresionä¸ºä¾‹
			- ```python
			  
			  class LogisticRegression:
			      def __init__(self, lr=0.05, num_iter=1000, add_bias=True, verbose=True):
			          self.lr = lr
			          self.verbose = verbose
			          self.num_iter = num_iter # å¤šå°‘ä¸ªepoch
			          self.add_bias = add_bias # ç”¨äºåŠ å…¥bias
			          self.weight = np.random.normal(0, 0.01, 50) # å¦‚æœçŸ¥é“å¤šå°‘å‚æ•°çš„è¯, å¯ä»¥ç›´æ¥åˆå§‹åŒ–, 
			          # ä¹Ÿå¯ä»¥fité‡Œé¢æ ¹æ®å®é™…featureæ•°é‡å†³å®š
			      
			      def __add_bias(self, X):
			          bias = np.ones((X.shape[0], 1)) # (10000, 1) å¤šåŠ äº†ä¸€ä¸ªbias æ”¾åœ¨åŸæœ¬çš„featureä¹‹å, ç”¨äºæ±‚å’Œçš„æ—¶å€™å¤šä¸€ä¸ªbias
			          return np.concatenate((bias, X), axis=1) # å¤šåŠ ä¸€ä¸ªåˆ—ç»´åº¦
			      
			  
			  	# æŸå¤±å‡½æ•°
			      def __loss(self, h, y):
			          ''' computes loss values '''
			          y = np.array(y,dtype=float)
			          ############################################################################
			          # Q: compute the loss 
			          ############################################################################
			          return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean() # äºŒåˆ†ç±»é—®é¢˜çš„cross-entropy loss, ä¸¤ä¸ªåˆ†ç±»åˆ†åˆ«çš„cross entropyç›¸åŠ , å› ä¸ºæ˜¯äºŒåˆ†ç±», æ‰€ä»¥æ˜¯y, 1-y
			        # æ€»å’Œä¸º1çš„ç±»æ¦‚ç‡, ç”¨ä¸Šcross entropy, å¯¹æƒé‡æ±‚å¯¼çš„ç»“æœå°±æ˜¯1/N (y^ - y)
			  
			      
			      def fit(self, X, y):
			          ''' 
			          Optimise our model using gradient descent
			          Arguments:
			              X input features
			              y labels from training data
			              
			          '''
			          if self.add_bias:
			              X = self.__add_bias(X)
			          
			          ############################################################################
			          # Q: initialise weights randomly with normal distribution N(0,0.01)
			          ############################################################################
			          self.theta = np.random.normal(0.0,0.01,X.shape[1])
			          
			          for i in range(self.num_iter):
			              ############################################################################
			              # Q: forward propagation
			              ############################################################################
			              z = X.dot(self.theta) # ä¹˜ä¸Šäº†æƒé‡åçš„ç»“æœï¼Œ æ¯ä¸ªfeature éƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„æƒé‡ï¼Œä¹˜èµ·æ¥ç›¸åŠ å°±æ˜¯æŸä¸ªsampleçš„å€¼
			              h = 1.0 / (1.0 + np.exp(-z)) # activation, å¾—åˆ°é¢„æµ‹æ¦‚ç‡
			              ############################################################################
			              # Q: backward propagation
			              ############################################################################
			              # (h - y) / y.size æ˜¯ Losså¯¹zçš„æ±‚å¯¼; Xåˆ™æ˜¯zå¯¹weightçš„æ±‚å¯¼, å› ä¸ºz
			              gradient = np.dot(X.T, (h - y)) / y.size # (785,) 11774ä¸ªsampleï¼Œ æ¯ä¸ªç»´åº¦éƒ½æ˜¯æ‰€æœ‰sampleæ¢¯åº¦çš„å’Œï¼Œ æœ€åé™¤ä»¥äº†sample é‡
			              # update parameters æ¢¯åº¦ä¸‹é™
			              self.theta -= self.lr * gradient
			              ############################################################################
			              # Q: print loss
			              ############################################################################
			              if(self.verbose == True and i % 50 == 0): 
			                  h = 1.0 / (1.0 + np.exp(-X.dot(self.theta)))
			                  print('loss: {} \t'.format(self.__loss(h, y)))
			      
			      def predict_probs(self,X):
			          ''' returns output probabilities
			          '''
			          ############################################################################
			          # Q: forward propagation
			          ############################################################################
			          if self.add_bias:
			              X = self.__add_bias(X)
			          z = X.dot(self.theta)
			          return 1.0 / (1.0 + np.exp(-z))
			  
			      def predict(self, X, threshold=0.5):
			          ''' returns output classes
			          '''
			          return self.predict_probs(X) >= threshold
			  
			  model = LogisticRegression(lr=1e-2, num_iter=1000)
			  # shuffle data
			  shuffle_index = np.random.permutation(len(selected_labels))
			  selected_data, selected_labels = selected_data[shuffle_index], selected_labels[shuffle_index]
			  model.fit(selected_train_data, selected_train_labels)
			  train_preds = model.predict(selected_train_data)
			  logistic_train_acc = 100.0 * (train_preds == selected_train_labels).mean()
			  ```
	- Week3 Classification
	  id:: 63dacd04-711a-4465-a450-60bf0076ed43
	  collapsed:: true
		- Lecture
			- feature extraction
			  collapsed:: true
				- pipeline
				  collapsed:: true
					- images->features->prediction
				- feature extraction and descriptors
				  collapsed:: true
					- â€“ Intensities
					  collapsed:: true
						- ![image.png](../assets/image_1674664156563_0.png)
					- â€“ Gradient
					  collapsed:: true
						- Gradients: Invariant to absolute illumination
					- â€“ Histogram
						- patch-levelå’Œimage-leveléƒ½å¯ä»¥ç”¨hist
					- â€“ SIFT
					  collapsed:: true
						- ![image.png](../assets/image_1674664387996_0.png)
					- â€“ HoG Histogram of gradients
					  collapsed:: true
						- ![image.png](../assets/image_1674664443665_0.png)
						- ![image.png](../assets/image_1674664461115_0.png)
						- ![image.png](../assets/image_1674664760075_0.png)
					- â€“ SURF
					  collapsed:: true
						- ![image.png](../assets/image_1674664774363_0.png)
						- ![image.png](../assets/image_1674664807093_0.png)
					- â€“ BRIEF
					  collapsed:: true
						- ![image.png](../assets/image_1674664845583_0.png)
						-
					- â€“ LBP
					  collapsed:: true
						- ![image.png](../assets/image_1674664927209_0.png)
					- â€“ Haar
					  collapsed:: true
						- ![image.png](../assets/image_1674664941210_0.png)
						- ![image.png](../assets/image_1674664975489_0.png)
			- image classification: Ensemble classifiers
			  collapsed:: true
				- classification models
				  collapsed:: true
					- â€“ Logistic regression D
					  â€“ NaÃ¯ve Bayes G
					  â€“ K-nearest neighbors (KNN) G
					  â€“ Support vector machines (SVM) D
					  â€“ Boosting 
					  â€“ Decision/Random forests D
					  â€“ Neural networks D
				- bias and variance
				  collapsed:: true
					- Bias error
					  â€“ how much, on average, **predicted values are different from the actual value**.
					  â€“ high bias error means we have an under-performing model, which misses important trends.
					  Accurateå’Œinaccurate
					- Variance error
					  â€“ how much **predictions made from different samples vary from one other**.
					  â€“ high variance error means the model will over-fit and perform badly on any observation beyond training.
					- variable å’Œconsistent
					- ![image.png](../assets/image_1674665312313_0.png)
					-
				- Ensemble learning
				  collapsed:: true
					- Aggregate the predictions of a group of predictors (either classifiers or regressors) èšåˆä¸€å †é¢„æµ‹çš„ç»“æœ, ä¸€å †é¢„æµ‹å™¨å°±æ˜¯ä¸€ä¸ªensemble
					- A learning algorithm which uses multiple models, such as classifiers or experts, is called Ensemble Learning (so called meta-algorithms) ä¸€ä¸ªä½¿ç”¨äº†å¤šä¸ªåˆ†ç±»å™¨çš„ç®—æ³•å°±å«åšensemble learning æˆ–è€…meta ç®—æ³•
					- Types of ensemble learning
					  collapsed:: true
						- â€¢ Homogenous:
						  â€“ Combine predictions made from models built from the same ML class
						  åŒè´¨çš„, ç”¨äº†åŒä¸€ç§æ¨¡å‹
						  æ¯”å¦‚å¾ˆå¤šweak learner, ç”¨äº†ä¸åŒsubset of data
						- â€¢ Heterogenous:
						  â€“ Combine predictions made from models built from different ML classes
						  ä¸åŒçš„ç§ç±»çš„æ¨¡å‹
						- â€¢ Sequential â€“ base (ML) learners are added one at a time; mislabelled
						  examples are upweighted each time
						  â€“ Exploits the dependence between base learners â€“ thus learning a complementary set of predictors that reduce the bias and increase the accuracy.
						  åºåˆ—åŒ–çš„, å‡å°bias, å› ä¸ºä¸€å±‚ä¸€å±‚ç­›é€‰, ä¼šéå¸¸ç²¾ç¡®
						- â€¢ Parallel â€“ many independent base learners are trained
						  simultaneously and then combined
						  â€“ Combines prediction learnt from multiple models run independently averaging away impact of isolated errors - thus reduces variance of the prediction.
						  å¹³è¡Œçš„, å‡å°æ–¹å·®, å‡å°å­¤ç«‹çš„é”™è¯¯çš„å½±å“
					- Decision Stump
					  collapsed:: true
						- ![image.png](../assets/image_1674666376082_0.png)
					- Ensemble methods
						- â€¢ Voting
						- â€¢ Bagging (Bootstrap Aggregation)
						  collapsed:: true
							- å¹³è¡Œçš„ä¸€ç§ensemble learning, é€šè¿‡ä¸€å †weak learnerå¹³è¡Œé¢„æµ‹, æ¥å‡å°variance, å› ä¸º$var(x^-) = var(x)/n$
							- Bootstrapping
							  collapsed:: true
								- 1. Take the original dataset E with N training samples
								  2. Create T copies  by sampling with replacement
								      â€“ Each copy will be different since some examples maybe repeated while others will not be sampled at all
								  3. Train a separate weak learner on each Bootstrap sample
							- Aggregating results
							  collapsed:: true
								- ![image.png](../assets/image_1674666835882_0.png)
							- Out-of-Bag (OOB) error
								- å¦‚ä½•è¯„ä»·errorå‘¢, æˆ‘ä»¬ç”¨left-out-setæ¥ä½œä¸ºvalidation set
						- â€¢ Boosting
						  collapsed:: true
							- åºåˆ—åŒ–çš„, ä¸€ä¸ªlearneræ¥ç€ä¸€ä¸ªlearner, å¤§å¤§é™ä½bias
							- Rather than building independent weak learners in parallel and aggregating at end:
							  â€“ build weak learners in serial
							  â€“ but adaptively reweight training data prior to training each new weak learner in order to give a higher weight to previously misclassified examples
							- ä¸æ–­é‡æ–°weight training data æ¥ç»™æ–°çš„weak learner, ç»™misclassified æ›´é«˜æƒé‡
							- ![image.png](../assets/image_1674668364470_0.png)
							- Adaboost - ä¸€ç§è‡ªé€‚åº”çš„boosting, adaptive
								- ![image.png](../assets/image_1674668416628_0.png)
						- â€¢ Random Forests
						  collapsed:: true
							- Single Decision Trees are prone to overfitting, but robustness can be significantly increased by combining trees in ensembles
							- Use decision trees for homogenous ensemble learning
							- Random forests form an ensemble of uncorrelated classifiers by
							  exploiting random subsampling of the 
							  â€“ training data used to build each tree
							  â€“ set of features that are used for selection of the splits 
							  â€“ set of feature values that are used for splitting
							- ![image.png](../assets/image_1674686267545_0.png)
							-
			- NN
			  collapsed:: true
				- Single-layer perceptron
				  collapsed:: true
					- ![image.png](../assets/image_1674686334440_0.png)
				- Neural networks (multilayer perceptron)
				  collapsed:: true
					- ![image.png](../assets/image_1674686588881_0.png)
				- Neural networks with K classes
				  collapsed:: true
					- one-hot encoding
						- ![image.png](../assets/image_1674686671846_0.png)
					- softmax
						- ![image.png](../assets/image_1674686779670_0.png)
					- cross entropy: Distance between probability distributions
						- ![image.png](../assets/image_1674686805554_0.png)
						- softmaxå’Œäº¤å‰ç†µçš„æ¢¯åº¦å¾ˆç®€å•, å°±æ˜¯1/N * (y^ - y)
					- back propogation
						- ![image.png](../assets/image_1674687354916_0.png)
						- å¾€å›èµ°é‡åˆ°å²”è·¯, æŠŠå½“å‰çš„æ¢¯åº¦åˆ†é…å‡ºå»
						- å¾€å›èµ°é‡åˆ°æ±‡åˆç‚¹, æ±‡åˆç‚¹çš„æ¢¯åº¦æ˜¯åˆ†å‰å‡ºå»çš„æ¢¯åº¦å’Œ
						- ![image.png](../assets/image_1674687623328_0.png)
			- Activation and optimisation
			  collapsed:: true
				- ![image.png](../assets/image_1674687714289_0.png)
				- Optimizing neural networks: Stochastic gradient descent
					- ![image.png](../assets/image_1674687748304_0.png)
					- Large batches provide a more accurate estimate of the gradient but with less than linear returns.
					- è€ƒè™‘memory, é™åˆ¶äº†batchæœ€å¤§size
					- Small batch sizes can have the effect of regularization (more about regularization later) as it adds noise to the learning process. (The generalisation ability is often best with a batch size of 1, but small batch sizes require small learning rates and can lead to slow convergence).
					- å°batchä¼šç»™æœ€ä½³çš„æ­£åˆ™åŒ–, æ›´å¥½çš„æ™®é€‚æ€§, ä½†æ˜¯éœ€è¦æ›´å°çš„å­¦ä¹ ç‡, å­¦ä¹ ä¼šå¾ˆæ…¢
				- gradient and learning rate
					- ![image.png](../assets/image_1674687940771_0.png)
			- tips and tricks
			  collapsed:: true
				- data augmentation
					- ![image.png](../assets/image_1674687996227_0.png)
				- Under- and overfitting
					- underfitting
						- adding more neurons, adding more layers
					- overfitting
						- adding more regularization
				- Regularisation
					- Early stopping: Interrupt training when its performance on the validation set starts dropping
					- ![image.png](../assets/image_1674688096633_0.png)
					- Max-norm
						- ![image.png](../assets/image_1674688127437_0.png)
					- Dropout
						- At every training step, every neuron (input or hidden) has a probability p of being temporarily â€œdropped outâ€:
				- Weight initialisation
					- ![image.png](../assets/image_1674688253094_0.png)
				- Normalisation
					- Standardization æ ‡å‡†åŒ–
						- å‡å»å‡å€¼, é™¤ä»¥æ ‡å‡†å·® å¾—åˆ°ä¸€ä¸ªå‡å€¼ä¸ºé›¶, æ ‡å‡†å·®ä¸º1çš„åˆ†å¸ƒ
					- å½’ä¸€åŒ–
						- (X-min)/(max-min), å¾—åˆ°ä¸€ä¸ªæœ€å¤§å€¼å’Œæœ€å°å€¼ä¸º1å’Œ-1çš„ç¼©æ”¾åˆ†å¸ƒ
					- Batch normalisation
					  collapsed:: true
						- ![09AF0F45-7CAD-465D-A6AC-65F3F0BEBBDB.jpeg](../assets/09AF0F45-7CAD-465D-A6AC-65F3F0BEBBDB_1674658218903_0.jpeg)
						- æ¯ä¸€ä¸ªfeatuerç»´åº¦, å¯¹äºä¸€ä¸ªbatché‡Œçš„æ‰€æœ‰sampleè¿›è¡Œæ ‡å‡†åŒ–
						- åœ¨NLPä¸­ä¼šå—åˆ¶äºæ¯ä¸ªsampleå¥å­é•¿åº¦ä¸ä¸€æ ·, é«˜åº¦å°±ä¸ä¸€æ ·, ä¼šunstable
						- ![image.png](../assets/image_1674688305892_0.png)
						- ![image.png](../assets/image_1674688422555_0.png)
					- Layer normalisation
						- å¯¹æ¯ä¸€ä¸ªsampleçš„æ‰€æœ‰featureåšæ ‡å‡†åŒ–
						- NLPä¸­ä¸€ä¸ªbatchæ˜¯æœ‰å¾ˆå¤šä¸åŒé•¿åº¦çš„å¥å­ç»„æˆçš„, æ¯ä¸ªsampleå¯ä»¥è®¤ä¸ºæ˜¯å¤šä¸ªè¯è¯­çš„ç»„åˆ, è¯è¯­çš„æ•°é‡ä¸ä¸€å®š
						- NLPä¸­å°±æ˜¯å¯¹æŸä¸ªè¯è¯­çš„æ•´ä¸ªvectoråšæ ‡å‡†åŒ–
			- CNN
			  collapsed:: true
				- ![image.png](../assets/image_1674688501664_0.png)
				- è®¡ç®—
					- Assuming padding of size p, no stride
						- $o_j=i_j-k_j+2p+1$
						- ä¾‹å¦‚åŸå›¾æ˜¯5, kernalæ˜¯4, paddingæ˜¯2, outputå°±æ˜¯5-4+4+1 = 6
					- Assuming padding of size p, stride $s_j$
						- $o_j=[(i_j-k_j+2p)/s_j]+1$
						- ä¾‹å¦‚åŸå›¾æ˜¯5, kernalæ˜¯3, paddingæ˜¯1, strideæ˜¯2, outputå°±æ˜¯ (5 âˆ’ 3 + 2)/2 + 1 = 3
				- è¾“å…¥å›¾æœ‰ä¸‰ä¸ªchannel, è¿™ä¸ªæ—¶å€™æˆ‘ä»¬ä¸€ä¸ªfilterä¹Ÿå¾—æœ‰ä¸‰ä¸ªchannelå’Œä»–ä»¬ä¸€ä¸€å¯¹åº”, å¦‚æœæˆ‘ä»¬æœ‰å››ä¸ªfilter, é‚£ä¹ˆæœ€åç”Ÿæˆçš„è¾“å‡ºä¹Ÿæœ‰å››ä¸ª
				- pooling:
					- â€¢ Reduce size (memory) of deeper layers
					  â€¢ Invariance to small translations
					  â€¢ Contraction forces network to learn high-level features
				- upsampling
					- æŠŠåŸå›¾å…ˆå¡«ä¸Š0, å†ç”¨å·ç§¯æ‰©å¤§
					- ![image.png](../assets/image_1674689023013_0.png)
		- Tutorial
		  collapsed:: true
			- Haar features
				- quickly computed by integral images
				- Computational complexity of computing haar features from integral images is constant
			- End-to-end learning
				- model èƒ½å¤Ÿç›´æ¥ä»å›¾åˆ°ç»“æœ
				- ä¸é€‚åˆå°æ•°æ®é›†, å› ä¸ºéœ€è¦å¤§æ•°æ®æ¥generalise model
				- ä¸éœ€è¦hand crafted features
				- not efficient in terms of model parameters, å› ä¸ºæˆ‘ä»¬éœ€è¦è¶³å¤Ÿå¤æ‚çš„æ¨¡å‹
				- models are not human readable
			- Regularisation
				- L1, L2
					-
				- dropout
				- small batch size + SGD
				- data augmentation
	- Week4 Image Segmentation
	  collapsed:: true
		- Lecture
			- Semantic Segmentation
				- In semantic segmentation, a segmented region is also assigned a
				  semantic meaning
			- Application
			  collapsed:: true
				- conducting quantitative analyses, e.g. measuring the volume of the
				  ventricular cavity.
				- determining the precise location and extent of an organ or a certain type of tissue, e.g. a tumour, for treatment such as radiation therapy.
				- creating 3D models used for simulation, e.g. generating a model of an abdominal aortic aneurysm for simulating stress/strain distributions.
			- challenges
			  collapsed:: true
				- noise
				- partial volume effects
				  collapsed:: true
					- coarse sampling, é‡‡æ ·å¯¼è‡´çš„ä¸è¶³
					- ![image.png](../assets/image_1675439370714_0.png)
				- intensity inhomogeneities, anisotropic resolution
				  collapsed:: true
					- ä¸åŒè´¨çš„intensityå’Œä¸åŒæ–¹å‘çš„resolutionçš„ä¸ä¸€è‡´
					- ![image.png](../assets/image_1675439440151_0.png)
					- ![image.png](../assets/image_1675439452285_0.png)
				- imaging artifacts
					- ä¸€äº›å¥‡æ€ªçš„ä¸œè¥¿å¯¼è‡´çš„æ€ªä¸œè¥¿
				- limited contrast
					- å›¾ç‰‡çš„å¯¹æ¯”åº¦ä¸å¤Ÿé«˜
				- morphological variability,
					- å½¢æ€å­¦ä¸Šå™¨å®˜éƒ½é•¿å¾—ä¸ä¸€æ ·, å…ˆéªŒçŸ¥è¯†å¾ˆéš¾åˆ©ç”¨å¥½
			- Evaluating Image Segmentation
				- ground truth
					- Reference or standard against a method can be compared, e.g. the optimal transformation, or a true segmentation boundary, ä¸ç°å®
					- å¯èƒ½äººä»¬ä¼šmake up phantoms
				- gold standard
					- Experts manually segment
					- ![image.png](../assets/image_1675439700146_0.png)
			- performance measures
				- ![image.png](../assets/image_1675439726743_0.png)
				- Confusion matrix #metrics
					- ![image.png](../assets/image_1675439766890_0.png)
				- Accuracy, Precision, Recall (sensitivity), Specificity #metrics
					- ![image.png](../assets/image_1675439825709_0.png)
					- F1 score #metrics
						- ![image.png](../assets/image_1675440141000_0.png)
					- Overlap
						- Jaccard Index(IoU)
						- ![image.png](../assets/image_1675440231460_0.png)
						- Dice's Coefficient (Sorensen Index) (DSC) (Dice Similarity Coefficient)
						  collapsed:: true
							- ![image.png](../assets/image_1675440410629_0.png)
							- ![image.png](../assets/image_1675441381926_0.png)
							- ![image.png](../assets/image_1675441399747_0.png)
					- Volume similarity
					  collapsed:: true
						- ![image.png](../assets/image_1675441496345_0.png)
					- Hausdorff distance
						- ![image.png](../assets/image_1675441591113_0.png)
						- Aä¸Šæ¯ä¸ªç‚¹æ‰¾åˆ°Bä¸Šç¦»è¿™ä¸ªç‚¹çš„æœ€çŸ­è·ç¦», è¿™äº›è·ç¦»ä¹‹ä¸­æœ€é•¿çš„å°±æ˜¯æˆ‘ä»¬æ±‚çš„è¿™ä¸ªè·ç¦»
					- Symmetric average surface distance
						- ![image.png](../assets/image_1675441634954_0.png)
						- Aåˆ°Bçš„æœ€çŸ­è·ç¦»çš„å¹³å‡æ•°
					-
				- Segmentation Algorithms & Techniques
					- â–ª Intensity-based segmentation
					  collapsed:: true
					  â–ª e.g., thresholding
						- ç”¨äºå¾ˆæ˜æ˜¾èƒ½åŒºåˆ†ä¸»ä½“å’ŒèƒŒæ™¯çš„æƒ…å†µ
						- UL Thresholding: Select a lower and upper threshold
						- Advantages
						  â–ª simple 
						  â–ª fast
						- Disadvantages
						  â–ªregions must be homogeneous and distinct
						  â–ªdifficulty in finding consistent thresholds across images 
						  â–ªleakages, isolated pixels and â€˜roughâ€™ boundaries likely
					- â–ª Region-based
					  collapsed:: true
					  â–ª e.g., region growing
						- Start from (user selected) seed point(s), and grow a region according to an intensity threshold, ä»ä¸€ä¸ªç‚¹å¼€å§‹ä¸‡ç‰©ç”Ÿé•¿
						- Advantages
						  â–ªrelatively fast
						  â–ªyields connected region (from a seed point)
						- Disadvantages
						  â–ªregions must be homogeneous â–ªleakages and â€˜roughâ€™ boundaries likely â–ªrequires (user) input for seed points
					- â–ª Graph-based segmentation
					  collapsed:: true
					  â–ª e.g., graph cuts
						- ![image.png](../assets/image_1675532483329_0.png)
						- ![image.png](../assets/image_1675532497544_0.png)
						- ![image.png](../assets/image_1675532523722_0.png)
						- Advantages
						  â–ª accurate
						  â–ªreasonably efficient, interactive
						- Disadvantages
						  â–ªsemi-automatic, requires user input 
						  â–ªdifficult to select tuning parameters
					- â–ª Active contours
					  â–ª e.g., level sets
					- â–ª Atlas-based segmentation
					  collapsed:: true
					  â–ª e.g., multi-atlas label propagation
						- åŸºäºå›¾è°±çš„å›¾åƒåˆ†å‰²
						- æ‰€è°“â€œåœ°å›¾é›†â€ï¼Œç›´è§‚æ¥è¯´ï¼ŒAtlas å°±æ˜¯äººå·¥æ ‡è®°å®Œå¤‡çš„æ•°æ®åº“ã€‚æ¯”å¦‚ BrainWeb çš„ Atlasï¼šåœ¨ä¸‰ç»´è„‘éƒ¨CTæ•°æ®ä¸­åŒ»ç”Ÿæ ‡æ³¨å®Œå¤‡çš„å„ç§è„‘éƒ¨ç»“æ„ï¼Œå¦‚ç°è´¨ã€ç™½è´¨ã€æµ·é©¬ç­‰ç­‰ç»“æ„ã€‚
						- å°† testing image å’Œå½“å‰ Atlas å†…çš„æ•°æ®è¿›è¡Œé…å‡†ï¼Œç„¶åç”¨ Label Propagation æ–¹æ³•å°† Atlas æ•°æ®çš„ Label é€šè¿‡ registration mapping corresponding å…³ç³»ä¼ é€’åˆ° testing image ä¸­ï¼Œä»è€Œå®Œæˆ testing image çš„åˆ†å‰²ä»»åŠ¡ã€‚
						- Segmentation using registration
						- ![image.png](../assets/image_1675533840911_0.png)
						- é€šè¿‡ä»åœ°å›¾é›†é‡Œäººå·¥æ ‡æ³¨çš„segmentationä¸æ ·æœ¬è¿›è¡Œæ˜ å°„, è¿™ä¸ªè¿‡ç¨‹å«åšregistration
						- Multi-Atlas Label Propagation
						- ![image.png](../assets/image_1675534069463_0.png)
						- Advantages
						  â–ªrobust and accurate (like ensembles) 
						  â–ªyields plausible segmentations
						  â–ªfully automatic
						- Disadvantages
						  â–ªcomputationally expensive
						  â–ªcannot deal well with abnormalities 
						  â–ªnot suitable for tumour segmentation
					- â–ª Learning-based segmentation
					  collapsed:: true
					  â–ª e.g., random forests, convolutional neural networks
						- ![image.png](../assets/image_1675534349869_0.png)
						- æå–å›¾åƒä¸­çš„patches, ç”¨æ ‘ä¸­çš„åˆ¤æ–­æ–¹æ³•æ¥åˆ¤æ–­
						- Advantages
						  â–ªensemble classifiers are robust and accurate 
						  â–ªcomputationally efficient
						  â–ªfully automatic
						  Disadvantages
						  â–ªshallow model, no hierarchical features 
						  â–ªno guarantees on connectedness
				- CNNs for Image Segmentation
				  collapsed:: true
					- Segmentation via Dense Classification
					- åˆ©ç”¨åˆ°äº†3Då·ç§¯
					- ![image.png](../assets/image_1675535246807_0.png)
					- ![image.png](../assets/image_1675535121849_0.png)
					- æŠ½å–ä¸€ä¸ªpatch, é€šè¿‡æ•´ä½“ä¿¡æ¯æ¥å­¦ä¹ ç¡®è®¤ä¸­å¤®åƒç´ çš„ç±»åˆ«
					- é€šè¿‡sliding window æ¥éå†å›¾ç‰‡, æ‰¾åˆ°æ¯ä¸ªç‚¹çš„ç±»åˆ«, éå¸¸çš„ä¸é«˜æ•ˆ
					- Fully Connected to Convolution: ç”¨convæ›¿ä»£fc
						- ![image.png](../assets/image_1675535590111_0.png)
						- é€šè¿‡å…±äº«æƒé‡, å¤§å¤§å‡å°‘äº†å‚æ•°é‡
						- ![image.png](../assets/image_1675535748846_0.png)
						- 1x1æ˜¯class label, ä½†æ˜¯9x9è¿˜æ˜¯feature map
				- Encoder-Decoder Networks
				  collapsed:: true
					- ![image.png](../assets/image_1675535864049_0.png)
				- U-Net
				  collapsed:: true
					- ![image.png](../assets/image_1675535884207_0.png)
					- é€šè¿‡åŸå›¾è¡¥å……åŸä¿¡æ¯, ä¸‹é‡‡æ ·å†ä¸Šé‡‡æ ·å­¦ä¹ å¦‚ä½•äº§ç”Ÿsegmentation map
				- Upsampling and transpose convolution
				  collapsed:: true
					- ![image.png](../assets/image_1675536138599_0.png)
					- ![image.png](../assets/image_1675536154098_0.png)
					- ![image.png](../assets/image_1675536226508_0.png)
				- Going Deeper
				  collapsed:: true
					- Just adding more layers is inefficient (too many parameters) 
					  â–ªIdea: Use only layers with small kernels
				- Multi-scale processing
				  collapsed:: true
					- How can we make the network to â€œseeâ€ more context
					  â–ªIdea: Add more pathways which process downsampled images
					- å¦‚ä½•è®©ç½‘ç»œçœ‹åˆ°æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯å‘¢, å¢åŠ pathway, ä¸¤æ¡è·¯ç»™ç¥ç»ç½‘ç»œèµ°, ä¸€ä¸ªé«˜åˆ†è¾¨ç‡çš„å¤§å›¾, ä¸€ä¸ªä½åˆ†è¾¨ç‡çš„å°å›¾, concatèµ·æ¥çš„æ—¶å€™å°±æœ‰äº†æ›´å¤§çš„è§†é‡, æ–°å¢çš„ä¿¡æ¯å¢å¼ºäº†localisation capability
					- ![image.png](../assets/image_1675536363791_0.png)
					- ![image.png](../assets/image_1675536435030_0.png)
					- ä¾‹å¦‚ä¸Šé¢çš„é«˜åˆ†è¾¨ç‡çš„è§£å‰–å­¦ä¿¡æ¯, å’Œä¸‹é¢çš„ä½ç½®ä¿¡æ¯
	- Week5 Image Registration, åæ ‡ç³», transformation, intensity-based image registration, NN for regi
	  collapsed:: true
		- Image Registration: å›¾åƒé…å‡†, å°±æ˜¯å°†ä¸åŒçš„ç…§ç‰‡alignèµ·æ¥, åŒ¹é…èµ·æ¥å¯¹åº”çš„ä½ç½®
			- Establish spatial correspondences between images
			- å…¶å®å°±æ˜¯æ‰¾åˆ°ä¸¤å¼ ç…§ç‰‡å¯¹åº”çš„ç‚¹, ç®—å¾—transformationçš„æ–¹å¼, è¿›è¡ŒåŒ¹é…å¯¹åº”, ä¾‹å¦‚ä»¥å‰å­¦è¿‡çš„image warping, å°±æ˜¯æ‰­æ›²å›¾ç‰‡è®©ä¸¤å¼ ç…§ç‰‡æ‹¼æ¥èµ·æ¥, é‚£ä¸ªæ—¶å€™ç”¨çš„æ˜¯ç‰¹å¾ç‚¹å¯¹åº”ç®—å¾—fundamental matrix, ç°åœ¨ç”¨çš„æ–¹æ³•å¯èƒ½å€¾å‘äºç”¨intensity å¯¹åº”, ç„¶åç®—ä¸ç›¸ä¼¼åº¦, æ…¢æ…¢è°ƒæ•´. ç›¸å¯¹äºçº¿æ€§çš„transformation, è¿™é‡Œè¿˜ç”¨åˆ°äº†deformation, ä¼šæœ‰æ›´é«˜æ— é™ç»´åº¦çš„degree of freedom
		- Coordinate system
		  collapsed:: true
			- ![image.png](../assets/image_1676133104628_0.png)
		- Transformation Models
		  collapsed:: true
			- ![image.png](../assets/image_1676133128197_0.png)
			- ![image.png](../assets/image_1676133167133_0.png)
			-
				- ![image.png](../assets/image_1676133183172_0.png)
			- Free-form Deformation
				- ![image.png](../assets/image_1676134427405_0.png)
		- Applications
		  collapsed:: true
			- Satellite Imaging, Point Correspondences, Panoramic Image Stitching, Optical Flow
			- ![image.png](../assets/image_1676134672345_0.png)
		- Intra-subject Registration
		  collapsed:: true
			- ![image.png](../assets/image_1676134707810_0.png)
			- åŒä¸€ä¸ªç‰©ä½“çš„registration, å¯¹åº”å¥½ä»¥åå°±èƒ½æ‰¾åˆ°ä¸ä¸€æ ·çš„åœ°æ–¹, å°±æœ‰å¯èƒ½æ˜¯ç—…å˜ç»„ç»‡
		- Inter-subject Registration
		  collapsed:: true
			- ![image.png](../assets/image_1676134746307_0.png)
			- ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªå¹³å‡çš„atlas, æˆ‘ä»¬æœ‰ä¸€å †subjects, å–è‡ªä¸åŒçš„äººçš„è„‘å­çš„MR
			- é¦–å…ˆæˆ‘ä»¬å¯¹è¿™äº›å›¾ç‰‡å¤§è‡´å¯¹é½, è¿›è¡Œä¸€ä¸ªavgçš„æ“ä½œ, ä½œä¸ºtarget image
			- ç„¶åç”¨å…¶ä»–çš„å›¾ç‰‡å¯¹è¿™ä¸ªtargetè¿›è¡Œregistration, å¯¹é½åé‡æ–°avg, è·å¾—æ›´æ¸…æ™°çš„å›¾ç‰‡, ä¸æ–­é‡å¤
			- Iterative registration 
			  1) Rigid
			  2) Affine
			  3) Nonrigid
		- Segmentation using Registration
			- Atlasæ˜¯åœ°å›¾é›†, å·²ç»æ‰‹åŠ¨æ ‡æ³¨äº†label
			- æˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¹atlasè¿›è¡Œregistration, mapåˆ°æƒ³è¦è¿›è¡Œæ–°çš„æ ‡æ³¨çš„å›¾ç‰‡ä¸Š, å°±å¯ä»¥propagatelabelåˆ°éœ€è¦segmentationçš„å›¾ç‰‡ä¸Šå»äº†
		- Multi-Atlas Label Propagation
		  collapsed:: true
			- ![image.png](../assets/image_1676135510352_0.png)
			- ç”¨å¤šä¸ªatlaså¯¹ç›®æ ‡å›¾ç‰‡è¿›è¡Œé…å‡†, å¾—åˆ°å¯¹åº”çš„segmentation label, ç„¶åfusion å¾—åˆ°ä¸€ä¸ªæ›´å‡†çš„ç»“æœ
		- Intensity-based Registration
			- Estimation of transformation parameters is driven by the appearance of the images
			- Images are registered when they appear similar
			- ![image.png](../assets/image_1676135634245_0.png)
			- å°±æ˜¯æ ¹æ®åƒç´ çš„å€¼, è¿›è¡Œç›¸ä¼¼åº¦æ£€æŸ¥, ç›¸ä¼¼åº¦æœ€é«˜çš„æ—¶å€™å°±æ˜¯é…å‡†å‡†ç¡®ç‡é«˜çš„æ—¶å€™
			- Objective function (cost, energy)
			  collapsed:: true
				- ![image.png](../assets/image_1676135773999_0.png)
				- moving image é€šè¿‡transformationå¾—åˆ°çš„å›¾ç‰‡, ä¸ç›®æ ‡å›¾ç‰‡çš„ä¸ç›¸ä¼¼åº¦å°±æ˜¯è¿™ä¸ªtransformationçš„cost
			- Optimisation problem
			  collapsed:: true
				- ![image.png](../assets/image_1676135831878_0.png)
			- Mono-modal vs Multi-modal
				- Mono-modal registration
				  â–ª Image intensities are related by a (simple) function
				  åƒç´ å€¼å¯èƒ½åªå’Œå•ä¸€ç®€å•çš„function æœ‰å…³ç³»
				- Multi-modal registration
				  â–ª Image intensities are related by a complex function or statistical relationship
				  æœ‰å¤æ‚çš„å‡½æ•°å…³ç³», æˆ–è€…æœ‰ç»Ÿè®¡å…³ç³»
			- (Dis)similarity Measures
				- Sum of squared differences (SSD)
					- ![image.png](../assets/image_1676135932507_0.png)
					- å‡è®¾çš„æ˜¯identity relationship, ç”¨äºmono model, ä¾‹å¦‚CT-CT
				- Sum of absolute differences (SAD)
					- ![image.png](../assets/image_1676135958621_0.png)
					- å‡è®¾çš„æ˜¯identity relationship, ç”¨äºmono model, ä¾‹å¦‚CT-CT
				- Correlation coefficient (CC)
					- ![image.png](../assets/image_1676135998373_0.png)
					- å‡è®¾çš„æ˜¯linear relationship, ç”¨äºmono model, ä¾‹å¦‚MR-MR
				- Statistical relationship
					- ![image.png](../assets/image_1676136107566_0.png)
					- å¯¹åº”intensityçš„ç‚¹ä¼šå½¢æˆé›†ç¾¤å¯¹åº”çš„mapping å…³ç³»
					- ![image.png](../assets/image_1676136145591_0.png)
					- å•modalå°±æ˜¯y=xçš„å…³ç³»
					- ![image.png](../assets/image_1676136167495_0.png)
					- multi-modal æœ‰ä¸€äº›å…¶ä»–çš„å¯¹åº”å…³ç³»
					- ![image.png](../assets/image_1676136229289_0.png)
					- [[Joint Entropy]]å¯ä»¥å¯¹å¯¹åº”ç‚¹çš„ä¸€èµ·å‡ºç°æ¦‚ç‡å»ºæ¦‚ç‡æ¨¡å‹, è¿™ä¸ªç³»ç»Ÿè¶Šç¨³å®š, è¿™ä¸ªæ¦‚ç‡æ¨¡å‹å¯¹åº”çš„ç†µè‚¯å®šè¶Šå°
					- ![image.png](../assets/image_1676136293852_0.png)
					- é€šè¿‡åˆ°è€ƒè™‘æœ¬èº«åƒç´ ç‚¹çš„ç†µ, å¯ä»¥å¾—åˆ°mutual information, ä¸€å¼ å›¾èƒ½è¢«å¦ä¸€å¼ å›¾æè¿°çš„å¯èƒ½æ€§
					- ![image.png](../assets/image_1676136343015_0.png)
					- [[NMI]]: æ›´é«˜é˜¶ä¸”å®ç”¨çš„æ–¹æ³•æ˜¯Normalised mutual information, ç”¨joint entropyä½œä¸ºnormalisation
					- ![image.png](../assets/image_1676136418948_0.png)
					- å»ºç«‹çš„å‡è®¾æ˜¯ç»Ÿè®¡å­¦ä¸Šçš„å…³ç³», å› æ­¤å¯ä»¥ç”¨äºå¤šæ¨¡æ€äº†, ä¸æ˜¯é‚£ä¹ˆé€‚ç”¨äºå•æ¨¡æ€, å¯èƒ½æ— æ³•å‘ç°æƒ³è¦çš„difference
				- Multi-scale, hierarchical Registration
				  collapsed:: true
					- é…å‡†è¿‡ç¨‹ä¸­ä¼šå‡ºç°çš„é—®é¢˜
					  collapsed:: true
						- Image Overlap: ä»…å¯¹overlapping éƒ¨åˆ†åšç›¸ä¼¼åº¦æ£€æŸ¥, é‡åˆéƒ¨åˆ†è¦è¶³å¤Ÿå¤§æ‰å¥½ç”¨
						- Capture Range: åœ¨capture rangeä¸­æ‰æ˜¯æœ‰æ•ˆçš„ç›¸ä¼¼åº¦æ£€æŸ¥, åˆ†ç¦»å¼€æ¥äº†æ¯”å¦‚åŒ¹é…ç©ºæ°”, é‚£è‚¯å®šéƒ½æ˜¯100%match
							- ![image.png](../assets/image_1676136663474_0.png)
						- Local Optima
							- ![image.png](../assets/image_1676136673974_0.png)
					- ç”¨äºè§£å†³ä¸Šè¿°é—®é¢˜, ä½¿ç”¨å¤šå°ºåº¦çš„é«˜æ–¯é‡‘å­—å¡”
						- â–ªSuccessively increase degrees of freedom 
						  â–ªGaussian image pyramids
						- ![image.png](../assets/image_1676136735499_0.png)
				- Interpolation
					- ![image.png](../assets/image_1676136769578_0.png)
					- Translate åçš„å€¼ä¸æ˜¯ä¸€ä¸€å¯¹åº”çš„, éœ€è¦å·®å€¼æ‰¾åˆ°
				- Registration as an Iterative Process
					- ![image.png](../assets/image_1676136804437_0.png)
					- Strategies: â–ª Gradient-descent â–ªStochastic optimisation â–ªBayesian optimisation â–ªDiscrete optimisation â–ªConvex optimisation
					  â–ª Downhill-simplex
				-
			-
		- Registration with Neural Networks
		  collapsed:: true
			- FlowNet: Learning Optical Flow with Convolutional Networks
				- ![image.png](../assets/image_1676136881616_0.png)
				- ![image.png](../assets/image_1676136889351_0.png)
				- ![image.png](../assets/image_1676136898878_0.png)
			- FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks
				- ![image.png](../assets/image_1676136931367_0.png)
				- é€šè¿‡ä¸€ä¸ªä¸ªç½‘ç»œblock, correctå‰é¢çš„ç»“æœ
			- Nonrigid Image Registration Using Multi-scale 3D CNNs
				- ![image.png](../assets/image_1676136997348_0.png)
				- å¤šå°ºåº¦çš„åº”ç”¨
			- Spatial Transformer Networks (STN)
				- [Spatial Transformer Networks Tutorial â€” PyTorch Tutorials 1.13.1+cu117 documentation](https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html) #PyTorch #Python #tutorial #ML
				- STNå¼•å…¥äº†ä¸€ä¸ªæ–°çš„å¯å­¦ä¹ çš„ç©ºé—´è½¬æ¢æ¨¡å—ï¼Œå®ƒå¯ä»¥ä½¿æ¨¡å‹å…·æœ‰ç©ºé—´ä¸å˜æ€§ã€‚è¿™ä¸ªå¯å¾®åˆ†æ¨¡å—å¯ä»¥æ’å…¥åˆ°ç°æœ‰çš„å·ç§¯ç»“æ„ä¸­ï¼Œä½¿ç¥ç»ç½‘ç»œèƒ½å¤Ÿåœ¨Feature Mapæœ¬èº«çš„æ¡ä»¶ä¸‹è‡ªåŠ¨åœ°å¯¹ç‰¹å¾è¿›è¡Œç©ºé—´å˜æ¢ï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒç›‘ç£æˆ–ä¼˜åŒ–è¿‡ç¨‹çš„ä¿®æ”¹ã€‚
				- `Localisation net`æ¨¡å—é€šè¿‡CNNæå–å›¾åƒçš„ç‰¹å¾æ¥é¢„æµ‹å˜æ¢çŸ©é˜µÎ¸ \thetaÎ¸
				- `Grid generator`æ¨¡å—å°±æ˜¯åˆ©ç”¨`Localisation net`æ¨¡å—å›å½’å‡ºæ¥çš„Î¸ \thetaÎ¸å‚æ•°æ¥å¯¹å›¾ç‰‡ä¸­çš„ä½ç½®è¿›è¡Œå˜æ¢ï¼Œè¾“å…¥å›¾ç‰‡åˆ°è¾“å‡ºå›¾ç‰‡ä¹‹é—´çš„å˜æ¢ï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„çš„æ˜¯è¿™é‡ŒæŒ‡çš„æ˜¯å›¾ç‰‡åƒç´ æ‰€å¯¹åº”çš„ä½ç½®ã€‚
				- `Sampler`å°±æ˜¯ç”¨æ¥è§£å†³`Grid generator`æ¨¡å—å˜æ¢å‡ºç°å°æ•°ä½ç½®çš„é—®é¢˜çš„ã€‚é’ˆå¯¹è¿™ç§æƒ…å†µï¼Œ`STN`é‡‡ç”¨çš„æ˜¯`åŒçº¿æ€§æ’å€¼(Bilinear Interpolation)`
				- ![image.png](../assets/image_1676137528984_0.png)
			- End-to-End Unsupervised Deformable Image Registration with a CNN
				- ![image.png](../assets/image_1676137622449_0.png)
			- An Unsupervised Learning Model for Deformable Image Registration
				- ![image.png](../assets/image_1676137665251_0.png)
			- Supervised below
			  collapsed:: true
				- VoxelMorph: A Learning Framework for Deformable Image Registration
					- ![image.png](../assets/image_1676137693469_0.png)
				- Learning Conditional Deformable Templates
					- ![image.png](../assets/image_1676137724604_0.png)
			- Template Registration
			  collapsed:: true
				- TeTrIS: Template Transformer Networks for Image Segmentation
					- ![image.png](../assets/image_1676137748647_0.png)
					- ![image.png](../assets/image_1676137761687_0.png)
			- Structure-Guided Registration
				- ISTNs: Image-and-Spatial Transformer Networks
					- ![image.png](../assets/image_1676137792291_0.png)
					- ![image.png](../assets/image_1676137866601_0.png)
					- ![image.png](../assets/image_1676137875129_0.png)
					- ![image.png](../assets/image_1676137884096_0.png)
				- Atlas-ISTNs: Joint Segmentation, Registration and Atlas Construction
				  collapsed:: true
					- ![image.png](../assets/image_1676137916670_0.png)
					- ![image.png](../assets/image_1676137932553_0.png)
					- ![image.png](../assets/image_1676137950381_0.png)
					-
	- Week6
	- Week7
	- Week8
	- è€ƒå·
		- 2021
		  collapsed:: true
			- logistic regression parameters è®¡ç®— includes raw pixels and a bias; e.g. 64 x 64 +1
			- Receptive Field è®¡ç®—
			  collapsed:: true
				- ä»å¤–åˆ°å†…, ä»ä¸Šåˆ°ä¸‹å†™.
				- æœ€å¤–å±‚çš„convçš„kernelå¤§å°, å°±æ˜¯ä¸‹ä¸€å±‚convéœ€è¦ç”Ÿæˆçš„ç‚¹æ•°, ä¾‹å¦‚æœ€å¤–å±‚æ˜¯3
				- æ ¹æ®ä¸‹ä¸€å±‚çš„kernelå’Œstride, ç”»ç”»å›¾ç®—ä¸€ç®—æ‰¾åˆ°ä¸‹ä¸€å±‚éœ€è¦çš„åƒç´ æ•°é‡, ä¾‹å¦‚ç¬¬äºŒå±‚æ˜¯2 stride2, é‚£ä¹ˆç¬¬äºŒå±‚çš„convç”Ÿæˆæœ€å¤–å±‚çš„3æ‰€éœ€çš„åƒç´ æ•°é‡å°±æ˜¯2*3=6
				- å†ä¸¾ä¸ªä¾‹å­, ç¬¬ä¸‰å±‚æ˜¯3 stride1, å› ä¸ºstrideæ˜¯1, æ‰€ä»¥é™¤äº†å‰ä¸‰ä¸ªåƒç´ ä»¥å¤–, åé¢ä¾æ¬¡å¢åŠ , å³3 + 5 = 8
				- è¿™æ ·å­ç®—åˆ°æœ€åä¸€å±‚å³å¯
				- $RF_N=1,\  RF_{N-1}= (RF_N - 1)\times stride_n + k_n$
			- Convolution & pooling è®¡ç®—
				- $o=[(i-k+2p)/s]+1$ - conv
				- $o = (i-1) \times s - 2p + k$ - transposed conv
			- Decision boundaries è®¡ç®—
			- Accuracy, precision (positive predictive value, PPV), recall (sensitivity, hit rate, true positive rate, TPR), specificity (recall for neg, true negative rate, TNR), F-beta
				- F-\beta = (1 + \beta^2) * (Precision * Sensitivity) / (\beta^2 * Precision + Sensitivity)
			- SSD, SAD, CC, and NMI
			- Harr features: efficient using integral images within constant time; suitable for extracting structures like edges and corners, not general features, cannnot approximate any filters via convolution
			- Options for regularization in a neural network:
				- L1 & L2 penalty on the weights
				- Dropout
				- Stochastic Gradient Descent with small batch sizes
				- Data augmentation
			- YOLO
				- architecture and how it works
				  id:: 64120319-9f33-4806-90c3-6bd66a5caecc
				- how to avoid computational difficulties from 1000 classes
					- one could separate the object detection from the object classification. For example, one trains a generic object detector to detect regions of interest (as in R-CNN) and then a separate object classifier.
			- L1 & L2 loss
			- Perceptual loss #dj w7
				- Computes loss on the output ğœƒ of an intermediate layer ğ‘™ of a pre-trained network:
				- è®¡ç®—ä¸­é—´å±‚çš„feature mapçš„åŒºåˆ«, ç”¨çš„L2
			- Interpretability and Trustworthy ML #dj w8
				- interpret an existing ML model: saliency maps (çªå‡ºé‡ç‚¹ä¿¡æ¯)
					- Gradient (backpropagation) f>0
					- Deconvolution R>0
					- Guided backpropagation f & R >0
					- f is activated feature map; R is backpropogated value from l+1 to l
				- Adversarial attacks
					- Perturbation
						- ä¸ºäº†è®©æ‰°åŠ¨è¶³å¤Ÿå°, æˆ‘ä»¬é™åˆ¶äº†etaçš„å¤§å°ä¸ºepsilon, $\eta = \epsilon Â· sign(\theta)$
						- å½“æƒé‡thetaä¹˜ä¸Šè¿™ä¸ªæ‰°åŠ¨çš„æ—¶å€™, æœ€ç»ˆçš„æ‰°åŠ¨æ€»å’Œä¼šæ˜¯$\epsilon m n$
						- mè¡¨ç¤ºæŸä¸ªthetaçš„å‡å€¼, nè¡¨ç¤ºç»´åº¦, å³åƒç´ æ•°é‡, å¯è§è¿™ä¸ªæ‰°åŠ¨ä¸nçº¿æ€§ç›¸å…³, å› æ­¤å°etaä¹Ÿä¼šå¸¦æ¥å¤§å˜åŒ–
						- This means that the change in activation given by the perturbation increases linearly with respect to ğ‘› (or the dimensionality). If ğ‘› is large, one can expect even a small perturbation capped at ğœ– to produce a perturbation big enough to render the model susceptible to an adversarial attack.
					- Fast Gradient Sign Method
						- $\tilde{x} = x + \epsilon Â· sign(\nabla_{x}\mathcal{L}(\theta,x,y))$
						- æ¢¯åº¦ä¸‹é™å‡å°‘æ¢¯åº¦, æˆ‘ä»¬å¢åŠ æ¢¯åº¦, å¯¹xå¢åŠ æ¢¯åº¦ä¸ºæ­£çš„éƒ¨åˆ†, æ¯æ¬¡å¢åŠ epsilon
		- 1920
		  collapsed:: true
			- L1 and L2 regularisation
			- Mechanisms for fixing high variance
				- L1 and L2 regularisation
				- dropout
				- train with small batch
				- early stopping
				- reduce model complexity, flexibility (degree)
				- reduce features
				- data augmentation and more data
				- bagging
			- Mechanisms for fixing high bias
				- Increase model complexity, flexibility (degree)
				- add features
				- boosting
				- feature engineering
				- more training data
			- Partial Volume Effects #dj w4
				- Due to the coarse sampling, both tissue types (black and white) contribute to the intensity value of the generated image (right) due to the relatively large influence area of each pixel (gray square in left image)
			- encoder-decoder network design
				- tricks:
					- Encoder: (k, s, p), (5, 2, 2), (3,2,1) å¯ä»¥å‡åŠ(å‘ä¸Šå–æ•´); (5,1,0) å‡4, (3,1,0)å‡2
					- Decoder: (4,2,1) å¯ä»¥å¢å€, (4,2,2)å¯ä»¥åœ¨å¢å€åŸºç¡€ä¸Šå‡2, (4,2,3)åŸºç¡€ä¸Šå‡3
			- Sketch the iterative process of intensity-based image registration. Include three main components of this process
				- Initialize the transformation parameters.
				- Apply the transformation model to the floating image using the current transformation parameters.
				- Calculate the similarity measure between the transformed floating image and the reference image.
				- Update the transformation parameters using the optimization strategy.
				- Repeat steps 2-4 until a termination criterion is met (e.g., the maximum number of iterations is reached or the change in similarity measure falls below a threshold).
				- Image transformation, Similarity measure using objective function, update the transformation parameters using optimisation strategy
			- spatial transformer network (STN)
				- It is a spatial transformer block which performs transformation on input data, make it invariant to translation, scale, rotation. With STN, network can learn to selectively apply spatial transformations to the input data, making it more robust to variations in scale, rotation, and other spatial factors.
			- Haar features
				- ä½¿ç”¨integral imagesè®¡ç®—é£é€Ÿ, D-B-C+A
				- haar featureså¯ä»¥scaleä¸ºä»»æ„å¤§å°, æ”¾åœ¨ä»»æ„ä½ç½®, è¡¨ç¤ºä¸ºç™½å‡é»‘çš„æ•°å€¼
			- RCNN, Fast-RCNN, Faster-RCNN, YOLO
			- Multi-label classificatin
				- This can be done by using a binary vector for each image, where each element of the vector corresponds to one of the possible classes, and a value of 1 indicates that the class is present in the image, while a value of 0 indicates that the class is absent.
				- Softmax and CE do not suit this case, as they assume there is only one correct class with sum of prob to be 1, while here we may have at most all ones. This may lead to unexpected results
				- Using sigmoid for each class and BCE to calculate the loss for each class and sum will be a better solution.
		- 1819
		  collapsed:: true
			- implementation of gradient descent
				- initialise weights, in training loop, calculate the predicted value, compute the loss, backpropogate the loss, compute the gradient for all samples and divided by number of samples, update weights by - alpha * gradient
			- implement stochastic gradient descent.
				- gradient calculated for each sample and update once per sample.
			- segmentation Overlapping measures
				- Jaccard Index (IoU, Jaccard Similarity Coefficient, JSC)
					- Intersection over Union
				- Diceâ€™s Coefficient (SÃ¸rensen Index, Dice Similarity Coefficient, DSC) F1
					- $DSC = \frac{2|A âˆ© B|}{|A|+|B|} = F1$
					- ![image.png](../assets/image_1678963717311_0.png)
					- Issue: Different shapes have identical DSC
				- volume similarity
					- ![image.png](../assets/image_1678963740289_0.png)
				- Hausdorff distance
					- ![image.png](../assets/image_1678963772894_0.png){:height 86, :width 287}
					- Aä¸Šæ¯ä¸ªç‚¹æ‰¾åˆ°Bä¸Šç¦»è¿™ä¸ªç‚¹çš„æœ€çŸ­è·ç¦», è¿™äº›è·ç¦»ä¹‹ä¸­æœ€é•¿çš„å°±æ˜¯æˆ‘ä»¬æ±‚çš„è¿™ä¸ªè·ç¦»
					- ç›¸åä¹Ÿè¦æ‰¾ä¸€é, å–æœ€å¤§. å› ä¸º bä¸Šå‡¸èµ·çš„éƒ¨åˆ†, è¿™ä¸ªæ—¶å€™æ‰ä¼šå‘ç°è·ç¦»è´¼å¤§.
				- average surface distance
					- ![image.png](../assets/image_1678963937586_0.png)
					- Aåˆ°Bçš„æœ€çŸ­è·ç¦»çš„å¹³å‡æ•°
			- Ensemble learning methods
				- Decision tree, info gain, entropy, adaboost
			- Neural Networks
				- purpose of the 1 x 1 convolutions
					- Dimensionality reduction
					- Feature combination across channels
				- computational graph
		- 1718
		  collapsed:: true
			- image modality
				- NMI é€‚åˆå¤šæ¨¡æ€multimodal similarity åˆ†æ, ä¸é€‚åˆ
			- challenges that might effect the accuracy of an image segmentation algorithm
				- Partial volume effect
				- Intensity Inhomogeneities
				- Anisotropic Resolution
				- Imaging Artifacts
				- Limited Contrast
				- Morphological Variability
			- Leakage in image segmentation
				- 'leakage' refers to the phenomenon where the boundaries of segmented regions overlap or bleed into neighboring regions, leading to inaccurate segmentation results. Leakage can occur when the intensity or texture characteristics of neighboring regions are similar to the region of interest, leading to the misclassification of neighboring pixels as part of the segmented region.
			- Segmentation Algorithms
				- â–ª Intensity-based segmentation
				  â–ª e.g., thresholding
				- â–ª Region-based: region growing
					- Start from (user selected) seed point(s), and grow a region according to an intensity threshold
				- â–ª Graph-based: graph cuts
					- Segmentation based on max-flow/min-cut algorithm
				- â–ª Active contours: â–ª e.g., level sets
				- â–ª Atlas-based segmentation: multi-atlas label propagation
					- Use multiple atlas to register a new data and get multiple segmentations. Do majority voting for the final class.
				- â–ª Learning-based segmentation
				  â–ª e.g., random forests, convolutional neural networks
	- Tutorials
	  collapsed:: true
		- W2
			- MLIé‡Œé¢biaséœ€è¦é»˜è®¤åŠ ä¸Š
			- convæ“ä½œé»˜è®¤ä¸ç”¨flip, ä½†æ˜¯å¯ä»¥è‡ªå·±æ³¨ä¸Šå»
		- W3
			- Haar features é€‚åˆæå–edge, cornerè¿™ç§ä¿¡æ¯
			- Support å’Œ receptive fieldçš„è®¡ç®—
			- convçš„parametersè®¡ç®—: kernels * (kernel size* input channels + bias)
		- W4
			- Ensemble methodsçš„advantages å’Œdisadv
			- Segmentation evaluation: ç»“åˆå¤šä¸ªä¸€èµ·ä¼šæ¯”è¾ƒæœ‰ç”¨ä¸€ç‚¹
			- Pitfalls in segmentation evaluation
				- structure size
				- structure shape, spatial alignment ä¸èƒ½è¢«DSCå’ŒIoUä½“ç°, ä½†å¯ä»¥ä»HDçœ‹å‡º
				- hole: æœ‰ä¸¤å±‚boundary, éƒ½è¦çœ‹è¿‡, å†…å±‚ç¦»å¤–è¾¹ç¼˜è·ç¦»ä¼šè¿œä¸€ç‚¹
				- annotation noise
				- resolution
			- challenges that might affect segmentation
				- â–ª noise,
				  â–ª partial volume effects,
				  â–ª intensity inhomogeneities, anisotropic resolution, â–ª imaging artifacts,
				  â–ª limited contrast,
				  â–ª morphological variability,
		- W5
			- (Dis)similarity measures
				- å¦‚æœå„ä¸ªç»„ç»‡çš„intensityé—´æœ‰ä¸€è‡´çš„çº¿æ€§å…³ç³»(ä¾‹å¦‚contrast change), æ­¤æ—¶å¯ä»¥ä½¿ç”¨ correlation coefficient; å¦‚æœçº¿æ€§å…³ç³»ä¸æ˜ç¡®, æˆ–å®Œå…¨å°±æ˜¯ä¸¤ä¸ªmodality, é‚£å°±å¾—ä½¿ç”¨ NMI
				- Joint Histograms: ä¸¤ä¸ªè½´æ˜¯ä¸¤å¼ å›¾ç‰‡çš„åƒç´ å¼ºåº¦å€¼, æ•£ç‚¹æ˜¯ä¸¤å¼ å›¾æ¯ç»„å¯¹åº”åƒç´ çš„å¼ºåº¦å€¼åˆ†å¸ƒ, å¦‚æœæ•´ä½“æ˜¯çº¿æ€§åˆ†å¸ƒçš„, çªç„¶å‡ºæ¥çš„ä¸€å—å°±æ˜¯lesionä¹‹ç±»çš„
			- Spatial Transformers
				- estimating the parameters of a **2D rigid** transformation for inputs of size 1 x 64 x 64. æœ€åè¦è¾“å‡ºä¸€ä¸ª3ç»´çš„(2ä¸ªtranslation, 1ä¸ªrotation) (3*2çš„affine matrixåŠ ä¸Šäº†scaleå’Œshear)
				- ![image.png](../assets/image_1679057312383_0.png)
		- W7
			- L1 L2 éƒ½è¦é™¤ä»¥åƒç´ æ•°é‡
			- Perceptual loss ç”¨äº†å‡ ä¸ªfilter, å°±è¦é™¤ä»¥å‡ , ç”¨çš„æ˜¯L2
		- W8
			- saliency maps using three methods
			- Fast Gradient Sign Method: x + epsilon * gradient
	- éš¾ç‚¹
		- Model complexity ä¸€èˆ¬æŒ‡çš„å°±æ˜¯parameteræ•°é‡
		- Receptive Field è®¡ç®— #card
		  id:: 6411aa7f-80ee-4a0d-bdaf-e0e6b35cec02
		  collapsed:: true
			- ä»å¤–åˆ°å†…, ä»ä¸Šåˆ°ä¸‹å†™.
			  id:: 6411aa8d-352f-4a94-898b-7e39bc7ac406
			- æœ€å¤–å±‚çš„convçš„kernelå¤§å°, å°±æ˜¯ä¸‹ä¸€å±‚convéœ€è¦ç”Ÿæˆçš„ç‚¹æ•°, ä¾‹å¦‚æœ€å¤–å±‚æ˜¯3
			  id:: 6411aaa6-dea5-4525-95dd-1e4b719125fb
			- æ ¹æ®ä¸‹ä¸€å±‚çš„kernelå’Œstride, ç”»ç”»å›¾ç®—ä¸€ç®—æ‰¾åˆ°ä¸‹ä¸€å±‚éœ€è¦çš„åƒç´ æ•°é‡, ä¾‹å¦‚ç¬¬äºŒå±‚æ˜¯2 stride2, é‚£ä¹ˆç¬¬äºŒå±‚çš„convç”Ÿæˆæœ€å¤–å±‚çš„3æ‰€éœ€çš„åƒç´ æ•°é‡å°±æ˜¯2*3=6
			  id:: 6411aad9-d4b0-45a1-91cb-dfbe745d79d4
			- å†ä¸¾ä¸ªä¾‹å­, ç¬¬ä¸‰å±‚æ˜¯3 stride1, å› ä¸ºstrideæ˜¯1, æ‰€ä»¥é™¤äº†å‰ä¸‰ä¸ªåƒç´ ä»¥å¤–, åé¢ä¾æ¬¡å¢åŠ , å³3 + 5 = 8
			  id:: 6411ab48-bb6f-4b24-959b-526e78050234
			- è¿™æ ·å­ç®—åˆ°æœ€åä¸€å±‚å³å¯
			- â€¢æ„Ÿå—é‡è®¡ç®—å…¬å¼ï¼š
			- é¦–å…ˆä»æœ€å¼€å§‹çš„layerå¼€å§‹ï¼Œè®¾å®šRF(receptive field) =1, SP(Stride Product)=1ï¼Œç„¶åå½“å‰å±‚çš„æ„Ÿå—é‡æ˜¯ RF=RF + (kernel_size-1ï¼‰* SPï¼Œç„¶åSPä¹Ÿè¦æ ¹æ®å½“å‰çš„strideæ›´æ–°ï¼šSP=SP*stride
			- $RF_N=1,\  RF_{N-1}= (RF_N - 1)\times stride_n + k_n$
		- Convolutionå’ŒTransposed Convolutionçš„è®¡ç®— #card
		  collapsed:: true
			- $o=[(i-k+2p)/s]+1$ - conv, é™¤æ³•å‘ä¸‹å–æ•´
			- $o = (i-1) \times s - 2p + k$ - transposed conv (è¾“å…¥å…ƒç´ é—´å¡«å……s-1, å››å‘¨å¡«å……k-p-1)
			- ```python
			  def conv_output(height, width, kernel_size, padding, stride, dilation): 
			  
			      height_out = int(np.floor(((height + 2 * padding - dilation * (kernel_size - 1) - 1) / stride) + 1)) 
			  
			      width_out = int(np.floor(((width + 2 * padding - dilation * (kernel_size - 1) - 1) / stride) + 1)) 
			  
			      print(height_out, width_out) 
			  
			      return height_out, width_out 
			  ```
		- AdaBoost
		- ensemble çš„
		- W7 W8
		- W2
		  collapsed:: true
			- Gradient descent
				- ![image.png](../assets/image_1679062755582_0.png)
				- ![image.png](../assets/image_1679062818738_0.png)
			- Stochastic gradient descent
				- Update once per sample
			- One-vs-all: learn ğ‘˜-binary classifiers, æ¯ä¸ªç±»éƒ½è®­ç»ƒä¸€ä¸ªbinary classifier
			-
		- W3
		  collapsed:: true
			- features: Distinctive, local, illumination invariant; intensity, gradient, SIFT, Harr
			- LBP: ä¹å®«æ ¼ç¬¬äºŒä¸ªå¼€å§‹é€†æ—¶é’ˆå†™vector, æ¯”ä¸­é—´å¤§ä¸º1
			- Haar
			- Ensemble classifiers
				- Error(xï¼‰ =Bias^2 + Variance + Irreducible Error
				- Homogenous, Heterogenous: using same or different ML model classes
				- Decision Stump
					- a one-level decision tree that is axis-aligned, åœ¨æŸä¸ªè½´ä¸Šå¤§äºæˆ–å°äº, å¾—åˆ°binaryè§£
				- Bagging: bootstrap, å°†è®­ç»ƒé›†åˆ†å‡ éƒ¨åˆ†è®­ç»ƒå‡ ä¸ªweak learner, aggregating, æ±‡æ€»å¤§å®¶çš„ç­”æ¡ˆ
				- Boosting: build weak learners in serial, adaptively reweight training data prior to training each new weak learner in order to give a higher weight to previously misclassified examples, training the next learner on the mistakes of the previous one
					- ![image.png](../assets/image_1679066430042_0.png)
					- Adaboost: equal weighted samples, åˆ†ç±», upweight misclassified, use loss to weight this classifier, é‡å¤tæ¬¡, ä½¿ç”¨weighted classifiers å¾—åˆ°æœ€åç»“æœ
				- Decision tree
					- ![image.png](../assets/image_1679066938090_0.png)
					- ![image.png](../assets/image_1679066945670_0.png)
					- Gini index: how mixed the classes are, 0 is best
						- ![image.png](../assets/image_1679067051183_0.png)
						- ![image.png](../assets/image_1679067056578_0.png)
			- Neural networks
				- multilayer perceptron çš„è¯ ä¼šéœ€è¦ neuronæ•°é‡ä¸ªbias
				- ![image.png](../assets/image_1679068069735_0.png)
				- ![IMG_1174.jpeg](../assets/IMG_1174_1679070695462_0.jpeg)
				- sigmoid': 0-0.25; tanh': 0-1
				- Stochastic gradient descent
					- ![image.png](../assets/image_1679070652989_0.png)
				- Residual connection
					- H(x)=F(x)+x
			-
		- W4
		  collapsed:: true
			- Challenges for Image Segmentation
			- Performance Measures
				- accuracy, precision, recall, specificity, F1 Score, Jaccard Index, Dice Similarity Coefficient (DSC), volume similarity,
			- Segmentation Algorithms & Techniques
				-
			- Fully Convolutional networks
			- Boundary Effects in transpose convolution
				- padding with 0 or nearest neighbour may give strange boundaries
			- Use only layers with small kernels to deal with deep issues
				- 2 3x3 instead of 1 5x5
			- Multi-Scale Processing
				- Two pathways, one using normal resolution, another with high resolution, concat.
				- This adds additional spatial info which enhances network's localisation capability
		- W5
		  collapsed:: true
			- ![image.png](../assets/image_1679075131522_0.png)
			- æ³¨æ„rigidæ˜¯2+1=3ä¸ªå‚æ•°, affineæ˜¯6ä¸ª
			- Free-Form Deformations: non-linear
			- Atlas construction
				- Iteration:
				- 1) Rigid
				- 2) Affine
				- 3) Nonrigid
			- Intensity-based Registration
				- Estimation of transformation parameters is driven by the appearance of the images
				- æ‰¾åˆ°è®©intensity relation å·®å¼‚æœ€å°çš„é‚£ä¸ªT
				- (Dis)similarity Measures D
					- SSD, SAD (identity relationship, CTCT) mono-modal
					- Correlation coefficient(CC, linear relationship, MRMR)mono-modal
					- multi-modal have statistical relationship, CTMR, multi-modal
						- $p(i,j)=\frac{h(i,j)}{N}$ : ä¸¤å¼ å›¾å¯¹åº”åƒç´ ç‚¹çš„å¼ºåº¦ä¸ºiå’Œjçš„æ¦‚ç‡
						- ![image.png](../assets/image_1679084168261_0.png)
						- ç†µè¶Šå°è¶Šå¥½, joint entropyä¹Ÿå¯ä»¥ä½œä¸ºD
						- ![image.png](../assets/image_1679084227247_0.png)
						- mutual info, è¡¨è¾¾äº†ä¸€å¼ å›¾è¢«å¦å¤–ä¸€å¼ å›¾çš„æè¿°æ€§, è¶Šå¤§è¶Šå¥½, ç”¨ä½œDæ—¶å–è´Ÿ
						- ![image.png](../assets/image_1679084330081_0.png)
						- normalised mutual info, ä¸overlap æ— å…³, åŒæ ·è¶Šå¤§è¶Šå¥½, ç”¨ä½œDæ—¶å–è´Ÿ
				- å› ä¸ºè¦åœ¨overlapçš„éƒ¨åˆ†è¡¡é‡, å¦‚æœåˆšå¥½åªæœ‰èƒŒæ™¯é‡åˆä¹Ÿä¼šæœ‰å¾ˆå¥½çš„æˆç»©, æ‰€ä»¥è¦å°å¿ƒçš„init
				- ![image.png](../assets/image_1679084646766_0.png){:height 287, :width 654}
			- Registration with Neural Networks
			- STN
		- W6
		  collapsed:: true
			- K-means
				- handles anisotropic data and non-linearities poorly
			- Gaussian Mixture Model (GMM)
				- ![image.png](../assets/image_1679086313611_0.png){:height 107, :width 260}
			- Dimensionality reduction / data encoding
				- PCA
					- ![image.png](../assets/image_1679086465067_0.png)
					- PCA is a linear encoder/decoder
				- Autoencoders
					- Auto-Encoders are general encoder/decoder architectures with non-linearities
					- non trivial to:
					  â€¢ visualize data in the latent space of Autoencoders
					  â€¢ generate new samples: perturb latent code of training example and iterate
			- Generative Modelling
				- Model the process through which data is generated
				  Capture the structure in the data
		- W7 inverse problem
		  collapsed:: true
			- Inverse problems
				- Inpainting, Deblurring, Denoising, Super-resolution
			- Classical approach: Least square solution, ç”¨yå’ŒAç›´æ¥è§£å‡ºx
			-
			- General approach:
				- ![image.png](../assets/image_1679089373884_0.png){:height 47, :width 305}
				- Aæ˜¯å·²çŸ¥çš„, æƒ³è¦ä»yå­¦å˜å›x
				- Ræ˜¯æ ¹æ®priorå†™çš„ä¸€ä¸ªregulariser, ç”¨æ¥é™åˆ¶æ¯”å¦‚è¯´åƒç´ çš„å¤§å°, åˆ†å¸ƒç­‰ç­‰, è¿™é‡Œæ˜¯äººä¸ºä½¿ç”¨åƒç´ çš„å¤§å°, åç»­ä¹Ÿå¯ä»¥ç”¨ç¥ç»ç½‘ç»œæ¥æ‰¾ä¸€ä¸ªåˆé€‚çš„å…ˆéªŒ, ä¾‹å¦‚å›¾ç‰‡å°±åº”è¯¥æ˜¯ç±»ä¼¼è¿™æ ·åˆ†å¸ƒçš„
			- Deep Learning approach
				- Model agnostic (ignores forward model)
					- ç›´æ¥åœ¨é€†å‘è¿‡ç¨‹ä¸­æ¥ç¥ç»ç½‘ç»œå»å­¦ä¹ , ä»yåˆ°xçš„ç›´æ¥æ˜ å°„
				- Decoupled (First learn, then reconstruct)
					- Deep proximal gradient: z^kä½œä¸ºä¸­é—´éšå˜é‡, è¡¨ç¤ºæ¢¯åº¦ä¸‹é™æ›´æ–°ä»¥åçš„x^, æ–°çš„x^é€šè¿‡ç¥ç»ç½‘ç»œä»z^kä¸­ç”Ÿæˆ, å­¦ä¹ ä»ä¸€ä¸ªä¸å¥å…¨çš„zç”Ÿæˆç§‘å­¦çš„xçš„è¿‡ç¨‹, denoising
					- GANs: ä»Gç”Ÿæˆçš„å›¾ç‰‡é‡Œæ‰¾åˆ°æœ€é€‚åˆdataçš„é‚£ä¸ª
				- Unrolled optimisation
					- Gradient descent networks, Ræ­£åˆ™é¡¹ä¹Ÿæ˜¯ä¸ªå¯å¯¼çš„ç¥ç»ç½‘ç»œ, é€šè¿‡ä¸æ–­è¿­ä»£æ¥å¯»æ‰¾åˆé€‚çš„é‡æ„å’Œdenoisingæ–¹å¼
			- Super-Resolution
				- Post-upsampling super-resolution: ç›´æ¥ç«¯åˆ°ç«¯, éœ€è¦å­¦ä¹ æ•´ä¸ªpipeline, é™åˆ¶åœ¨äº†ç‰¹å®šçš„ä¸Šé‡‡æ ·factor
				- Pre-upsampling super-resolution: å…ˆä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•upsampleåˆ°ç‰¹å®šå¤§å°, å†ç”¨nn refine, å¯ä»¥ä½¿ç”¨å„ç§upscaling factorså’Œimages sizes, éœ€è¦æ›´é«˜çš„è®¡ç®—åŠ›å’Œå†…å­˜
				- Progressive upsampling super-resolution: multi-stage, upsample to a higher resolution and refined at each stage, efficient, but difficult to train deep models
				- Iterative up-and-down sampling super-resolution: Alternate between upsampling and downsampling (back-projection) operations, ä¸”ä¸Šä¸‹é‡‡æ ·stagesä»¬æ˜¯ç›¸äº’è¿æ¥çš„, superior performance by allowing error feedback, easier to train with deep
			- Perceptual loss: äº‹å®å’Œä¼°è®¡éƒ½ç”¨åŒä¸€ä¸ªç½‘ç»œç‰¹å¾æå–åçš„mapè¿›è¡ŒL2 norm, ç”¨äº†å‡ ä¸ªç½‘ç»œå°±æŠŠè¿™å‡ ä¸ªç»“æœåŠ èµ·æ¥å–å¹³å‡
			- GAN:
				- ![image.png](../assets/image_1679091710772_0.png)
				- ![image.png](../assets/image_1679092208935_0.png)
			- Image Reconstruction
				- X-ray computed tomography (CT)
					- Sinogram to CT image
				- Magnetic Resonance (MR)
					- Slow acquisition, bad for moving objects
					- performed in k-space by sequentially traversing sampling trajectories. need to inverse to signal space, IFT
			-
		- W7 Object detection
		  collapsed:: true
			- classification(softmax) + localisation(L2)
			- å› ä¸ºä¸å¯èƒ½sliding windowå…¨éƒ¨éå†, æ‰€ä»¥éœ€è¦region proposal
			- R-CNN
				- ![image.png](../assets/image_1679093581412_0.png){:height 349, :width 654}
			- Spatial Pyramid Pooling (SPP-net)
				- ![image.png](../assets/image_1679095241578_0.png)
				- åœ¨FMä¸Šregion proposal, åªè®¡ç®—ä¸€æ¬¡feature, é€šè¿‡SPPæŠŠregionså˜æˆç»Ÿä¸€é•¿åº¦å‘é‡; SPPä¸‹æ— æ³•æ›´æ–°
			- Fast R-CNN
				- åœ¨å›¾ç‰‡é‡Œregion proposal, æ˜ å°„åˆ°FMä¸­, é€šè¿‡ROI poolingå˜æˆ7x7, FCç»™åˆ†ç±»å’Œå›å½’, è¿™é‡Œçš„åˆ†ç±»ç”¨çš„ä¹Ÿæ˜¯FCå’Œsoftmax
				- ![image.png](../assets/image_1679095599657_0.png)
			- Faster R-CNN
				- Insert Region Proposal Network (RPN) to predict proposals from features
				- 1. RPN classify object / not object
				  2. RPN regress box coordinates
				  3. Final classification score (object classes)
				  4. Final box coordinates
				- ç”¨RPNæ¥é¢„æµ‹feature mapä¸Šæ˜¯å¦ä¸ºobject, ä»¥åŠbounding box
			- YOLO
				- åˆ†å‰²å›¾ç‰‡æˆ7x7ä¸ªæ ¼å­, æ¯ä¸ªæ ¼å­è´Ÿè´£é¢„æµ‹ä¸€å®šæ•°é‡çš„bounding box, æœ‰4ä¸ª coordinates, 1ä¸ªconfidence score, ä»¥åŠ1ä¸ªç±»åˆ«æ•°dimçš„åˆ†ç±»vector
					- 7 x 7 x (2 x 5 + 20)
				- perform NMS and threshold æ¥é€‰æ‹©å¥½ç”¨çš„, increase the confidence for the best ones, decrease others
		- W8 trustworthy
			- Federated learning: train a ML model across decentralized clients with local data, without exchanging them
			- Federated learning: åˆ†å‘æ¨¡å‹ç»™clients, å„è‡ªç”¨è‡ªå·±æ•°æ®è®¡ç®—æ¨¡å‹æ›´æ–°, å‘å›owneræ¥aggregate
				- ![image.png](../assets/image_1679133860277_0.png){:height 256, :width 237}
				- ä¹Ÿå¯ä»¥ç›´æ¥æ›´æ–°weights, serveré‚£ä¹Ÿç›´æ¥average weights
				- æŒ‘æˆ˜: non-iid, unbalanced, commu cost
			-
			- Homomorphic encryption which enables learning from encrypted data
			- Secure multi-party computing where processing is performed on encrypted data shares, split among them in a way that no single party can retrieve the entire data on their own. å…±åŒè®¡ç®—ä¸€ä¸ªå¹³å‡åˆ†, ç¬¬ä¸€ä¸ªäººå…ˆåŠ ä¸ªéšæœºæ•°, å•å‘ä¼ æ’­, å›åˆ°è‡ªå·±çš„æ—¶å€™å¯ä»¥è®¡ç®—å¹³å‡åˆ†
			- Interpretability
				- social issue, debuging
				- Ablation test: How important is a data point or feature?
				- Fit functions (use first derivatives, sensitivity and saliency maps)
				- visualize activations generated by kernels
				- Mask out region in the input image and observe network output
				- DeconvNet: Chose activation at one layer, å–gradientä¸­éè´Ÿçš„æ•°
				- Gradient (backprogagation): differentiate activation with respect to input pixels, å–gradienté‡Œactivationæ¿€æ´»çš„å¯¹åº”ä½ç½®é¡¹
				- Guided backpropagation: gradienté‡Œé¢ä¸ºè´Ÿçš„ä¹Ÿä¸å–
				- ![image.png](../assets/image_1679136454035_0.png)
				- Inverting codes via deep image priors: ç”Ÿæˆç½‘ç»œä»z0ç”Ÿæˆx, lossç”±å¦ä¸€ä¸ªç‰¹å¾æå–ç½‘ç»œå¯¹åŸå›¾å’Œç”Ÿæˆå›¾çš„ç»“æœå·®å¾—åˆ°ã€
				- Learning the inverter from data: ç”Ÿæˆç½‘ç»œä»ç‰¹å¾æå–ç½‘ç»œå¾—åˆ°çš„ç‰¹å¾å¼€å§‹ç”Ÿæˆx, ç›´æ¥ä¸åŸå›¾è®¡ç®—loss
			- DeepDreamå’ŒInversionæ˜¯éå¸¸ç›¸ä¼¼çš„æ–¹æ³•ï¼Œå®ƒä»¬è¯•å›¾å°†ç¥ç»ç½‘ç»œçš„å·¥ä½œå†…å®¹å’Œå·¥ä½œæ–¹å¼å¯è§†åŒ–ã€‚Deep Imageçš„å…ˆéªŒæ˜¯ä¸åŒçš„ï¼Œå®ƒä¸ºå›¾åƒæŒ‡å®šäº†ä¸€ä¸ªç‰¹å®šçš„å…ˆéªŒï¼ˆè€Œä¸æ˜¯ä½¿ç”¨L1 normè¿™æ ·çš„å…ˆéªŒï¼‰ã€‚DeepDreamå’ŒInversionå¯ä»¥ä½¿ç”¨ä¸åŒçš„å…ˆéªŒã€‚
			- Adversarial Methods
				- ![image.png](../assets/image_1679137387448_0.png)
				- ![image.png](../assets/image_1679137396653_0.png)
				- ![image.png](../assets/image_1679137459790_0.png)
			-
	- ç–‘ç‚¹:
		-
- ## Coursework
  collapsed:: true
	- Age Regression from Brain MRI
	  â–ª mini-project on a real-world medical imaging task
	  â–ª implement two different machine learning approaches â–ª work in groups of two
	- Start: Friday, February 17 (week 6)
	- End: Thursday, March 2 (week 8)
	-
- ## Info
  collapsed:: true
	- 8:2
	- â–ª Fridays, 14:00, LT308: Q&A, invited talks, quizzes
	  â–ª Fridays, 15:00, LT202/206/210 : programming tutorials
- ## Syllabus
  collapsed:: true
	- Introduction to machine learning for imaging
	- Image classification
	- Image segmentation
	- Object detection & localisation
	- Image registration
	- Generative models and representation learning
	- Application to real-world problems
- ## Links
  collapsed:: true
	- [Scientia](https://scientia.doc.ic.ac.uk/2223/modules/70014/materials)
	- [Panopto](https://imperial.cloud.panopto.eu/Panopto/Pages/Sessions/List.aspx#folderID=%229755113e-85d4-4a84-afcc-aedd01525333%22)
	- [Spatial Transformer Networks Tutorial â€” PyTorch Tutorials 1.13.1+cu117 documentation](https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html)
	- [GitHub - ecekayan/mli-cw: Machine Learning for Imaging CW](https://github.com/ecekayan/mli-cw)
	- [GitHub - marektopolewski/ic-mli-cw](https://github.com/marektopolewski/ic-mli-cw)
	- [GitHub - Cy-r0/mli_cw2: Second coursework of Machine Learning for Imaging](https://github.com/Cy-r0/mli_cw2)
	-
