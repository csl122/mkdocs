{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Colin's Space","text":"<p>This is a place for me to keep track of my learning progress and share my thoughts with others.</p>"},{"location":"#aigc","title":"AIGC","text":"<p>Anything about Diffusion models, GANs, VAEs, etc. will be put here.</p>"},{"location":"#nlp","title":"NLP","text":"<p>GPT, Large Language Models and other NLP stuff.</p>"},{"location":"#mldl","title":"ML/DL","text":"<p>General ML/DL stuff.</p>"},{"location":"#mac","title":"Mac","text":"<p>Anything interesting about Mac, such as <code>brew</code>, Mac apps, etc.</p>"},{"location":"#linux","title":"Linux","text":"<p>Practical Linux stuff.</p>"},{"location":"#networking","title":"Networking","text":"<p>Server, proxy, network, etc.</p>"},{"location":"#game","title":"Game","text":"<p>Share my game experience.</p>"},{"location":"start/","title":"Start From Here","text":""},{"location":"start/#install","title":"Install","text":"test.py<pre><code>pip install mkdocs\nimport mkdocs\ndef abc():\npass\n</code></pre> test.py<pre><code>pip install mkdocs\nimport mkdocs\ndef abc():\npass\n</code></pre> <pre><code>theme:\nfeatures:\n- content.code.annotate # (1)\n</code></pre> <ol> <li> I'm a code annotation! I can contain <code>code</code>, formatted     text, images, ... basically anything that can be written in Markdown.</li> </ol> <pre><code># (1)!\n</code></pre> <ol> <li>Look ma, less line noise!</li> </ol>"},{"location":"aigc/","title":"AIGC","text":"<p>The following content is reproduced from: The Roadmap of Generative AI</p> <p></p>"},{"location":"aigc/#the-roadmap-of-generative-ai-ai","title":"The Roadmap of Generative AI \u751f\u6210\u5f0fAI\u7684\u5e94\u7528\u8def\u7ebf\u56fe","text":"<p>The roadmap of generative AI: use cases and applications.</p>"},{"location":"aigc/#license","title":"LICENSE","text":"<p>This work is licensed under a Creative Commons Attribution 4.0 International License.</p>"},{"location":"aigc/#localization","title":"Localization","text":"<p>The original diagrams and text contents are in Chinese. We are considering translating them into English.</p>"},{"location":"aigc/#ai-1","title":"\u751f\u6210\u5f0fAI\u7684\u5e94\u7528\u8def\u7ebf\u56fe | \u56fe1 \u53ef\u63a7\u6027\u7684\u6f14\u8fdb\u89c4\u5f8b","text":""},{"location":"aigc/#ai-2","title":"\u751f\u6210\u5f0fAI\u7684\u5e94\u7528\u8def\u7ebf\u56fe | \u56fe2 \u53ef\u63a7\u6027\u4e0e\u5e94\u7528\u65b9\u5411","text":""},{"location":"aigc/#ai-3","title":"\u751f\u6210\u5f0fAI\u7684\u5e94\u7528\u8def\u7ebf\u56fe | \u56fe3 \u5e94\u7528\u9886\u57df\u4e0e\u5178\u578b\u6848\u4f8b","text":""},{"location":"aigc/#ai-4-ai","title":"\u751f\u6210\u5f0fAI\u7684\u5e94\u7528\u8def\u7ebf\u56fe | \u56fe4 \u591a\u6a21\u6001AI\u7684\u5e94\u7528\u80fd\u529b\u6f14\u8fdb","text":""},{"location":"aigc/#_1","title":"\u5927\u6a21\u578b\u6280\u672f\u4e0e\u5e94\u7528\u601d\u8003\u5bfc\u56fe","text":""},{"location":"aigc/Stable%20Diffusion/","title":"Stable Diffusion","text":"<p>tags:: Generative, AIGC, Tool, Diffusion alias::  SD, StableDiffusion</p> <ul> <li>Paper: [2112.10752] High-Resolution Image Synthesis with Latent Diffusion Models</li> <li>Code: GitHub - CompVis/stable-diffusion: A latent text-to-image diffusion model<ul> <li>GitHub - Stability-AI/stablediffusion: High-Resolution Image Synthesis with Latent Diffusion Models</li> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> </ul> </li> </ul>"},{"location":"aigc/Stable%20Diffusion/#latent-diffusion-models","title":"\u2b07\ufe0fLatent Diffusion Models","text":"collapsed:: true<ul> <li>Latent Diffusion Models</li> <li>\u9996\u5148\u662f\u6574\u4e2a\u5927\u7c7b\u7684\u5b9a\u4e49, \u9700\u8981UNet, AutoEncoder, CLIPTextEmbedder\u8fd9\u51e0\u4e2a\u6a21\u578b; latent scaling factor\u7528\u6765scale encoder encode\u7684encodings, \u518d\u8f93\u5165\u5230UNet\u91cc\u9762, n_steps\u8868\u793a\u6211\u4eec\u5b9e\u9645\u8981\u8fdb\u884c\u7684diffusion steps, \u4e24\u4e2alinear\u662f\u65b9\u5deebeta\u7684schedule\u8d77\u59cb\u548c\u7ed3\u675f\u7684\u4f4d\u7f6e. Beta\u4f1a\u5728\u521d\u59cb\u5316\u4e2d\u5b9a\u4e49\u597d, alpha\u4e5f\u4f1a\u5b9a\u4e49\u597d\u4e3a\u4e4b\u540e\u505a\u51c6\u5907</li> <li></li> <li>Text conditioning \u662f\u7528CLIP\u6a21\u578b\u5f97\u5230prompts string list\u4ee5\u540e\u5f97\u5230\u7684. Encode\u51fd\u6570\u4f1a\u628a\u56fe\u7247\u7ed9\u5230autoencoder\u53bb\u5f97\u5230\u4e00\u4e2adistribution, \u7136\u540e\u6211\u4eec\u4ece\u4e2dsample\u4e00\u4e2a, \u4f46\u662f\u6700\u540e\u8fd8\u8981\u4e58\u4e0a\u4e00\u4e2ascaling  factor. Decode\u51fd\u6570\u4f1a\u628a\u53cd\u5411diffusion sample\u5230\u7684z\u9664\u4ee5\u4e4b\u524dscale\u7684\u53c2\u6570\u7ed9\u89e3\u7801\u5f97\u5230\u56fe\u7247. \u6574\u4e2a\u6a21\u578b\u7684forward\u5c31\u662f\u8c03\u7528UNet\u6765\u9884\u6d4b\u8be5\u65f6\u95f4\u70b9\u524d\u6240\u4f7f\u7528\u7684epsilon</li> </ul>"},{"location":"aigc/Stable%20Diffusion/#autoencoder-for-stable-diffusion","title":"\u2b07\ufe0fAutoencoder for Stable Diffusion","text":"collapsed:: true<ul> <li>Autoencoder for Stable Diffusion</li> <li>This implements the auto-encoder model used to map between image space and latent space.</li> <li>\u8fd9\u91cc\u7684autoencoder\u6709\u4e24\u79cd embedding space. \u4e00\u4e2a\u662fembedding space, \u4e00\u4e2a\u662fquantised embedding space. \u8fd9\u91cc\u662f\u56e0\u4e3aSD\u4f7f\u7528\u4e86VQ-reg, \u7528vector quantisation\u6765\u5b9e\u73b0\u6b63\u5219, \u4f53\u73b0\u5728\u4ee3\u7801\u4e2d\u5c31\u4f7f\u7528\u4e00\u4e2a11conv\u6765\u53d8\u6362\u4e86\u7ef4\u5ea6, (\u771f\u6b63\u7684VQ\u5e94\u8be5\u8fd8\u4f1aquantise\u51fd\u6570\u8c03\u7528, \u4ececodebook\u91cc\u627e\u5230\u6700\u63a5\u8fd1\u7684) \u540c\u65f6\u8fd9\u4e2a\u7c7b\u5176\u5b9e\u662fAutoencoderKL, \u662f\u4e00\u4e2aVAE, \u56e0\u6b64\u8fd8\u4ece\u4e2d\u91c7\u6837\u5f97\u5230\u4e00\u4e2aposterior. \u5404\u81ea\u6709\u5404\u81ea\u7684channels\u6570\u91cf, \u540c\u65f6\u4ed6\u4eec\u4f1a\u6709\u4e00\u534a\u7684channels\u662f\u8868\u793amean, \u4e00\u534a\u8868\u793avairance</li> <li></li> <li>Encode \u548c Decode\u8fc7\u7a0b\u5305\u62ec\u9996\u5148\u4f7f\u7528encoder\u5f97\u5230latent vector z, \u518d\u5bf9z quantise\u5230 quantised embedding space\u4f5c\u4e3amoments, \u7136\u540e\u5229\u7528\u8fd9\u4e2amean\u548cvariance\u6765sample; decode\u5219\u662f\u5f97\u5230quantised\u7684z \u5bf9\u4ed6\u8fdb\u884c\u53cd\u5411\u6620\u5c04\u56deembedding space, \u7136\u540edecode\u5230\u56fe\u7247</li> <li></li> <li>Encoder module \u8d1f\u8d23\u628a\u7ed9\u5230\u7684\u56fe\u7247\u4e00\u70b9\u70b9\u7f29\u5c0f, \u63d0\u53d6\u5230\u6700\u9700\u8981\u7684\u7279\u5f81\u4f5c\u4e3alatent z, \u5c3a\u5bf8\u53d8\u5c0f\u7684\u8fc7\u7a0b\u4e2d, channels\u6570\u4f1a\u8d8a\u6765\u8d8a\u591a, channel\u7684\u589e\u957f\u662f\u7531channel_multipliers\u63a7\u5236\u7684</li> <li></li> <li>Encode\u8fc7\u7a0b\u4e2d, \u5206\u8fa8\u7387\u7684\u53d8\u5316\u53d6\u51b3\u4e8e\u6709\u591a\u5c11\u4e2achannel_multipliers, \u5373\u8981\u53d8\u5316\u591a\u5c11\u6b21channel, \u6bcf\u6b21\u90fd\u4f1a\u51cf\u534a\u5206\u8fa8\u7387. \u9996\u5148\u4f7f\u7528\u4e00\u4e2a3311\u7684conv2d\u4e0d\u6539\u53d8\u5206\u8fa8\u7387\u4f46\u662f\u628achannel\u6570\u5148\u53d8\u6210\u7b2c\u4e00\u4e2a\u9700\u8981\u7684channels\u6570\u91cf. \u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5206\u8fa8\u7387, \u90fd\u4f1a\u6709\u6570\u4e2a\u5b9a\u4e49\u597d\u6570\u91cf\u7684resnet_block\u7528\u6765\u63d0\u53d6\u7279\u5f81, \u8fd9\u4e2aresnet list\u4f1a\u9996\u5148\u628a\u5f53\u524dchannel\u53d8\u6210\u4e0b\u4e00\u8f6e\u9700\u8981\u7684channel\u6570\u91cf, \u7136\u540e\u540e\u9762\u51e0\u4e2aresnet\u5c31\u4fdd\u6301channels\u4e0d\u53d8\u4e86. \u6bcf\u4e2a\u5206\u8fa8\u7387\u90fd\u4f1a\u6709\u8fd9\u4e48\u4e00\u7ec4resnet, \u7136\u540e\u5728\u6700\u540e\u4e00\u6b21downsample\u5230\u4e00\u534a\u7684\u5206\u8fa8\u7387. \u6700\u540e\u8fd8\u4f1a\u6709mid blocks, \u4e0d\u6539\u53d8\u5206\u8fa8\u7387\u548c\u901a\u9053, \u5229\u7528\u4e86\u4e24\u4e2aresnet\u4e2d\u95f4\u5939\u4e86\u4e00\u4e2aattention. \u6700\u540enormalise\u4e86\u4e00\u4e0b\u7136\u540e\u752811\u5377\u79efmap\u5230\u4e86\u9700\u8981\u7684mean+variance embedding channel</li> <li></li> <li>Encoder\u7684\u524d\u5411\u8fc7\u7a0b\u5305\u62ec\u5148\u628a\u56fe\u7247\u53d8\u6210\u521d\u59cbchannel, \u7136\u540e\u5bf9\u4e8e\u591a\u4e2a\u5206\u8fa8\u7387, \u7528\u591a\u4e2aresnet\u5904\u7406\u4ee5\u540edownsample, \u6700\u540e\u7ecf\u8fc7mid\u5904\u7406 attention\u6700\u540econv\u5230mean+variance \u7684 embeeding space\u8f93\u51fa</li> <li></li> <li>Decoder \u6a21\u5757\u7531\u4e8e\u6ca1\u6709\u4f7f\u7528convTranspose2D, \u6240\u4ee5\u6574\u4e2a\u8fc7\u7a0b\u5c31\u662f\u53cd\u8f6c\u4e00\u4e0bencode\u7684\u8fc7\u7a0b</li> <li></li> <li></li> <li>Decoder\u7684\u524d\u5411\u8fc7\u7a0b\u5c31\u662f, \u5148\u628az\u53d8\u6362\u6210\u6700\u540e\u7684\u90a3\u4e2achannel\u6570, \u7136\u540e\u7ecf\u8fc7mid blocks attention\u7136\u540e\u518d\u63d0\u5347\u5206\u8fa8\u7387\u7684\u8fc7\u7a0b\u4e2d, \u7ecf\u8fc7\u4e00\u7cfb\u5217\u7684resnet\u4ee5\u53caupsample, \u6700\u540enorm\u4e00\u4e0b\u518dmap\u5230\u56fe\u7247\u76843channel</li> <li></li> <li>Gaussian Distribution\u662f\u5f97\u5230quantised embedding \u4ee5\u540e\u5229\u7528\u5176mean\u548cvariance\u6765\u4ece\u4e2dsample\u6837\u672c\u7528\u7684</li> <li></li> <li>Attention block \u5b9e\u73b0\u7684\u662f\u628a\u6cbf\u7740channel\u7ef4\u5ea6\u7684\u50cf\u7d20\u4eec\u4f5c\u4e3asequence, \u4e00\u4e2afeature map\u662f\u4e00\u4e2asequence, \u4e00\u5171\u6709\u50cf\u7d20\u4e2a\u8bcd, \u6bcf\u4e2a\u50cf\u7d20\u8bcd\u7684embedding\u957f\u5ea6\u90fd\u662fchannels\u6570.  torch.einsum\u505a\u7684\u4e5f\u662f\u6700\u540e\u751f\u6210ixj\u7684attention matrix, \u6700\u540e\u6cbf\u7740j\u7ef4\u505a\u6a2a\u5411\u7684softmax\u5f97\u5230\u52a0\u6743\u767e\u5206\u6bd4, \u4e58\u4e0a\u5404\u81ea\u7684v\u6700\u540e\u5f97\u5230att\u5b8c\u4e86\u7684\u5168\u5c40att\u7684v. \u5f53\u7136\u6700\u540e\u8fd8\u6709\u989d\u5916\u768411conv \u548c residual connection.</li> <li></li> <li>Upsampling \u548cDownsampling\u7684\u5b9e\u73b0\u6bd4\u8f83\u7b80\u5355. Up\u662f\u5148\u76f4\u63a5\u4f7f\u7528interpolate nearest\u67652\u500d\u653e\u5927, \u518d\u7528\u4e0d\u6539\u53d8\u5206\u8fa9\u7387\u7684conv\u8c03\u6574. Down\u7684\u8bdd\u5728\u4e0b\u9762\u548c\u53f3\u8fb9\u586b\u5145\u4e860(\u56e0\u4e3a\u8981pad1\u800c\u4e0d\u662f\u4e24\u8fb9\u90fdpad2\u624d\u80fd\u5b9e\u73b0\u521a\u597d/2), \u7136\u540e\u75283320conv\u53d8\u5c0f</li> <li></li> <li>ResNet Block\u7684\u5b9e\u73b0\u4e5f\u662f\u7b80\u5355\u7684, \u5b9a\u4e49\u597din\u548cout Channels\u540e, norm, conv(in, out), norm, conv(out, out), \u4ee5\u53cashortcut\u5c31\u884c\u4e86(shortcut\u9700\u8981\u8003\u8651\u5230inout\u4e0d\u540c\u7684\u60c5\u51b5\u5c31\u898111conv\u8c03\u4e00\u4e0b). \u8fd9\u91cc\u6ce8\u91ca\u4e00\u4e0b\u6bd4\u8f83\u795e\u5947\u7684\u662fact\u5728conv\u524d\u9762</li> <li></li> </ul>"},{"location":"aigc/Stable%20Diffusion/#u-net-for-stable-diffusion","title":"\u2b07\ufe0fU-Net for Stable Diffusion","text":"collapsed:: true<ul> <li>U-Net for Stable Diffusion</li> <li>\u8fd9\u4e2aU-Net\u7528\u5230\u4e86attention, \u662f\u4e2aspatialTransformer, \u5176\u6784\u9020\u548cDDPM\u91cc\u9762\u7528\u5230\u7684\u5f88\u50cf, \u53ef\u4ee5\u53bb\u53c2\u8003\u90a3\u8fb9\u7684\u4e5f. \u591a\u7684\u4e24\u4e2a\u53c2\u6570\u662ftf</li> <li></li> <li>\u4e0b\u9762\u8fd9\u4e2a\u90e8\u5206\u5b9a\u4e49\u4e86\u4e00\u4e0bU-Net\u6709\u591a\u5c11\u4e2a\u5206\u8fa8\u7387level, \u9020\u4e86\u4e00\u4e2atimeembedding\u7684MLP, \u7136\u540e\u521d\u59cb\u5316\u4e86input_blocks\u4f5c\u4e3a\u5de6\u534a\u8fb9\u7684ModuleList. TimestepEmbedSequential\u5305\u88f9\u4f4f\u4e86\u7b2c\u4e00\u4e2a\u7528\u6765\u5bf9\u9f50channel\u7528\u7684conv2d, \u539f\u56e0\u662f\u8fd9\u4e2a\u7c7b\u53ef\u4ee5\u81ea\u52a8\u8bc6\u522b\u91cc\u9762\u662f\u4ec0\u4e48\u7c7b\u578b\u7684function, \u6765\u81ea\u52a8\u5206\u914d\u6240\u9700\u8981\u7684feature map, \u6216\u662ffm\u4ee5\u53catime embedding</li> <li></li> <li>\u4e0b\u9762\u6784\u9020\u4e86U-Net\u7684\u5de6\u534a\u8fb9, \u4e00\u5171\u6709level\u4e2a\u5206\u8fa8\u7387\u5c42, \u6bcf\u4e2a\u5206\u8fa8\u7387\u5c42\u6709n_res_block\u4e2aresnet_block, \u5176\u4e2d\u5982\u679c\u8fd9\u91cc\u662f\u9700\u8981\u7528attention\u7684\u8bdd, \u5c31\u4f1a\u5728ResBlock\u540e\u9762append\u4e0a\u4e00\u4e2aattention block, \u8fd9\u91cc\u662fSpatialTransformer. \u7136\u540e\u5728\u6700\u540e\u4e00\u5c42\u8fdb\u884cdownsample</li> <li></li> <li>\u4e2d\u95f4\u5c42\u7531res, ST, res\u7ec4\u6210</li> <li></li> <li>U-Net\u7684\u53f3\u534a\u8fb9\u5c31\u662f\u5de6\u534a\u8fb9\u53cd\u4e00\u4e0b</li> <li></li> <li>Time embedding\u548cDDPM\u91cc\u7684\u5dee\u4e0d\u591a</li> <li></li> <li>\u524d\u5411\u8fc7\u7a0b\u5c31\u662f\u5de6\u534a\u8fb9input_blocks\u91cc\u9762\u7684module\u4e00\u4e2a\u4e2a\u5904\u7406x, \u7136\u540e\u5e76\u4e14\u8bb0\u5f55\u4e0b\u6765, \u7528\u4f5c\u53f3\u8fb9\u52a0\u4e0a\u53bb\u7684skip connection. \u6ce8\u610finput\u548coutput_blocks\u91cc\u7684\u6bcf\u4e00\u4e2amodule\u90fd\u4f1a\u8f93\u5165\u5230t_emb, \u4e5f\u5c31\u662f\u8bf4\u91cc\u9762\u7684resblock\u90fd\u4f1a\u7528\u5230t_emb</li> <li></li> <li>Sequential block for modules with different inputs \u53ef\u4ee5\u63a5\u6536x, t, cond\u4e09\u79cd, \u4f46\u662f\u9009\u62e9\u6027\u5730apply\u8fdb\u5176\u4e2d\u7684module\u4e2d\u4f5c\u4e3a\u8f93\u5165, \u89e3\u51b3\u4e86\u4e0d\u540csignature\u7684\u5bf9\u9f50\u8f93\u5165\u95ee\u9898</li> <li></li> <li>UP\u548cDown sample\u548cautoencoder\u7c7b\u4f3c</li> <li></li> <li>ResNet Block \u4e5f\u5927\u540c\u5c0f\u5f02, \u5b9a\u4e49\u4e86in\u548cout layers, \u7136\u540e\u662fskip_connection</li> <li></li> <li>forward\u8fc7\u7a0b\u5c31\u662f\u5148in, \u52a0time embedding, \u662f\u4e00\u4e2achannel\u4e00\u4e2a\u503c, \u5e7f\u64ad\u5230\u8fd9\u4e2achannel\u6240\u6709\u50cf\u7d20, \u7136\u540eout, \u6700\u540eskip</li> <li></li> </ul>"},{"location":"aigc/Stable%20Diffusion/#transformer-for-stable-diffusion","title":"\u2b07\ufe0fTransformer for Stable Diffusion","text":"collapsed:: true<ul> <li>Transformer for Stable Diffusion U-Net</li> <li>Channels\u5c31\u662f\u6bcf\u4e2a\u8bcd\u7684feature dim, \u4e5f\u4f5c\u4e3a\u6a21\u578b\u7684dimension, \u591a\u5934\u6ce8\u610f\u529b\u56e0\u6b64\u6709n_heads, n_layers define the number of transformer layers, d_cond\u5b9a\u4e49\u4e86conditional embedding\u7684\u5927\u5c0f. \u5176\u4e2d\u53c8\u5b9a\u4e49\u4e861x1\u7684\u5377\u79ef\u7528\u6765\u5904\u7406\u8fdb\u5165\u7684tensor, \u4ee5\u53ca\u5806\u780c\u4e86n_layers\u5c42\u7684transformer block, \u6700\u540e\u518d\u52a0\u4e0a\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362, \u53ea\u4e0d\u8fc7\u4e5f\u7528conv\u6765\u8fdb\u884c\u4e86</li> <li></li> <li>Forward\u90e8\u5206, spatialTransformer \u53ea\u5229\u7528\u5230\u4e86\u524d\u4e00\u6b65resblock\u5904\u7406\u5b8c\u7684x\u548ccondition, \u5148\u5bf9x norm \u518d\u7ebf\u6027\u53d8\u6362, \u518d\u628a\u5f62\u72b6\u53d8\u6362\u6210transformer\u80fd\u591f\u5904\u7406\u7684\u5f62\u5f0f[batch, sequence, embedding], \u7136\u540e\u8f93\u5165\u5230transformer_blocks\u4e2d\u7ecf\u8fc7\u591a\u5c42transformer\u5904\u7406, \u6700\u540e\u53d8\u56de\u539f\u6765\u7684\u6a21\u6837, \u5e76\u4e14\u52a0\u4e00\u4e2aresidual</li> <li></li> <li>Transformer layer\u7684basic block\u7528\u7684\u662fcross attention, \u4f46\u662f\u5728\u6ca1\u6709cond\u7684\u60c5\u51b5\u4e0b, \u5c31\u4f1a\u53d8\u6210self-attention. \u8fd9\u4e2abasic block\u7531\u4e24\u4e2aattention\u5c42\u7ec4\u6210, \u7b2c\u4e00\u4e2a\u56fa\u5b9a\u8fdb\u884cself-attention, \u7b2c\u4e8c\u4e2a\u5219\u662f\u4f1a\u8003\u8651\u5230cond, \u505a\u7684\u662fcross-attention, \u6700\u540e\u518d\u6709\u4e00\u4e2aFFN. \u548c\u6700\u666e\u901a\u7684transformer\u975e\u5e38\u7c7b\u4f3c, \u7c7b\u4f3c\u7684\u662fdecoder\u90e8\u5206, \u4e5f\u662f\u4e00\u4e2aselfattention, cross attention, ffn, \u7136\u540e\u6bcf\u4e00\u5c42\u505a\u5b8c\u90fd\u4f1a\u6709add &amp; norm</li> <li></li> <li>CrossAttention\u7684\u5b9a\u4e49\u4e3b\u8981\u8fd8\u662f\u5305\u542b\u4e86\u548c\u4f20\u7edftransformer\u4e00\u6837\u7684\u90e8\u5206, \u6709\u4e09\u4e2aFC\u6765map\u5230qkv. \u5176\u4e2d\u9700\u8981\u6ce8\u610f\u7684\u662fattention\u7684dimension\u662fhead*d_head, \u4e5f\u5c31\u662fmodel dimension</li> <li></li> <li>CrossAttention\u7684forward\u8fc7\u7a0b\u5c31\u662f\u5148\u628ax\u548ccond\u53d8\u6210qkv, \u7136\u540e\u8fdb\u884c\u5e38\u89c4\u7684attention\u64cd\u4f5c, \u8fd9\u91cc\u628aattention\u7684\u5177\u4f53\u8fc7\u7a0b\u5b9a\u4e49\u5230\u4e86\u4e00\u4e2a\u53e6\u5916\u7684\u51fd\u6570\u91cc\u9762, \u5305\u62ecnormal_attention \u548c\u53e6\u4e00\u4e2aflash attention(\u4f1a\u66f4\u5feb\u4e00\u70b9)</li> <li></li> <li>FlashAttention \u4f1a\u6bd4\u8f83\u5feb\u4e00\u70b9</li> <li></li> <li>Normal Attention, \u7531\u4e8e\u4e4b\u524d\u7684qkv\u7684fc map\u5230\u7684\u662fn_heads x d_head\u5927\u5c0f\u7684\u5411\u91cf, \u56e0\u6b64\u8fd9\u91cc\u8981\u628a\u6bcf\u4e2ahead\u7684\u7ed9\u5206\u5f00\u6765, \u4ece [batch_size,\u00a0seq,\u00a0d_attn] -&gt;[batch_size,\u00a0seq_len,\u00a0n_heads,\u00a0d_head], \u7136\u540e\u751f\u6210\u7684attention matrix\u4e5f\u8981\u662fb\u4e2ah\u4e2aseq*seq. softmax\u540e\u4e58v\u5f97\u5230\u6700\u7ec8\u7684seq\u4e2ah\u4e2av, \u8fd9\u91cc\u8fd8\u662fd_attn, \u6700\u540e\u6709\u4e00\u4e2amlp map\u56ded_model\u6765\u4f7f\u5f97\u53ef\u4ee5\u653e\u5230\u4e0b\u4e00\u4e2aattention\u91cc\u9762</li> <li></li> <li>FFN, \u5bf9\u6bcf\u4e2asequence\u91cc\u7684\u8bcd\u4f5c\u540c\u6837\u7684\u4e0a\u4e0b\u53d8\u6362</li> <li></li> </ul>"},{"location":"aigc/Stable%20Diffusion/#base-class-for-sampling-algorithms","title":"\u2b07\ufe0fBase class for sampling algorithms","text":"collapsed:: true<ul> <li>Sampling algorithms for stable diffusion</li> <li>\u4e0b\u9762\u662f\u57fa\u7840Stable Diffusion sampler\u7684\u6784\u9020\u6a21\u5f0f</li> <li></li> <li>\u9996\u5148\u6709\u4e00\u4e2a\u51fd\u6570\u7528\u4e8e\u83b7\u53d6\u6a21\u578b\u4f30\u8ba1\u7684epsilon, \u8f93\u5165\u7684\u662fx, t, c, CFG scale, \u548cuncond\u7684cond\u4f8b\u5982None. x\u548ct\u90fd\u4f1aduplicate\u4e24\u4efd, \u4f20\u5165\u6a21\u578b\u4e2d\u83b7\u53d6con\u7684\u4f30\u8ba1\u7684epsilon\u548cuncon\u4f30\u8ba1\u7684epsilon, \u7136\u540e\u7531CFG\u516c\u5f0f\u5f97\u5230\u6700\u540e\u7684epsilon, \u5373\u6307\u5411x0\u7684\u65b9\u5411.</li> <li></li> <li>Sampling \u548c painting\u7684loop \u5b9a\u4e49\u662f\u4e0b\u56fe\u8fd9\u6837, \u5177\u4f53\u5728DDIM\u4e2d\u6709\u89e3\u91ca\u8fc7</li> <li> </li> </ul>"},{"location":"aigc/Stable%20Diffusion/#_1","title":"Stable Diffusion","text":""},{"location":"aigc/Stable%20Diffusion/#stable-diffusion-code","title":"\u2b07\ufe0fStable Diffusion Code\u7684\u5168\u5c40\u8ba4\u77e5","text":"collapsed:: true<ul> <li>Stable Diffusion\u5728\u5404\u4e2a\u5143\u4ef6\u7684\u5b9e\u73b0\u4e0a\u6709\u4e00\u4e9b\u7ec6\u8282\u4e0a\u7684\u5dee\u5f02</li> <li>\u9700\u8981UNet, AutoEncoder, CLIPTextEmbedder\u8fd9\u51e0\u4e2a\u6a21\u578b;</li> <li>latent scaling factor\u7528\u6765scale encoder encode\u7684encodings, \u518d\u8f93\u5165\u5230UNet\u91cc\u9762</li> <li>Autoencoder \u5f97\u5230\u7684latent z\u4e5f\u662f\u591achannel\u7684, \u4e14\u8bbe\u5b9a\u4e0az\u6709dim_z, \u5b9e\u9645\u4e0a\u4f1a\u7ed9\u52302*dim_z\u7528\u6765\u5206\u522b\u5bb9\u7eb3mean\u548cvariance; \u800c\u4e14\u8fd9\u4e2az\u8fd8\u4e0d\u662f\u6700\u7ec8\u7ed9\u5230\u7684latent embedding, \u8fd8\u8981\u7ecf\u8fc7quantisation\u5230quantised embedding space, \u5e76\u4e14\u8fd8\u8981\u6839\u636e\u8fd9\u4e2aquantised embedding space \u7684mean\u548cvariance\u518dsample \u4e00\u4e2asample, \u8fd9\u4e2a\u624d\u662f\u7528\u6765diffusion\u7684\u4e1c\u897f   collapsed:: true<ul> <li></li> </ul> </li> <li>U-Net\u4e5f\u662f\u4e00\u4e2a\u5927\u6539\u8fc7\u7684\u6a21\u578b. \u4e0eDDPM\u76f8\u4f3c\u7684\u5730\u65b9\u5728\u4e8e, \u4e24\u4e2a\u6a21\u578b\u7684\u7ed3\u6784\u5199\u6cd5\u90fd\u975e\u5e38\u76f8\u4f3c, \u90fd\u662f\u5de6\u534a\u8fb9, \u4e2d\u95f4, \u53f3\u534a\u8fb9(\u51e0\u4e4e\u662f\u5de6\u534a\u8fb9\u7684\u53cd\u8f6c\u7248). \u6bcf\u4e2a\u534a\u8fb9\u7684\u5faa\u73af\u90fd\u662fresolution level\u4e3a\u4e3b\u8981\u5faa\u73af, \u6bcf\u4e2a\u5faa\u73af\u91cc\u6709\u5b9a\u4e49\u6b21\u6570\u7684resblock, \u5982\u679c\u542f\u7528\u4e86attention\u7684\u8bdd\u5c31\u4f1a\u5728\u6bcf\u4e2aresblock\u540e\u9762\u7d27\u8ddf\u7740\u4e00\u4e2aattention\u6a21\u5757, \u53ea\u4e0d\u8fc7\u4e24\u4e2a\u7248\u672c\u7684attention\u6a21\u5757\u6709\u70b9\u4e0d\u4e00\u6837. DDPM\u7684attention\u662f\u5b8c\u5168\u7684selfattention, \u6bcf\u4e2a\u50cf\u7d20\u901a\u9053\u4f5c\u4e3a\u4e00\u4e2a\u8bcd, \u8bcd\u5411\u91cf\u957f\u5ea6\u5c31\u662fchannel\u6570, \u50cf\u7d20\u4eec\u4f5cself-attention. \u800cSD\u4e2d, attention\u5316\u8eab\u4e3a\u4e86spatialTransformer, \u7c7b\u4f3c\u4e8e\u4f20\u7edf\u7684transformer decoder, \u4f5c\u4e3a\u4e00\u6574\u4e2a\u6a21\u5757\u51fa\u73b0, \u800c\u4e0d\u662f\u5355\u5355\u53ea\u662f\u505a\u4e00\u6b21attention\u5c31\u7ed3\u675f\u4e86, \u5305\u542b\u4e86\u4e00\u6b21self-attention, \u4e00\u6b21cross-attention(\u4e0econd\u505a), \u4ee5\u53ca\u4e00\u4e2aFFN, \u57fa\u672c\u4e0a\u548ctransformer decoder\u4e00\u6a21\u4e00\u6837   collapsed:: true<ul> <li></li> </ul> </li> <li>Autoencoder\u53ea\u662f\u7528\u6765\u8f6c\u6362\u6210\u6f5c\u5728\u7a7a\u95f4, U-Net\u662f\u7528\u6765\u9884\u6d4bepsilon, \u771f\u6b63sampling\u5730\u65f6\u5019\u662f\u53ef\u4ee5\u4f7f\u7528\u4e0d\u540c\u7684sampler\u7684, sampler\u4f1a\u4ece\u8bad\u7ec3\u597d\u7684\u6a21\u578b(LatentDiffusion model, \u5305\u542b\u4e86\u5b9a\u4e49\u597d\u7684\u603bdiffusion\u6b65\u6570, beta, alpha\u7b49\u53c2\u6570, \u4ee5\u53ca\u4e09\u4e2a\u4e3b\u8981\u6a21\u578bunet, autoencoder, clip)\u5904\u5f97\u5230\u5b9a\u4e49\u597d\u7684\u603bdiffusion\u6b65\u6570, beta, alpha\u7b49\u53c2\u6570, \u518d\u7528\u5404\u4e2asampler\u7684\u7b97\u6cd5\u5982DDPM, DDIM\u7b49\u6765\u8fdb\u884csample</li> </ul>"},{"location":"aigc/Stable%20Diffusion/#text2img","title":"\u2b07\ufe0fText2Img","text":"collapsed:: true<ul> <li>Generate images using stable diffusion with a prompt</li> <li>\u8bfb\u5165\u6a21\u578b, \u548csampler. \u7528\u4e86utils\u91cc\u9762\u7684load_model\u51fd\u6570, \u8fd9\u4e2a\u51fd\u6570\u91cc\u9762\u52a0\u8f7d\u4e86\u6240\u6709\u9700\u8981\u7684sub model\u6bd4\u5982\u8bf4autoencoder\u7684encoder\u548cdecoder\u7c7b, clip\u548cunet, \u7136\u540e\u5b9e\u4f8b\u5316\u4e86\u4e00\u4e2alatentDiffusion model\u5176\u4e2d\u5305\u542b\u4e86\u8fd9\u4e9b\u90e8\u5206, \u52a0\u8f7d\u7684statedict\u4e5f\u662f\u5305\u542b\u4e86\u8fd9\u4e9b\u5b50\u6a21\u578b\u7684\u53c2\u6570\u7684</li> <li></li> <li>call\u7684\u65f6\u5019\u9700\u8981\u7ed9\u5230 \u6587\u4ef6\u8def\u5f84, \u4e00\u6b21\u751f\u6210\u591a\u5c11\u5f20\u56fe\u7247(batch_size), prompt, \u4ee5\u53ca\u56fe\u7247\u5bbd\u9ad8\u6570\u636e\u548cCFG scale. \u6709\u591a\u5c11batch\u5c31copy\u591a\u5c11\u4e2aprompt\u6765\u91cd\u590d\u751f\u6210, CFG scale\u4e5f\u5728\u8fd9\u91cc\u5904\u7406\u5230, \u5927\u4e8e1\u4f1a\u6709\u5f3aguidance, \u7b49\u4e8e1\u7684\u65f6\u5019\u5c31\u662f\u5355\u5355cond, uncon\u8fd9\u91cc\u662f\u201c\u201d. \u7528sampler sample\u5c31\u5f97\u5230\u4e86latent space\u7684\u53bb\u566a\u7ed3\u679c. \u8fd9\u91cc\u6ce8\u610f\u6709\u4e00\u4e2a\u5f88\u795e\u5947\u7684\u5730\u65b9\u5c31\u662ff\u662fImage to latent space resolution reduction, \u4e5f\u5c31\u662f\u8bf4\u6211\u4eec\u63d0\u524d\u77e5\u9053\u4e86img\u5230latent\u4f1a\u6709\u516b\u500d\u7f29\u653e, \u6240\u4ee5\u6211\u4eec\u7528\u6765sample\u5730noise\u5c31\u662f\u957f\u5bbd\u9664\u4ee5f\u7684\u5927\u5c0f, autoencoder\u4f1a\u8d1f\u8d23\u628a1/8\u7684latent\u6062\u590d\u5230\u539f\u59cb\u7684\u5c3a\u5bf8, \u662f\u81ea\u9002\u5e94\u7684, \u7531\u4e8e\u662f\u5168\u5377\u79ef, autoencoder\u548cunet\u90fd\u662f\u81ea\u9002\u5e94\u7684</li> <li></li> <li>\u4e3b\u51fd\u6570</li> <li></li> </ul>"},{"location":"aigc/Stable%20Diffusion/#img2img","title":"\u2b07\ufe0fImg2Img","text":"collapsed:: true<ul> <li>Generate images using stable diffusion with a prompt from a given image</li> <li>\u4e0e\u4e0a\u9762\u7c7b\u4f3c\u7684\u521d\u59cb\u5316</li> <li></li> <li>call\u4e2d\u589e\u52a0\u4e86original image\u548cstrength, \u4e5f\u5c31\u662f\u4e00\u4e2a0-1\u7684\u6bd4\u4f8b, \u8981\u4ecediffusion\u7684\u7b2c\u51e0\u6b65\u5f00\u59cbdenoise \u52a0\u566a\u7684\u539f\u56fe, \u4e0e\u4e4b\u524d\u7684\u533a\u522b\u5728\u4e8e, \u8981\u7528q_sample\u83b7\u53d6\u4e00\u4e2a\u521d\u59cb\u7684noised original image, \u7136\u540e\u628a\u5b83\u653e\u5230paint\u91cc\u9762sample</li> <li></li> <li>\u4e3b\u51fd\u6570</li> <li></li> </ul>"},{"location":"aigc/Stable%20Diffusion/#impaint","title":"\u2b07\ufe0fImpaint","text":"collapsed:: true<ul> <li>In-paint images using stable diffusion with a prompt</li> <li>\u5728i2i\u7684\u57fa\u7840\u4e0a\u591a\u4e00\u4e2amask\u4f20\u5165paint\u51fd\u6570</li> <li></li> </ul>"},{"location":"aigc/Stable%20Diffusion/#utils","title":"\u2b07\ufe0fUtils","text":"collapsed:: true<ul> <li> <p><code>def load_model(path: Path = None) -&gt; LatentDiffusion:</code> \u5b9e\u4f8b\u5316\u51e0\u4e2a\u6a21\u578b, \u8f7d\u5165weights         -          - ```python   def load_img(path: str):       \"\"\"       ### Load an image</p> <p>This loads an image from a file and returns a PyTorch tensor.</p> <p>:param path: is the path of the image   \"\"\"   # Open Image   image = Image.open(path).convert(\"RGB\")   # Get image size   w, h = image.size   # Resize to a multiple of 32   w = w - w % 32   h = h - h % 32   image = image.resize((w, h), resample=PIL.Image.LANCZOS)   # Convert to numpy and map to <code>[-1, 1]</code> for <code>[0, 255]</code>   image = np.array(image).astype(np.float32) * (2. / 255.0) - 1   # Transpose to shape <code>[batch_size, channels, height, width]</code>   image = image[None].transpose(0, 3, 1, 2)   # Convert to torch   return torch.from_numpy(image)           <code>-</code>python           def save_images(images: torch.Tensor, dest_path: str, prefix: str = '', img_format: str = 'jpeg'):   \"\"\"   ### Save a images</p> <p>:param images: is the tensor with images of shape <code>[batch_size, channels, height, width]</code>   :param dest_path: is the folder to save images in   :param prefix: is the prefix to add to file names   :param img_format: is the image format   \"\"\"</p> <p># Create the destination folder   os.makedirs(dest_path, exist_ok=True)</p> <p># Map images to <code>[0, 1]</code> space and clip   images = torch.clamp((images + 1.0) / 2.0, min=0.0, max=1.0)   # Transpose to <code>[batch_size, height, width, channels]</code> and convert to numpy   images = images.cpu().permute(0, 2, 3, 1).numpy()</p> <p># Save images   for i, img in enumerate(images):       img = Image.fromarray((255. * img).astype(np.uint8))       img.save(os.path.join(dest_path, f\"{prefix}{i:05}.{img_format}\"), format=img_format)           ``` - Explained:     - Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models | ML Coding Series - YouTube     - Stable Diffusion Clearly Explained! | by Steins | Medium - Info   collapsed:: true     - \u4e00\u5f00\u59cb\u88ab\u79f0\u4e3aLatent Diffusion Models. use an auto-encoder to map between image space and latent space. The diffusion model works on the latent space, which makes it a lot easier to train.     - \u81ea\u7f16\u7801\u5668AutoEncoder\u548c\u6587\u5b57\u7f16\u7801\u5668CLIP\u90fd\u662f\u9884\u8bad\u7ec3\u597d\u7684, \u4e14\u662f\u914d\u5957\u7684, \u5176\u4e2d\u7684U-Net with attention\u9700\u8981\u73b0\u573a\u8bad\u7ec3     - \u2b07\ufe0f\u539f\u56fe\u5c55\u5f00\u53ef\u89c1       collapsed:: true         -          -     - \u548c\u4e4b\u524ddiffusion models \u7684\u4e0d\u540c\u5728\u4e8e, diffusion\u8fc7\u7a0b\u5728latent space\u4e2d\u8fdb\u884c, \u800c\u4e4b\u524dDDPM\u662f\u5728pixel space\u4e2d\u8fdb\u884c\u7684. \u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42. \u5148\u5c06x encode\u5230latent space, \u6240\u6709\u7684diffusion \u524d\u5411\u548c\u540e\u5411\u8fc7\u7a0b\u90fd\u662f\u5728latent space\u4e2d\u8fdb\u884c\u7684         - \u95ee\u9898: \u7f16\u7801\u548c\u89e3\u7801\u662f\u7528\u4ec0\u4e48\u6a21\u578b\u505a\u7684, \u662f\u4e0d\u662f\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u6a21\u578b         - \u7b54\u6848: \u662f\u7684, VAE\u662f\u9488\u5bf9\u7279\u5b9a\u6570\u636e\u96c6\u8bad\u7ec3\u7684. \u7279\u5b9a\u6a21\u578b\u9700\u8981\u7279\u5b9a\u7684VAE     - ZT\u548c\u4e00\u4e9bconditioning\u7684\u4e1c\u897f, \u4f8b\u5982\u8bf4text, images\u8fd9\u4e9b\u7f16\u7801\u540e\u7684\u5411\u91cf\u4e00\u8d77\u4f5c\u4e3adenoising\u7684\u8f93\u5165, \u8f93\u5165\u5230Denoising U-Net\u4e2d.     - \u8fd9\u91cc\u7684Denoising U-Net\u662f\u4e13\u95e8\u8bbe\u8ba1\u7684\u5e26\u6709attention\u7684\u7c7btransformer\u7684U-Net. ZT\u5404\u9636\u6bb5\u5e94\u8be5\u4f1a\u4f5c\u4e3aQ, \u4e00\u65b9\u9762\u88abU-Net up down, \u4e00\u65b9\u9762\u4e0econditioning \u8fdb\u884ccross attention \u64cd\u4f5c, \u6765\u83b7\u53d6condition\u7684\u4fe1\u606f, \u5bf9\u8f93\u51fa\u52a0\u4ee5\u9650\u5236     - \u95ee\u9898: How img2img diffusion works         - \u7b54\u6848: \u4f8b\u598250\u6b65\u7684inference steps, Image/Noise Strength Parameter \u4f1a\u63a7\u5236\u52a0\u767e\u5206\u4e4b\u591a\u5c11\u7684noise\u5230\u539f\u56fe\u4e2d, \u4f8b\u59820.8\u5c31\u662f\u52a040\u6b65\u7684noise\u5230\u539f\u56fe\u4e2d, \u7136\u540einference\u5269\u4f59\u768440\u6b65         - \u6ce8\u610f, 1\u4ec5\u4ec5\u8868\u793a\u52a0\u4e8650\u6b65noise, \u5e76\u4e0d\u4f1a\u5b8c\u5168\u4e0d\u4fdd\u7559\u539f\u56fe\u7684\u4fe1\u606f         - Reference: How img2img Diffusion Works \u00b7 Chris McCormick     - - WebUI     - GitHub - AUTOMATIC1111/stable-diffusion-webui: Stable Diffusion web UI     - Home \u00b7 AUTOMATIC1111/stable-diffusion-webui Wiki \u00b7 GitHub #GitHub #\u5b89\u88c5 #SD #WebUI     - GitHub - Mikubill/sd-webui-controlnet: WebUI extension for ControlNet       id:: 64526dd6-022b-42cd-bdb3-3844bc23ae3e     - Prompt       collapsed:: true         -          - \u6b63\u5411\u63d0\u793a\u8bed\uff1a           collapsed:: true - \u4e07\u80fd\u753b\u8d28\u8981\u6c42   (masterpiece, best quality),         - \u53cd\u5411\u63d0\u793a\u8bed\uff1a           collapsed:: true - \u907f\u514d\u7cdf\u7cd5\u4eba\u50cf\u7684   ugly, fat, obese, chubby, (((deformed))), [blurry], bad anatomy,disfigured, poorly drawn face, mutation, mutated, (extra_limb),(ugly), (poorly drawn hands fingers), messy drawing, morbid,mutilated, tranny, trans, trannsexual, [out of frame], (bad proportions),(poorly drawn body), (poorly drawn legs), worst quality, low quality,normal quality, text, censored, gown, latex, pencil,</p> <p>\u907f\u514d\u751f\u6210\u6c34\u5370\u548c\u6587\u5b57\u5185\u5bb9   lowres, bad anatomy, bad hands, text, error, missing fingers,extra digit, fewer digits, cropped, worst quality, low quality,normal quality, jpeg artifacts, signature, watermark, username, blurry,</p> <p>\u901a\u7528   lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry,</p> <p>\u907f\u514d\u53d8\u5f62\u7684\u624b\u548c\u591a\u4f59\u7684\u624b   extra fingers,fused fingers,too many fingers,mutated hands,malformed limbs,extra limbs,missing arms,poorly drawn hands,     - Sampler       collapsed:: true         -  - \u6559\u7a0b     - Home \u00b7 AUTOMATIC1111/stable-diffusion-webui Wiki \u00b7 GitHub     - \u3010SD\u6559\u7a0b\u00b7Stable Diffusion\u672c\u5730\u90e8\u7f72\u6559\u5b66 \u5b98\u65b9WebUI-\u54d4\u54e9\u54d4\u54e9\u3011 SD\u6559\u7a0b\u00b7Stable Diffusion\u672c\u5730\u90e8\u7f72\u6559\u5b66 \u5b98\u65b9WebUI_\u54d4\u54e9\u54d4\u54e9_bilibili     - https://mp.weixin.qq.com/s/8czNX-pXyOeFDFhs2fo7HA     - \uff08\u4e0a\uff09\u3010\u4e00\u6587\u638c\u63e1Al\u7ed8\u56fe\u3011Stable Diffusion \u7ed8\u56fe\u5582\u996d\u6559\u7a0b     - \uff08\u4e0b)\u3010\u4e00\u6587\u638c\u63e1A\u7ed8\u56fe\u3011Stable Diffusion \u7ed8\u56fe\u5582\u996d\u6559\u7a0b     - Home \u00b7 AUTOMATIC1111/stable-diffusion-webui Wiki \u00b7 GitHub - Training     - Train a diffusion model     - ControlNet     - Create a dataset for training     - - \u6a21\u578b   collapsed:: true     - Models - Hugging Face     - AI\u7ed8\u753b\u6a21\u578b\u535a\u7269\u9986     - SD - WebUI \u8d44\u6e90\u7ad9     - Civitai | Stable Diffusion models, embeddings, LoRAs and more     - model         - CompVis/stable-diffusion-v-1-4-original \u00b7 Hugging Face         - runwayml/stable-diffusion-v1-5 \u00b7 Hugging Face         - stabilityai/stable-diffusion-2 \u00b7 Hugging Face         - model         - coreml (Core ML Models) Apple\u4f18\u5316\u7684\u6a21\u578b     - VAE         - stabilityai/sd-vae-ft-ema-original \u00b7 Hugging Face     - embedding         - GitHub - autumn-moon-py/aimodel-embeddings: \u4e00\u4e2a\u6536\u96c6embeddings\u7684\u4ed3\u5e93     - lora         - lora     - controlnet         - GitHub - Mikubill/sd-webui-controlnet: WebUI extension for ControlNet         - GitHub - lllyasviel/ControlNet: Let us control diffusion models!         - lllyasviel/ControlNet-v1-1 at main         - TencentARC/T2I-Adapter at main     - Upscale ([[ESRGAN]])         - Model Database - Upscale Wiki     - Tiled \u4f4e\u663e\u5b58\u751f\u6210\u9ad8\u5206\u8fa8\u7387         - GitHub - pkuliyi2015/multidiffusion-upscaler-for-automatic1111: Tiled Diffusion and VAE optimize, licensed under CC BY-NC-SA 4.0 - \u5176\u4ed6\u5de5\u5177\u548c\u8d44\u6e90     - GitHub - Akegarasu/stable-diffusion-inspector: read pnginfo in stable diffusion generated images / inspect models #StableDiffusion #Tool #GitHub #Prompt \u8bfb\u53d6png\u7684\u4fe1\u606f         - Stable Diffusion \u6cd5\u672f\u89e3\u6790     - GitHub - pharmapsychotic/clip-interrogator: Image to prompt with BLIP and CLIP #StableDiffusion #Tool #GitHub #Prompt         - methexis-inc/img2prompt \u2013 Run with an API on Replicate     - Stable Diffusion prompt Generator - promptoMANIA     - https://github.com/camenduru/controlnet-colab     - https://github.com/camenduru/stable-diffusion-webui-colab     - GitHub - godly-devotion/MochiDiffusion: Run Stable Diffusion on Mac natively #GitHub #SD #Mac #app/\u521b\u4f5c\u8f6f\u4ef6     - https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/fast-kohya-trainer.ipynb#scrollTo=gZ1WxKjp_1-L -</p> </li> </ul>"},{"location":"crypto/","title":"Crypto","text":""},{"location":"crypto/Cryptography%20Engineering/","title":"Cryptography Engineering","text":"<p>public:: true tags:: IC, Security, Course, Uni-S10 alias:: CE Ratio: 8:2 Time: \u661f\u671f\u4e00 9:00 - 11:00</p> <p>- - ## Notes</p> <p>Template:: Module Notes     - \u672f\u8bed\u8868       collapsed:: true         - [[Shift cipher]] (THE CAESAR CIPHER)           collapsed:: true             - \u7edf\u4e00\u5f80\u540e\u79fb\u52a80-25\u4f4d         - [[Substitution cipher]] (MONOALPHABETIC SUBSTITUTION)           collapsed:: true             - 26\u4e2a\u5b57\u6bcd\u6253\u4e71\u66ff\u6362\u5bf9\u5e94\u5173\u7cfb         - [[Vigenere cipher]] (POLYALPHABETIC SUBSTITUTION)           collapsed:: true             - key set\u662f\u5404\u79cd\u539f\u5b57\u6bcd\u8868\u7684permutation, \u7528\u591a\u4e2akey\u6765\u8f6e\u6d41\u8fdb\u884c\u66ff\u6362\u52a0\u5bc6, \u76f8\u540c\u5b57\u6bcd\u53ef\u4ee5\u88ab\u66ff\u6362\u6210\u4e0d\u540c\u7684\u5bc6\u6587         - {{embed ((63c2c66c-9ab2-45a2-a347-e981592ee698))}}         - Kerckhoffs\u2019s principle is that a cryptographic system must be secure even if everything is known about the system with the exception of the secret key.         - DES: Data Encryption Standard           collapsed:: true             - 64-bit data blocks, 56-bit keys (8 bits for redundancy)         - AES: Advanced Encryption Standard           collapsed:: true             - 128-bit data blocks, key size of 128, 192, 256 bits         - ECB Electronic Code Book \u7535\u5b50\u5bc6\u7801\u672c\u6a21\u5f0f           collapsed:: true             - \u76f8\u540c\u660e\u6587\u52a0\u5bc6\u5230\u76f8\u540c\u5bc6\u6587         - CBC Cipher Block Chaining \u5bc6\u7801\u5206\u7ec4\u94fe\u63a5\u6a21\u5f0f           collapsed:: true             - \u524d\u4e00\u4e2a\u5bc6\u6587\u4e0e\u5f53\u524d\u660e\u6587XOR\u540e\u7684\u7ed3\u679c\u52a0\u5bc6         - CFB Cipher FeedBack \u5bc6\u6587\u53cd\u9988\u6a21\u5f0f           collapsed:: true             - \u524d\u4e00\u4e2a\u5bc6\u6587\u52a0\u5bc6\u7ed3\u679c\u4e0e\u660e\u6587XOR         - OFB Output FeedBack \u8f93\u51fa\u53cd\u9988\u6a21\u5f0f           collapsed:: true             - \u521d\u59cb\u5411\u91cf\u4e0d\u65ad\u52a0\u5bc6\u5f62\u6210\u5bc6\u94a5\u5e8f\u5217\u4e0e\u5bf9\u5e94\u660e\u6587XOR         - CTR counTeR \u8ba1\u6570\u5668\u6a21\u5f0f           collapsed:: true             - \u521d\u59cb\u5411\u91cf\u4e0d\u65ad\u7d2f\u52a0\u540e\u52a0\u5bc6, \u4e0e\u5bf9\u5e94\u660e\u6587XOR         - Collision resistance\u00a0\uff1a\u6709\u4eba\u627e\u5230\u4e24\u6761\u6563\u5217\u76f8\u540c\u7684\u6d88\u606f\uff08\u4efb\u610f\u4e24\u6761\u6d88\u606f\uff09\u6709\u591a\u96be .         - Preimage Resistance\u00a0\uff1a\u7ed9\u5b9a\u54c8\u5e0c\u503c\uff0c\u627e\u5230\u53e6\u4e00\u4e2a\u54c8\u5e0c\u76f8\u540c\u7684\u6d88\u606f\u6709\u591a\u96be\uff1f\u4e5f\u79f0\u4e3a\u5355\u5411\u6563\u5217\u51fd\u6570 .         - Second preimage resistance\u00a0\uff1a\u7ed9\u5b9a\u4e00\u6761\u6d88\u606f\uff0c\u627e\u5230\u53e6\u4e00\u6761\u6563\u5217\u76f8\u540c\u7684\u6d88\u606f .         - {{embed ((63ce8f73-25c8-48cc-817a-d17069e19ad3))}}         - Linear Cryptanalysis (\u7ebf\u6027\u5206\u6790): approximate all non-linear components with linear functions; for example the S-boxes of DES; then do a probabilistic analysis about the key. \u5c06\u660e \u6587\u548c\u5bc6\u6587\u7684\u4e00\u4e9b\u5bf9\u5e94\u6bd4\u7279\u8fdb\u884cXOR\u5e76\u8ba1\u7b97\u5176\u7ed3\u679c\u4e3a\u96f6\u7684\u6982\u7387\u5982\u679c\u5bc6\u6587\u5177\u5907\u8db3\u591f\u7684\u968f\u673a\u6027\uff0c\u5219\u4efb\u9009\u4e00\u4e9b\u660e\u6587\u548c\u5bc6\u6587\u7684\u5bf9\u5e94\u6bd4\u7279\u8fdb\u884c XOR \u7ed3\u679c\u4e3a\u96f6\u7684\u6982\u7387\u5e94\u8be5\u4e3a1/2\u3002 \u5982\u679c\u80fd\u591f\u627e\u5230\u5927\u5e45\u504f\u79bb1/2\u7684\u90e8\u5206\uff0c\u5219\u53ef\u4ee5\u501f\u6b64\u83b7\u5f97\u4e00\u4e9b\u4e0e\u5bc6\u94a5\u6709\u5173\u7684\u4fe1\u606f\u3002 \u4f7f\u7528\u7ebf\u6027\u5206\u6790\u6cd5\uff0c\u5bf9\u5343 DES \u53ea\u9700\u8981 2^\u200647\u7ec4\u660e\u6587\u548c \u5bc6\u6587\u5c31\u80fd\u591f\u5b8c\u6210\u7834\u89e3\uff0c\u76f8\u6bd4\u9700\u8981\u5c1d\u8bd5 2^56\u4e2a\u5bc6\u94a5\u7684\u66b4\u529b\u7834\u89e3\u6765\u8bf4\uff0c\u6240\u9700\u7684\u8ba1\u7b97\u5740\u5f97\u5230\u4e86\u5927\u5e45\u51cf\u5c11\u3002         - Integrity refers to that data can only be modified by authorized parties or that unauthorized modifications of data are detectable         - Availability means that we can access data when we need it or will receive it across a network within the expectations set out in service-level agreements         - Nonce: \u4e34\u65f6\u9020\u7684\u6570\u5b57, \u53ea\u7528\u4e00\u6b21, \u4f46\u662f\u751f\u6210\u65b9\u5f0f\u5e76\u4e0d\u4e00\u5b9a\u662frandom\u7684         - Semantic Security: a\u00a0semantically secure cryptosystem\u00a0is one where only negligible information about the\u00a0plaintext\u00a0can be feasibly extracted from the\u00a0ciphertext.         - Avalanche effect desired: small changes in the input should result in           unpredictable changes in the output.         - Horton\u2019s Principle states that it is wise to sign what is meant and not what is being said. but not in blinding protocols for anonymity solutions.         - Abelian group: Commutative group, x+y = y+x         - QKE: quantum-key-exchange         - qubit: quantum bit         - BB84: Quantum key exchange protocol, Invented by Charles Bennett and Gilles Brassard in 1884         - information-theoretic security: attacker with infinite computing power           cannot break the scheme         - computational security: attacker with specified computation resources           (e.g. probabilistic polynomial time) cannot break the scheme     - \u7b97\u6cd5\u8868       collapsed:: true         - DES: Data Encryption Standard, Symmetri ccryptosystem         - AES: Advanced Encryption Standard, Symmetri ccryptosystem         - SHA: Secure Hash Algorithm         - RSA: public-key cryptosystem that is widely used for secure data transmission. It is also one of the oldest. The acronym \"RSA\" comes from the surnames of Ron Rivest, Adi Shamir and Leonard Adleman         - MAC: Message Authentication Code         - HMAC: Hash Message Authentication Code         - ECC: Elliptic Curve Cryptography         - DH: Diffie-Hellman key exchange         - ECDH: Elliptic Curve Diffie-Hellman key exchange         - ECDHE: Elliptic Curve Diffie-Hellman Ephemeral         - ECDHE-RSA: Elliptic Curve Diffie-Hellman Ephemeral key exchange, signed by RSA         - ECDHE-RSA-AES128-GCM-SHA256: using AES128 \u52a0\u5bc6, GCM mode, hash\u91c7\u7528SHA256         - DSA: Digital Signature Algorithm         - ECDSA: Elliptic Curve Digital Signature Algorithm     - Week1 Mathematics       collapsed:: true         - ((63c55d15-f4b2-474b-870d-f5cf1db87fbd)) Addition modulo n           collapsed:: true             - \\(a +_n b = (a +b)(mod\\ n)\\)             - \\(inv_n: Z_n -&gt; Z_n\\)             - \\(inv_n(a) = \u2212a(mod\\ n) = n - a(mod\\ n)\\)             - \u56e0\u4e3a\u662fmod n\u64cd\u4f5c, \u6240\u4ee5\u5b9a\u4e49\u57df\u548c\u503c\u57df\u90fd\u5f97\u662fn             - inv\u7684\u7ed3\u679c\u548c\u539f\u503c\u8fdb\u884c\u4e86\u8be5group\u7684operation\u4ee5\u540e\u4f1a\u662f\u4e00\u4e2a\u56fa\u5b9a\u7684element e             - \u4f8b\u5982: \\(12 +_13 inv_13 (12) = (12 + 13-12(mod n)) (mod 13) = 12 + 1 (mod 13) = 0\\)             - \u5206\u6570\u7684modulo:               collapsed:: true                 - \\(5/2 mod 7 = 5 * 2^{-1}\\ mod\\ 7 = 20 mod 7 = 6\\)                 - 2n = 5 mod 7; n=6         - ((63c56058-2cf2-46a3-979f-7c9225796653))           collapsed:: true             - ((63c56080-b507-4555-9937-a1678ad09ddd))             - \u6570\u5b66\u7fa4\u5305\u542b\u4e86\u4e00\u4e2a\u96c6\u5408, \u4e00\u4e2a\u56fa\u5b9a\u7684\u5143\u7d20(\u4e0einv\u4e2d\u548c\u540e\u7684\u56fa\u5b9a\u503c), operation, \u4ee5\u53ca\u4e00\u4e2ainverse\u51fd\u6570.             - Commutative (Abelian) group\u662f\u4ea4\u6362\u7fa4, \u6216\u8005\u963f\u8d1d\u5c14\u7fa4, \u9700\u8981\u6ee1\u8db3\u4ea4\u6362\u5f8b\u548c\u7ed3\u5408\u5f8b, \u4f8b\u5982\u52a0\u6cd5\u548c\u4e58\u6cd5\u662f\u5173\u4e8e0 \u548c 1\u7684\u4ea4\u6362\u7fa4         - ((63c56519-6dd1-4516-baf4-57776e6fcc61))           collapsed:: true             - ((63c56536-1a43-47a1-b55e-5cdb50728d3f))             - \u7fa4\u540c\u6001, \u901a\u8fc7\u4e00\u4e2amapping\u51fd\u6570, \u4eceG\u5230G\u2018, \u8fd9\u4e2amapping\u4e0d\u7ba1\u662f\u5bf9\u67d0\u4e24\u4e2a\u6570\u7684\u64cd\u4f5c\u7ed3\u679c\u8fdb\u884cmapping\u8fd8\u662f\u5148map\u4e24\u4e2a\u6570\u5230\u65b0\u7684\u57df\u518d\u8fdb\u884c\u65b0\u7684\u64cd\u4f5c, \u90fd\u80fd\u5f97\u5230\u4e00\u6837\u7684\u7ed3\u679c             -              - ((63c56645-dd64-4330-bc3d-22562f08e41a))             - \u4f8b\u5982modulo\u64cd\u4f5c, \u4e24\u4e2a\u6570\u52a0\u8d77\u6765mod n, \u548c\u4e24\u4e2a\u6570\u5206\u522bmod n\u518d\u52a0\u8d77\u6765\u7684\u7ed3\u679cmod n, \u662f\u4e00\u6837\u7684             -              - \u6709\u4e00\u4e2a\u540c\u6a21\u7684\u6982\u5ff5, \u7528a = b mod N \u8868\u793aa mod n = b mod n     - Week2 Ch 7 &amp; 9 Historical ciphers, Perfect secrecy       collapsed:: true         - Slides             - Aims of [[Cryptography]]               collapsed:: true                 - CIA #\u672f\u8bed\u5361                   id:: 63c2c66c-9ab2-45a2-a347-e981592ee698                   collapsed:: true                     - [[Confidentiality]]: \u673a\u5bc6\u6027, \u4fdd\u8bc1\u4fe1\u606f\u53ea\u88ab\u9700\u8981\u77e5\u9053\u7684\u4eba\u83b7\u53d6                       id:: 63c2c6c0-f7b0-4706-8ce0-b7b3dafd7d25                       collapsed:: true                         - The prevention of unauthorised users reading                           sensitive (private, secret) information                     - [[Integrity]]: \u5b8c\u6574\u6027, \u65e2\u6307\u6d88\u606f\u4f20\u9012\u4e2d\u4fdd\u8bc1\u5b8c\u6574\u4e0d\u53d8, \u4e5f\u6307\u4fdd\u8bc1\u53d1\u9001\u8005\u7684\u8eab\u4efd                       id:: 63c2c6d2-3b91-4091-a6fa-6b7ecb513f11                       collapsed:: true                         - The prevention of unauthorised modification of data, and                           the assurance that data remains unmodified                     - [[Availability]]: \u53ef\u8bbf\u95ee\u6027, \u9632\u6b62\u672a\u6388\u6743\u7684\u4fe1\u606f\u62d2\u7edd\u8bbf\u95ee,\u5373\u4fbf\u7cfb\u7edf\u88ab\u653b\u51fb, \u4f9d\u65e7\u6709\u529e\u6cd5\u4fdd\u8bc1\u6700\u4f4e\u9650\u5ea6\u7684function, \u80fd\u591f\u4fdd\u8bc1\u4fe1\u606f\u88ab\u83b7\u53d6                       id:: 63c2c6d9-3f3e-48bb-996e-6d56e7ed0cc8                       collapsed:: true                         - The property of being accessible and useable upon demand by                           an authorised entity             - Symmetric Cryptosystems Definition Basic Examples               collapsed:: true                 - [[Symmetric Encryption]]                   collapsed:: true                     - m: plaintext messages                       k: key                       c: ciphertext                     - e: encryption function to encrypt plaintext m using key k                       d: decryption function to decrypt ciphertext c using key k                     - Kerckhoffs\u2019s principle is that a cryptographic system must be secure even if everything is known about the system with the exception of the secret key.                     - \u6839\u636e\u79d1\u8003\u592b\u539f\u5219, d and e\u662f\u516c\u5f00\u7684, c\u4e5f\u662f\u53ef\u4ee5\u88ab\u516c\u5f00\u7684, m\u7684\u673a\u5bc6\u6027\u53d6\u51b3\u4e8ek, \u53ea\u6709k\u548cm\u672c\u8eab\u662f\u4e0d\u5e94\u8be5\u88ab\u83b7\u53d6\u7684                 - Examples #[[Symmetric Encryption]]                   collapsed:: true                     - [[Shift cipher]] (THE CAESAR CIPHER)                       collapsed:: true                         - An early substitution cipher, we replace each letter of plaintext with a shifted letter further down the cyclic alphabet, \u4f8b\u5982key=4(E), \u5c31\u662f\u628a\u5b57\u6bcd\u987a\u79fb4\u4f4d, a-&gt;e, b-&gt;f, ...                         - Vulnerable to frequency analysis                           collapsed:: true                             - The frequency of occurrences of each character are very consistent across the same language                             - The longer the ciphertext, the easier this becomes                     - [[Substitution cipher]] (MONOALPHABETIC SUBSTITUTION)                       collapsed:: true                         - \u6bcf\u4e2a\u5b57\u6bcd\u6362\u4e00\u4e2a\u5b57\u6bcd\u66ff\u4ee3                         - a key is used to alter the cipher alphabet, the key is a permutation of original alphabet; key space is 26! 26\u4e2a\u5b57\u6bcd\u7684\u6392\u5217                         - vulnerable to frequency analysis                     - [[Vigenere cipher]] (POLYALPHABETIC SUBSTITUTION)                       collapsed:: true                         - key set\u662f\u5404\u79cd\u539f\u5b57\u6bcd\u8868\u7684permutation, \u7528\u591a\u4e2akey\u6765\u8f6e\u6d41\u8fdb\u884c\u66ff\u6362\u52a0\u5bc6, \u76f8\u540c\u5b57\u6bcd\u53ef\u4ee5\u88ab\u66ff\u6362\u6210\u4e0d\u540c\u7684\u5bc6\u6587                         - thus a key k is a tuple (k1,k2,...,kp) of p &gt; 1 many permutations of set {a,b,...,z}, \u771f\u6b63\u7528\u5230\u7684key\u662fkeyset\u4e2d\u9009\u62e9\u4e86p\u79cdpermutation, \u7136\u540e\u4f9d\u6b21apply\u5230\u6bcf\u4e2a\u5b57\u6bcd\u4e0a                         - for example, we have e(k1,k2)(hello) = shljv for permutations k1 and k2 satisfying k1(h) = s, k2(e) = h, k1(l) = l, k2(l) = j, and k1(o) = v; \u8fd9\u4e2a\u4f8b\u5b50\u4e2d\u7684key\u7528\u4e86\u4e24\u4f4d, \u6240\u4ee5hello\u4e2d\u7684\u4e24\u4e2al\u88ab\u4e0d\u540c\u7684key\u52a0\u5bc6, \u83b7\u5f97\u4e86\u4e0d\u540c\u7684ciphertext, Spreads out the occurrences of characters making frequency analysis hard                         - in general, this cipher uses key k1 on letters m1,mp+1,m2p+1,... and keykj onlettersmqp+j forallqwith1\u2264qp+j\u2264k                         - \u4f46\u662f: weak to Kasiski examination, Repeated phrases in the ciphertext give away clues as to the length of the running key, \u4e24\u4e2a\u76f8\u540cphrase\u4e2d\u95f4\u7684\u8ddd\u79bb\u53ef\u4ee5\u5e2e\u52a9\u83b7\u53d6key\u7684\u957f\u5ea6, \u7136\u540e\u5c31\u53ef\u4ee5\u505afrequency analysis\u4e86                     - [[Questions]]                       collapsed:: true                         - Is the Vigenere cipher more secure than the simple substitution cipher?                           collapsed:: true                             - \u5728\u7834\u89e3\u96be\u5ea6\u4e0a\u786e\u5b9e\u6bd4simple substitution\u9ad8\u4e86, \u56e0\u4e3a\u6709running keys, \u4f46\u662f\u4ecd\u7136\u5e76\u4e0d\u5b89\u5168                         - Is the Vigenere cipher secure?                           collapsed:: true                             - \u5e76\u4e0d\u5b89\u5168\u54c8, \u901a\u8fc7\u91cd\u590d\u77ed\u8bed\u5206\u6790\u51fakey\u7684\u957f\u5ea6\u4ee5\u540e\u5c31\u53ef\u4ee5\u901a\u8fc7frequency analysis\u6765\u627e\u5230\u5bf9\u5e94\u7684\u539f\u5b57\u6bcd\u4e86                         - in what sense are the shift cipher, substitution cipher, and Vigenere cipher symmetric encryption?                           collapsed:: true                             - \u76f8\u540c\u7684\u5bc6\u94a5\u53ef\u4ee5\u7528\u6765\u52a0\u5bc6, \u6216\u8005\u505a\u4e00\u4e9b\u5c0f\u7684\u8c03\u6574, \u4f8b\u5982\u53cd\u5411, inverse \u5c31\u53ef\u4ee5\u7528\u6765\u89e3\u5bc6                               collapsed:: true                                 -             - Three notions of security               collapsed:: true                 - id:: 63c2dec5-ed59-4fcc-82a9-5c03367b6440                   collapsed:: true                   1. Information-theoretic security: also known as [[unconditional security]] or [[perfect security]]; a cipher cannot be broken even with infinite computing power                       This holds forever                   2. Computational Security: cipher cannot be broken within specified computing power, e.g. where an attacker can compute only within probabilistic polynomial time \u5728\u6709\u9650\u7684\u7b97\u529b\u548c\u65f6\u95f4\u5185\u65e0\u6cd5\u88ab\u7834\u89e3                      \u5982\u540e\u91cf\u5b50\u8ba1\u7b97\u65f6\u4ee3, integer factorisation \u53ef\u80fd\u90fd\u662f\u5f88\u7b80\u5355\u7684                   3. Provable security: show security via a problem reduction; \u201cCipher can be broken implies that a known and believed to be hard problem can be solved.\u201d cipher\u80fd\u88ab\u7834\u89e3\u610f\u5473\u7740\u4e00\u4e2a\u5f88\u96be\u7684\u95ee\u9898\u80fd\u88ab\u89e3\u51b3                      [[Question]] : Why is the term \u201cprovable\u201d in \u2018provable security\u2019 potentially misleading?                     - \u662f\u4e0d\u662f\u56e0\u4e3a\u8fd9\u91cc\u89e3\u91ca\u7684\u662f, \u80fd\u88ab\u7834\u89e3\u7684proof, \u800c\u4e0d\u662f\u4e0d\u80fd\u88ab\u7834\u89e3\u7684proof, \u56e0\u6b64\u8fd9\u91cc\u7684provable\u6307\u7684\u4e0d\u662fprove\u8fd9\u4e2a\u662fsecure\u7684, \u800c\u662f\u7531subproblem\u6765prove\u5176\u4e0d\u5b89\u5168             - ((63c325bd-e4e5-4a57-8a8f-090b16c3cd0c))               collapsed:: true                 - ((63c2e3e7-f36a-48c0-9737-3c6e3934711f))                 - \u5bc6\u6587\u662fc\u7684\u53ef\u80fd\u6027\u662f \u5bf9\u4e8e\u6bcf\u4e2a\u53ef\u80fd\u7684key (\u6211\u4eec\u7528\u7684key\u6070\u597d\u662f\u8fd9\u4e2akey\u7684\u53ef\u80fd\u6027\u4e58\u4e0a\u660e\u6587\u521a\u597d\u662fc\u7528\u8fd9\u4e2akey\u89e3\u5bc6\u5f97\u5230\u7684\u503c\u7684\u6982\u7387\u4e58\u79ef) \u7684\u548c                 - Example:                   collapsed:: true                     - ((63c2e3ca-be33-4b36-b00e-9504aa90dec4))                 - Conditional probability                   collapsed:: true                     - p(C = c | P = m) = probability that ciphertext is c given that plaintext is m; \u660e\u6587m\u7684\u52a0\u5bc6\u662fc\u7684\u53ef\u80fd\u6027, \u5373\u80fd\u628a\u660e\u6587m\u52a0\u5bc6\u6210c\u7684\u5bc6\u94a5\u51fa\u73b0\u7684\u53ef\u80fd\u6027\u548c                     - ((63c2e6d4-6bf1-4b2a-9641-fa5f7067bd51))                     - \u4f46\u662fattacker\u60f3\u77e5\u9053\u7684\u662f, \u5f97\u5230\u4e86c\u8fd9\u4e2a\u5bc6\u6587, \u8fd9\u4e2a\u5bc6\u6587\u6240\u5bf9\u5e94\u7684\u660e\u6587\u7684\u6982\u7387                       collapsed:: true                         - ((63c2e768-e485-4eef-8b34-486bbd5c67c1))                         - \u8fd9\u91cc\u6709\u4e2aAssumption: for all c in C, we have p(C = c) &gt; 0. \u4e0d\u7136\u4e0a\u9762\u7684\u5206\u6bcd\u5c31\u53d8\u62100\u4e86. [[Question]]: why does this assumption not lose generality?                           collapsed:: true                             - \u53ef\u80fd\u662f\u56e0\u4e3a\u6211\u4eec\u4e0d\u4f1a\u53bb\u8003\u8651\u90a3\u4e9b\u4e0d\u53ef\u80fd\u51fa\u73b0\u7684ciphertext\u5427, \u4e0d\u7136\u5c31\u6ca1\u6709\u610f\u4e49\u4e86\u5440, \u4e0d\u4f1a\u53bb\u7528\u7684\u4e1c\u897f, \u8003\u8651\u4ed6\u5e72\u561b?                         - \u4f8b\u5b50                           ((63c2e789-74dc-49cc-b79d-daf68ee736a7))                         - \u8fd9\u6837\u5c31\u53ef\u4ee5\u63a8\u6d4b\u51fa\u6700\u6709\u53ef\u80fd\u7684\u5bf9\u5e94\u7684\u660e\u6587\u4e86, for perfect secrecy, we want to prevent inferences about most likely plaintext                           collapsed:: true                             -             - ((63c325aa-7555-445e-aef9-33856d8f46d0))                 - ((63c30bcc-f3ae-4111-b9cc-f9860a6aa3df))                 - \u6240\u8c13\u5b8c\u7f8e\u7684\u4fdd\u5bc6, \u5c31\u662f\u5bc6\u6587\u662f\u67d0\u4e2a\u660e\u6587\u7684\u6982\u7387\u548c\u8fd9\u4e2a\u660e\u6587\u672c\u8eab\u51fa\u73b0\u7684\u6982\u7387\u662f\u76f8\u540c\u7684, \u6211\u4eec\u6ca1\u6cd5\u4ece\u5bc6\u6587\u4e2d\u83b7\u53d6\u5230\u4efb\u4f55\u65b0\u7684\u5173\u4e8e\u660e\u6587\u7684\u4fe1\u606f, no information gain in this; \u4e0e\u4e4b\u540c\u7406\u7684\u662f, \u660e\u6587\u662f\u67d0\u4e2a\u5bc6\u6587\u7684\u6982\u7387\u548c\u8fd9\u4e2a\u5bc6\u6587\u51fa\u73b0\u7684\u6982\u7387\u662f\u76f8\u901a\u7684.                 - \u53e6\u5916\u4e00\u4e2a\u6761\u4ef6\u5219\u662f, \\(| K | \u2265 | C | \u2265 | P |\\). \u6570\u91cf, key\u8981\u6bd4\u5bc6\u6587\u591a, \u5bc6\u6587\u8981\u6bd4\u660e\u6587\u591a, \u8fd9\u6837\u624d\u80fd\u4fdd\u8bc1\u4e0d\u4f1a\u91cd\u590d                   collapsed:: true                     - \u4f46\u8fd9\u4e5f\u610f\u5473\u7740\u6211\u4eec\u9700\u8981\u81f3\u5c11\u660e\u6587\u6570\u91cf\u7684key\u6765\u5b9e\u73b0\u5b8c\u7f8e\u7684\u52a0\u5bc6\u7cfb\u7edf, \u56e0\u6b64\u5b8c\u7f8e\u52a0\u5bc6\u662f\u6709\u9650\u5b9e\u7528\u7684 (limited practical).                 - ((63c3259f-8d13-4f83-8ef3-5b3bf4576cf9))                   collapsed:: true                     - ((63c30f59-5e1d-495b-8ac5-ac6097d3cd22))                       \u56fe\u4e2d\u6240\u793a\u7684\u52a0\u5bc6\u7cfb\u7edf\u662f\u5b8c\u7f8e\u5b89\u5168\u7684\u5145\u5206\u5fc5\u8981\u6761\u4ef6\u662f: \u6bcf\u4e2akey\u7684\u4f7f\u7528\u6982\u7387\u90fd\u662f1/k, \u5373\u5747\u8861\u7684; \u53ea\u6709\u4e00\u4e2akey\u80fd\u8ba9m\u52a0\u5bc6\u6210c, \u5373key\u5bf9\u4e8emc\u7684mapping\u662f\u552f\u4e00\u7684                     - Proof: \u7531perfect secrecy -&gt; S1 &amp; S2                       collapsed:: true                         - S2: PS \u610f\u5473\u7740 \\(p(C =c|P =m)=p(C =c)&gt;0\\)                           \u7531\u4e8ep(C =c)&gt;0, \u56e0\u6b64\u4e00\u5b9a\u4f1a\u6709k in K allowing \\(e_k(m) = c\\)                           \u8fd8\u9700\u8981\u8bc1\u660ek\u7684\u552f\u4e00\u6027, \u5373\u4e00\u5bf9\u4e00\u7684\u5173\u7cfb, \u4e00\u4e2ak\u5bf9\u5e94\u4e00\u4e2ac                           \u90a3\u6211\u4eec\u53ef\u4ee5\u8bc1\u660eClaim: \\(|{e_k(m) | k \u2208 K}| = |C|\\)                           \u8fd9\u4e2a\u53ef\u4ee5\u7531\u53cc\u5411\u7684inclusion\u8bc1\u660e, \u9996\u5148\u5f88\u660e\u663e\u5bf9m\u52a0\u5bc6\u751f\u6210\u7684\u5b57\u7b26\u80af\u5b9a\u5728C\u7684\u96c6\u5408\u91cc\u9762; \u53e6\u4e00\u65b9\u9762, \u4efb\u4f55\u5bc6\u6587c, \u7531\u4e8e\u5176\u5b58\u5728\u53ef\u80fd\u6027\u5927\u4e8e\u96f6, \u5219\u4e00\u5b9a\u53ef\u4ee5\u7531\u67d0\u4e2akey\u5bf9m\u52a0\u5bc6\u751f\u6210, \u56e0\u6b64c\u4e5f\u5305\u6db5\u5728\\({e_k(m) | k \u2208 K}\\)\u96c6\u5408\u4e4b\u4e2d, \u53cc\u5411\u5305\u542b\u5219\u610f\u5473\u7740\u8fd9\u662f\u4e00\u4e2ainjective, \u4e00\u5bf9\u4e00\u7684\u5173\u7cfb, \u4e5f\u5c31\u662f\u552f\u4e00\u6027\u4fdd\u8bc1\u4e86                           ((63c328c2-656c-4bee-a640-f16cb3fe6d85))                         - S1: \u7528\u8d1d\u53f6\u65af\u7406\u8bba\u91cd\u5199\u4e86\u4e00\u4e0b\u5b8c\u7f8e\u52a0\u5bc6\u7684\u7b49\u5f0f, \u7531\u4e8e\u6211\u4eec\u8bf4\u53ea\u6709\u4e00\u4e2a\u552f\u4e00\u7684key\u53ef\u4ee5\u628a\u660e\u6587m\u52a0\u5bc6\u6210c, \u56e0\u6b64\\(p(C = c | P = mi)\\)\u4e0e\u8fd9\u4e2akey\u7684\u51fa\u73b0\u6982\u7387\u662f\u4e00\u6837\u7684, \u56e0\u6b64\u53ef\u4ee5\u91cd\u5199\u6210\\(p(K = ki)\\), \u4e8e\u662f\u56e0\u4e3a\u7b49\u5f0f\u5de6\u53f3\u8981\u76f8\u7b49, \u6240\u4ee5key\u7684\u51fa\u73b0\u6982\u7387\u548cc\u7684\u6982\u7387\u662f\u8981\u76f8\u7b49\u7684. \u8fd9\u610f\u5473\u7740\\(p(K = ki)\\)\u90fd\u662f\u76f8\u7b49\u7684, \u4e5f\u5c31\u662f1/K\u4e86                           ((63c31a57-638c-4345-8705-4111928fd787))                 - Examples of perfectly secure cryptosystem #[[Perfect Secrecy]]                   collapsed:: true                     - ((63c3253e-8557-4fc0-a51d-16c608ca1831))                       collapsed:: true                         - \u660e\u6587\u6709\u591a\u957f, key\u5c31\u8981\u6709\u591a\u957f, key\u4ece26\u4e2a\u5b57\u6bcd\u91cc\u9762independently, uniformly, randomly \u9009\u62e9\u660e\u6587\u957f\u5ea6\u4e2a, \u7b2c\u51e0\u4e2a\u5b57\u6bcd\u5c31\u610f\u5473\u7740\u8981\u79fb\u51e0\u4f4d(0\u5f00\u59cb), \u7ec4\u5408\u6570\u5462\u6709\\(26^n\\)                     - ((63c325e5-9101-4ade-ae90-3f799a444447)) (Vernam cipher)                       collapsed:: true                         - ((63c32640-db4a-447f-bf7a-05a04aa849dc))                         - \u5f02\u6216, \u4e0d\u540c\u7684\u65f6\u5019\u7ed3\u679c\u624d\u662f1; \u5bc6\u94a5\u53ef\u4ee5\u76f4\u63a5\u7528\u6765\u52a0\u89e3\u5bc6                         - n\u957f\u5ea6\u7684\u660e\u6587, \u4f1a\u6709\\(2^n\\)\u79cd\u8868\u73b0, key\u4e5f\u4f1a\u6709\u540c\u6837\u6570\u91cf, \u56e0\u6b64n\u957f\u5ea6\u7684key\u7684\u9009\u53d6\u53ef\u80fd\u6027\u662f\\(1/2^n\\)                 - ((63c3331a-987a-4cbf-b68d-ffb282e1f18f))                     - 1. Key has to be as long as the message.                       2. Key has to be truly random.                       3. Key can be used at most once.                     - \u5982\u679c\u91cd\u590d\u4f7f\u7528\u7684\u8bdd, Eve\u53ef\u4ee5\u901a\u8fc7\u81ea\u5df1generate\u4e00\u4e9b\u6587\u5b57\u8ba9Alice\u52a0\u5bc6, \u6765\u83b7\u53d6key, \u5e76\u4e14\u7528\u8fd9\u4e2akey\u89e3\u5bc6\u4e4b\u540eA\u4e0eB\u7684\u5bc6\u6587         - Notes             - ((63c338fb-bec0-4868-84e3-a5fe94ccbeef)) #\u672f\u8bed\u5361               collapsed:: true                 - A cryptosystem is given by a set \\(P\\) of plaintexts,                   a set \\(C\\) of cipertexts, a set \\(K\\) of keys, an encryption function \\(e : K \u00d7 P \u2192 C\\),                   and a decryption function \\(d : K \u00d7 C \u2192 P\\) such that for all \\(k\\) in \\(K\\) and for all m                   in \\(P\\) we have that \\(m = dk(ek(m))\\).             - ((63c33962-3eec-4549-847a-ac5c022ed703)) #\u672f\u8bed\u5361               collapsed:: true                 - A cryptosystem is symmetric if it has an efficient and deterministic method for converting keys used for encryption into the corresponding keys used for decryption.             - ((63c3398d-8bbd-497f-8c00-cb758ad53cf3)) #\u672f\u8bed\u5361               collapsed:: true                 - In cryptography, Kerckhoff\u2019s Principle                   stipulates that the security of a cryptosystem should not rest on the secrecy                   of its algorithms.                 - cryptographic system must be secure even if everything is known about the system with the exception of the secret key.             - ((63c339d5-a1fc-4b07-932b-aef369bdc69e)) #\u672f\u8bed\u5361               collapsed:: true                 - Let \\(A\\) be a non-empty set. Then Perm(\\(K\\)) is                   the set of permutations of \\(A\\), which are functions \\(f : A \u2192 A\\) that are 1-1 and                   onto.             - ((63c339fe-1497-44e7-9d7c-36656504fb63)) #\u672f\u8bed\u5361               collapsed:: true                 - ((63c2dec5-ed59-4fcc-82a9-5c03367b6440))         - Exercises             - ((63c32bd0-6c3a-4952-9b76-a136def3c3fb))               collapsed:: true                 -             - ((63c32cf9-97f6-4fef-a8c5-4b696186f42f))             - ((63c32d08-63ae-4744-8c63-48dac2bad567))             - ((63c32d12-47d2-4af7-9dda-ca8babb845fd))             - ((63c32d1b-950c-42b9-a450-70e47c744632))             - ((63c32d28-d8af-48ed-b949-411be4180f48))             - ((63c32d30-2fe8-4c9d-99c3-a8b8083f0f2a))               collapsed:: true                 -             - ((63c32d3a-af96-466a-9f8e-6f8c7d48ad89))               collapsed:: true                 - ((63c55656-8b79-49fd-97b4-9bfe703cdd23))                 - \u4e3a\u4ec0\u4e48C\u8981\u81f3\u5c11\u6bd4P\u5927                   collapsed:: true                     - \u56e0\u4e3aCP\u662f1-1\u7684\u5173\u7cfb, \u6bcf\u4e2ap\u5fc5\u987b\u8981\u6709c\u5bf9\u5e94, \u6240\u4ee5C\u7684\u5927\u5c0f\u4e00\u5b9a\u81f3\u5c11\u662f\u7b49\u4e8eP\u7684\u5927\u5c0f\u7684, C\u4e5f\u53ef\u4ee5\u66f4\u5927, \u4f46\u662f\u4f1aredundant, \u6240\u4ee5\u6211\u4eecassume p(C=c)&gt;0                 - \u4e3a\u4ec0\u4e48K\u8981\u81f3\u5c11\u6bd4C\u5927                   collapsed:: true                     - \u56e0\u4e3a\u5bf9\u4e8e\u67d0\u4e00\u4e2a\u660e\u6587m, \u5bf9\u4e8e\u6240\u6709\u53ef\u80fd\u7684\u5bc6\u6587, \u90fd\u5fc5\u987b\u8981\u6709\u4e00\u4e2akey \u80fd\u591f\u628am\u52a0\u5bc6\u6210\u8fd9\u4e9b\u5bc6\u6587; \u800c\u76f8\u540c\u7684key\u53ea\u80fd\u52a0\u5bc6\u660e\u6587m\u5230\u540c\u4e00\u4e2a\u5bc6\u6587, \u8fd9\u610f\u5473\u7740key\u7684\u6570\u91cf\u81f3\u5c11\u8981\u548cC\u7684\u5927\u5c0f\u4e00\u6837; \u4e3e\u4e2a\u4f8b\u5b50, shift cipher\u7684\u8fdb\u9636perfect\u7248\u672c\u9700\u8981\u6bcf\u4f4d\u5b57\u6bcd\u90fd\u62e5\u6709\u4e00\u4e2a\u72ec\u7acb\u968f\u673a\u7684key\u6765\u8fdb\u884c\u52a0\u5bc6, \u5404\u4e2a\u5b57\u6bcd\u4e92\u4e0d\u76f8\u5e72, \u624d\u80fd\u8ba9\u7834\u574f\u8005\u65e0\u6cd5\u4ece\u5bc6\u6587\u4e2d\u63a8\u7406\u5230\u4efb\u4f55\u660e\u6587\u7684\u4fe1\u606f         - \u5b8f\u89c2\u7406\u89e3             - CIA\u4e09\u4e2a\u7279\u6027, \u5176\u4e2dintegrity \u8fd8\u4f1a\u6307\u4fdd\u8bc1\u53d1\u9001\u8005\u7684\u8eab\u4efd             - \u5bf9\u79f0\u52a0\u5bc6\u7684\u6838\u5fc3\u5728\u4e8e\u89e3\u5bc6\u7528\u7684\u5bc6\u94a5\u53ef\u4ee5\u7528\u7b80\u5355\u7684\u65b9\u5f0f\u4ece\u52a0\u5bc6\u7684\u5bc6\u94a5\u4e2d\u63a8\u51fa             - \u52a0\u5bc6\u7cfb\u7edf\u7531KCP \u548c \u52a0\u5bc6\u51fd\u6570e\u89e3\u5bc6\u51fd\u6570d\u7ec4\u6210             - \u5bf9\u79f0\u52a0\u5bc6\u6709shift (Caesar), substitution (monoalphabetic, \u6240\u6709\u660e\u6587\u7528\u4e00\u4e2a26\u5b57\u6bcdpermutation), Vigenere(polyalphabetic, \u8f6e\u6d41running key\u7528\u4e00\u7ec4permutations substitute, Kasiski examination\u7834\u89e3), \u540e\u9762\u540e\u6709\u4fe9perfect\u7684 \u5bf9\u79f0\u52a0\u5bc6             - P(C=c)\u7684\u6982\u7387\u662f\u5bf9\u4e8e\u6bcf\u4e2a\u53ef\u80fd\u7684key (\u6211\u4eec\u7528\u7684key\u6070\u597d\u662f\u8fd9\u4e2akey\u7684\u53ef\u80fd\u6027\u4e58\u4e0a\u660e\u6587\u521a\u597d\u662fc\u7528\u8fd9\u4e2akey\u89e3\u5bc6\u5f97\u5230\u7684\u503c\u7684\u6982\u7387\u4e58\u79ef) \u7684\u548c; \u67d0\u4e2a\u660e\u6587\u662f\u67d0\u4e2a\u5bc6\u6587\u7684\u6982\u7387\u662f\u5efa\u7acb\u8fd9\u4fe9\u8054\u7cfb\u7684\u6240\u6709key\u7684\u6982\u7387\u548c (\u5b8c\u7f8e\u79d8\u5bc6\u9700\u8981\u53ea\u6709\u4e00\u4e2akey)             - Perfect secrecy\u662f\u5bf9\u4e8e\u6240\u6709\u660e\u6587\u548c\u5bc6\u6587, p(P=m|C=c) = p(P=m), \u6216\u8005p(C=c|P=m) = p(C=c) \u4e14\u662f\u5927\u4e8e\u96f6\u7684, \u56e0\u4e3a\u4e0d\u51fa\u73b0\u7684c\u6ca1\u6709\u610f\u4e49, \u4e5f\u610f\u5473\u7740K&gt;=C&gt;=P. Shannon \u5b9a\u4e49\u4e0b\u4e09\u8005\u4e00\u6837\u5927\u4e14k\u7684\u6982\u7387\u4e3a1/K\u4e14\u6bcf\u4e00\u7ec4m\u548cc\u90fd\u4e00\u4e00\u5bf9\u5e94\u53ea\u6709\u4e00\u4e2a\u552f\u4e00\u7684k\u65f6\u662fperfectly secure\u7684. \u56e0\u4e3ap(C=c)&gt;0\u5c31\u4e00\u5b9a\u6709k\u4ecem\u6620\u5c04\u5230c; m\u52a0\u5bc6\u5f97\u5230\u7684\u4e00\u5b9a\u5728C\u4e2d, c\u4e00\u5b9a\u5b58\u5728\u610f\u5473\u7740\u4e00\u5b9a\u6709m\u5230c, \u6240\u4ee5c\u7684\u89e3\u5bc6\u4e5f\u4e00\u5b9a\u5728P\u4e2d, \u6240\u4ee5\u662f\u53cc\u5411\u5305\u542b\u7684injective\u5173\u7cfb. \u6240\u4ee5\u4e00\u5b9a\u662f\u552f\u4e00\u7684key\u5728map. 1/K\u7684\u6982\u7387\u5219\u7531\u8d1d\u53f6\u65af\u5206\u89e3p(P=m|C=c)\u548ckey\u7684\u552f\u4e00\u6027\u53ef\u4ee5\u63a8\u51fap(K=ki)=p(C=c), \u5404\u4e2akey\u6982\u7387\u76f8\u540c             - \u5269\u4e0b\u4e24\u4e2a\u5bf9\u79f0\u52a0\u5bc6: modified shift cipher, \u6bcf\u4e2a\u660e\u6587\u90fd\u7531\u4e00\u4e2a\u968f\u673a\u7684shift\u91cf\u6765\u51b3\u5b9a, \u53ef\u80fd\u6027\u670926^n. one-time pad, \u6bcf\u4e2abit\u90fd\u548c\u968f\u673a\u76841\u6216\u80050 \u8fdb\u884cXOR, \u9700\u8981\u6bcf\u4e2a\u4f4d\u7f6e\u7684key\u90fd\u968f\u673a, key\u53ea\u80fd\u7528\u4e00\u6b21, key\u957f\u5ea6\u548c\u539f\u6587\u957f\u5ea6\u4e00\u81f4, \u67092^n\u79cd\u53ef\u80fd, \u91cd\u590d\u4f7f\u7528\u53ef\u4ee5chosen text attack\u83b7\u5f97\u5bc6\u94a5             - perfect secrecy does not depend on the probability distribution p(P=p)     - Week3 Ch 13 &amp; 14 Block ciphers, DES, Hash functions       collapsed:: true         - Slides             - ((63cdc1a3-cb13-49e2-b4e6-c0dc7a39dbc8))                 - \u660e\u6587\u88ab\u5206\u6210\u4e86\u7b49\u957f\u7684\u4e00\u4e2a\u4e2ablocks                   collapsed:: true                     - \\(m = m_1m_2 ...m_k\\)                 - Block ciphers assume cryptosystem (ek(\u00b7),dk(\u00b7)) for data of block length, \u5757\u662f\u57fa\u672c\u52a0\u5bc6\u5355\u4f4d                 - \u6211\u4eec\u4e0a\u5468\u8bb2\u5230\u7684\u52a0\u5bc6\u7cfb\u7edf\u90fd\u662f\u4f7f\u75281bit\u957f\u5ea6\u7684block, \u4f8b\u5982onetime-pad\u5c31\u662f\u6bcf\u4e00\u4f4d\u90fd\u4f1a\u7528\u4e00\u4e2asample\u7684\u5bc6\u94a5\u8fdb\u884c\u52a0\u5bc6                 - Modes of operation: Encryption of mi may depend on encryption of \\(m_j\\) where j &lt; i. \u52a0\u5bc6\u64cd\u4f5c\u6a21\u5f0f, \u8fd9\u8282\u8bfe\u8bb2\u5230\u7684\u52a0\u5bc6\u7cfb\u7edf, \u53ef\u80fd\u4f1a\u6709\u4e0e\u524d\u540e\u660e\u6587\u5757\u76f8\u5173\u7684\u52a0\u5bc6, \u4f8b\u5982\u7b2c\u4e8c\u5757\u7684\u52a0\u5bc6\u548c\u7b2c\u4e00\u5757\u6709\u5173                 - DES: Data Encryption Standard                   collapsed:: true                     - 64-bit data blocks, 56-bit keys (8 bits for redundancy)                 - AES: Advanced Encryption Standard                   collapsed:: true                     - 128-bit data blocks, key size of 128, 192, 256 bits                 - N.B. DES and AES\u90fd\u662fiterated block ciphers                 - ((63cd968e-01d3-4f82-8eed-04337dfea944)) 13.2 P244                   collapsed:: true                     - Block ciphers\u7684iteration\u88ab\u53eb\u505aRounds, \u6bcf\u4e2around i, \u52a0\u5bc6\u7528\u7684key k\u4f1adetermine\u4e00\u4e2around key \\(k_i\\) (\u5b50\u5bc6\u94a5subkey), \u4ecek\u8ba1\u7b97\u51fa\\(k_i\\)\u7684\u8fc7\u7a0b\u53eb\u505akey schedule,                       collapsed:: true                         - rounds\u6570\u91cf\u8d8a\u591a, \u8d8a\u5b89\u5168                         - key\u7684\u957f\u5ea6\u8d8a\u957f, \u8d8a\u5b89\u5168                         -                      - Round function: \u6211\u4eec\u9700\u8981round function\u6765\u51b3\u5b9a\u5982\u4f55\u6839\u636e\u53f3\u4fa7\u548c\u5b50\u5bc6\u94a5\u751f\u6210\u52a0\u5bc6\u540e\u7684\u6bd4\u7279\u5e8f\u5217                       collapsed:: true                         - Feistel function: \u88ab\u7528\u5728DES\u4e2d, DES\u91c7\u7528\u4e8616\u8f6e\u5faa\u73af\u7684Feistel\u7f51\u7edc, \u4e0b\u56fe\u662f\u5e94\u7528\u4e86Feistel function F\u7684\u4e00\u8f6e\u7684\u64cd\u4f5c                           collapsed:: true                             - ((63cd97d8-7709-4cd7-9cab-2d56e7ad11f6))                             - 64-bit\u7684\u8f93\u5165\u88ab\u9996\u5148\u5206\u6210\u4e86\u4e24\u534a, \u5de6\u53f3., \u6211\u4eec\u4e0d\u5bf9\u5de6\u8fdb\u884c\u989d\u5916\u64cd\u4f5c, \u6211\u4eec\u628a\u53f3\u4fa7\u4f5c\u4e3a\u8f93\u5165\u548c\u5b50\u5bc6\u94a5\u4e00\u8d77\u653e\u5165\u8f6e\u51fd\u6570F\u4e2d, \u5176\u5f97\u5230\u7684\u7ed3\u679c\u4e0e\u5de6\u4fa7\u539f\u6587\u8fdb\u884cXOR, \u7ed3\u679c\u5c31\u662f\u52a0\u5bc6\u540e\u7684\u5de6\u4fa7, \u53f3\u4fa7\u5728\u4e00\u8f6e\u4e2d\u4e0d\u8fdb\u884c\u52a0\u5bc6, \u76f4\u63a5\u4e0b\u653e                             -                              - \u7531\u4e8e\u53f3\u4fa7\u5728\u4e00\u8f6e\u4e2d\u4e0d\u8fdb\u884c\u52a0\u5bc6, \u56e0\u6b64\u8f6e\u4e0e\u8f6e\u95f4, \u6211\u4eec\u9700\u8981\u5bf9\u8c03\u5de6\u53f3, \u8fd9\u4e5f\u662f\u4e3a\u5565\u4e0a\u56feL-R\u7684\u539f\u56e0, \u56e0\u4e3a\u5bf9\u8c03\u4e86. \u5e76\u4e14\u5bf9\u8c03\u64cd\u4f5c\u53ea\u5728\u4e24\u8f6e\u4e4b\u95f4\u8fdb\u884c, \u6700\u540e\u4e00\u8f6e\u4e0d\u9700\u8981                             - Feistel\u7f51\u7edc\u7684\u4e00\u4e2a\u597d\u5904\u5728\u4e8e, \u7531\u4e8e\u4e00\u8fb9\u7684\u6570\u636e\u662f\u4e0d\u8fdb\u884c\u64cd\u4f5c\u7684, \u6211\u4eec\u5c06\u4e0b\u4e00\u8f6e\u52a0\u5bc6\u8f93\u51fa\u7ed3\u679c\u7528\u76f8\u540c\u7684\u5b50\u5bc6\u94a5\u91cd\u65b0\u8fd0\u7b97, \u662f\u5b8c\u7f8e\u8fd8\u539f\u4e3a\u539f\u6587\u7684, \u4e5f\u5c31\u662f\u8bf4Feistel\u7684\u89e3\u5bc6\u662f\u9700\u8981\u76f8\u53cd\u987a\u5e8fapply\u5b50\u5bc6\u94a5\u5373\u53ef, \u8fd9\u4e5f\u5c31\u4e0d\u9700\u8981\u8f6e\u51fd\u6570\u53ef\u4ee5\u9006\u5411\u8ba1\u7b97\u51fa\u8f93\u5165\u7684\u503c(\u89e3\u5bc6\u7684\u65f6\u5019\u7528\u7684\u662f\u76f8\u540c\u7684\u53f3\u4fa7\u548c\u5b50\u5bc6\u94a5). \u56e0\u6b64\u8f6e\u51fd\u6570\u53ef\u4ee5\u8bbe\u8ba1\u7684\u4efb\u610f\u590d\u6742. \u540c\u65f6\u53ef\u4ee5\u4fdd\u8bc1\u76f8\u540c\u7ed3\u6784\u5c31\u80fd\u5b9e\u73b0\u52a0\u5bc6\u548c\u89e3\u5bc6.                 - ((63cd9b44-fcba-4afe-90e3-1fc44cf3fcb2)) 13.2 P244                   collapsed:: true                     - 1. Number of rounds: trade-off btw security and performance                       2. Key schedule: key\u5230subkey\u7684\u6620\u5c04, \u8981\u53ef\u884c, \u53ef\u53cd                       3. Feistel function F: needs to contain non-linear behavior to ensure enough information-theoretic resiliency. \u975e\u7ebf\u5f62\u6765\u4fdd\u8bc1\u8db3\u591f\u7684\u5bf9\u4fe1\u606f\u8bba\u7684\u62b5\u6297                     - Advantage: Only have to implement or build an encryption device, no need for a decryption device, only need to manage D/E state. \u53ea\u9700\u8981\u4e00\u4e2adevice\u6765\u6267\u884c\u52a0\u5bc6\u89e3\u5bc6                 - ((63cd9c09-b3ca-4c6c-87f6-e4ae34c6632b)) 13.4 P247; 13.5 P247                   collapsed:: true                     - DES Key schedule: we refer to Nigel Smart\u2019s book for details on that; the point to remember is that this uses permutations and cyclic shifts based on the key and round number.                 - ((63cd9ca8-cd76-4b9b-98d5-2e9aac2d2e90)) 13.3 AES                   collapsed:: true                     - DES \u548c AES seem immune against:                       collapsed:: true                         - differential cryptanalysis (\u5dee\u5206\u5206\u6790): a chosen-plaintext attack, \u4fee\u6539\u67d0\u4e2a\u660e\u6587, \u67e5\u770b\u5bc6\u6587\u662f\u600e\u4e48\u53d8\u7684                         - linear cryptanalysis (\u7ebf\u6027\u5206\u6790): approximate all non-linear components with linear functions; for example the S-boxes of DES; then do a probabilistic analysis about the key. \u5c06\u660e \u6587\u548c\u5bc6\u6587\u7684\u4e00\u4e9b\u5bf9\u5e94\u6bd4\u7279\u8fdb\u884cXOR\u5e76\u8ba1\u7b97\u5176\u7ed3\u679c\u4e3a\u96f6\u7684\u6982\u7387\u5982\u679c\u5bc6\u6587\u5177\u5907\u8db3\u591f\u7684\u968f\u673a\u6027\uff0c\u5219\u4efb\u9009\u4e00\u4e9b\u660e\u6587\u548c\u5bc6\u6587\u7684\u5bf9\u5e94\u6bd4\u7279\u8fdb\u884c XOR \u7ed3\u679c\u4e3a\u96f6\u7684\u6982\u7387\u5e94\u8be5\u4e3a1/2\u3002 \u5982\u679c\u80fd\u591f\u627e\u5230\u5927\u5e45\u504f\u79bb1/2\u7684\u90e8\u5206\uff0c\u5219\u53ef\u4ee5\u501f\u6b64\u83b7\u5f97\u4e00\u4e9b\u4e0e\u5bc6\u94a5\u6709\u5173\u7684\u4fe1\u606f\u3002 \u4f7f\u7528\u7ebf\u6027\u5206\u6790\u6cd5\uff0c\u5bf9\u5343 DES \u53ea\u9700\u8981 2^\u200647\u7ec4\u660e\u6587\u548c \u5bc6\u6587\u5c31\u80fd\u591f\u5b8c\u6210\u7834\u89e3\uff0c\u76f8\u6bd4\u9700\u8981\u5c1d\u8bd5 2^56\u4e2a\u5bc6\u94a5\u7684\u66b4\u529b\u7834\u89e3\u6765\u8bf4\uff0c\u6240\u9700\u7684\u8ba1\u7b97\u5740\u5f97\u5230\u4e86\u5927\u5e45\u51cf\u5c11\u3002                 - ((63cd9d30-8e69-4f26-876e-6ed81fd31839))                     - \u5982 \u679c\u9700\u8981\u52a0\u5bc6\u4efb\u610f\u957f\u5ea6\u7684\u660e\u6587\uff0c\u5c31\u9700\u8981\u5bf9\u5206\u7ec4\u5bc6\u7801\u8fdb\u884c\u8fed\u4ee3\uff0c\u800c\u5206\u7ec4\u5bc6\u7801\u7684\u8fed\u4ee3\u65b9\u6cd5\u5c31\u79f0\u4e3a\u5206\u7ec4 \u5bc6\u7801\u7684\u201c\u6a21\u5f0f\u201d \u3002\u5176\u5b9e\u5c31\u662f\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u7684\u5bf9\u4e8e\u660e\u6587\u4f5c\u51fa\u7684\u4e00\u4e9b\u53d8\u5316, \u6765\u8ba9\u76f8\u540c\u7684\u660e\u6587\u5757\u52a0\u5bc6\u540e\u6ca1\u6709\u53ef\u89c1pattern                     - ECB Electronic Code Book \u7535\u5b50\u5bc6\u7801\u672c\u6a21\u5f0f                       collapsed:: true                         -                          - Real problem with this mode: If we encrypt m more than once with the same key, these ciphertexts are equal. Thus, this mode gives us deterministic encryption which attackers can exploit. Bad!                         - ECB \u6a21\u5f0f\u662f\u6240\u6709\u6a21\u5f0f\u4e2d\u6700\u7b80\u5355\u7684\u4e00\u79cd \u3002 ECB \u6a21\u5f0f\u4e2d\uff0c\u660e\u6587\u5206\u7ec4\u4e0e\u5bc6\u6587\u5206\u7ec4 \u662f\u4e00\u4e00\u5bf9\u5e94\u7684\u5173 \u7cfb\uff0c\u56e0\u6b64\uff0c\u5982\u679c\u660e\u6587\u4e2d\u5b58\u5728\u591a\u4e2a\u76f8\u540c\u7684\u660e\u6587\u5206\u7ec4\uff0c\u5219\u8fd9\u4e9b\u660e\u6587\u5206\u7ec4\u6700\u7ec8\u90fd\u5c06\u88ab\u8f6c\u6362\u4e3a\u76f8\u540c\u7684\u5bc6 \u6587\u5206\u7ec4 \u3002 \u8fd9\u6837\u4e00\u6765\uff0c\u53ea\u8981\u89c2\u5bdf\u4e00\u4e0b\u5bc6\u6587\uff0c\u5c31\u53ef\u4ee5\u77e5\u9053\u660e\u6587\u4e2d\u5b58\u5728\u600e\u6837\u7684\u91cd\u590d\u7ec4\u5408\uff0c\u5e76\u53ef\u4ee5\u4ee5\u6b64 \u4e3a\u7ebf\u7d22\u6765\u7834\u8bd1\u5bc6\u7801\uff0c\u56e0\u6b64 ECB\u6a21\u5f0f\u662f\u5b58\u5728\u4e00\u5b9a\u98ce\u9669\u7684\u3002                         - \u5047\u5982\u5b58\u5728\u4e3b\u52a8\u653b\u51fb\u8005 Mallory, \u4ed6\u80fd\u591f\u6539\u53d8\u5bc6\u6587\u5206\u7ec4\u7684\u987a\u5e8f \u3002 \u5f53\u63a5\u6536\u8005\u5bf9\u5bc6\u6587\u8fdb\u884c\u89e3\u5bc6\u65f6\uff0c \u7531\u4e8e\u5bc6\u6587\u5206\u7ec4\u7684\u987a\u5e8f\u88ab\u6539\u53d8\u4e86\uff0c\u56e0\u6b64\u76f8\u5e94\u7684\u660e\u6587\u5206\u7ec4\u7684\u987a\u5e8f\u4e5f\u4f1a\u88ab\u6539\u53d8 \u3002 \u4e5f\u5c31\u662f\u8bf4\uff0c \u653b\u51fb\u8005 Mallory\u65e0\u9700\u7834\u8bd1\u5bc6\u7801\u5c31\u80fd\u591f\u64cd\u7eb5\u660e\u6587\u3002\u4f8b\u5982\u5bf9\u8c03\u6536\u6b3e\u4eba\u548c\u4ed8\u6b3e\u4ebablock, \u5c31\u53ef\u4ee5\u4ea4\u6362\u8f6c\u8d26\u4fe1\u606f                     - CBC Cipher Block Chaining \u5bc6\u7801\u5206\u7ec4\u94fe\u63a5\u6a21\u5f0f                       collapsed:: true                         - CBC \u6a21\u5f0f \u662f \u5c06\u524d \u4e00 \u4e2a\u5bc6\u6587\u5206\u7ec4\u4e0e\u5f53\u524d\u660e\u6587\u5206\u7ec4\u7684\u5185\u5bb9\u6df7\u5408\u8d77\u6765\u8fdb\u884c\u52a0\u5bc6\u7684.\u8fd9\u6837\u5c31\u53ef\u4ee5\u907f\u514d ECB \u6a21\u5f0f\u7684\u5f31\u70b9 \u3002CB \u6a21\u5f0f\u53ea\u8fdb\u884c\u4e86\u52a0\u5bc6\uff0c\u800c CBC \u6a21\u5f0f\u5219\u5728\u52a0\u5bc6\u4e4b\u524d\u8fdb\u884c\u4e86 \u4e00\u6b21 XOR\u3002                         - \u6bcf\u4e2ablock\u7684\u660e\u6587\u4f1a\u9996\u5148\u4e0e\u4e0a\u4e00\u4e2ablock\u7684\u5bc6\u6587\u8fdb\u884c\u5f02\u6216\u64cd\u4f5c, \u518d\u88ab\u8be5block\u7684\u5bc6\u94a5\u8fdb\u884c\u52a0\u5bc6, \u7b2c\u4e00\u4e2ablock\u548cinitialisation vector IV\u64cd\u4f5c, IV\u9700\u8981\u5b8c\u5168\u968f\u673a.                         - \u540c\u6837\u7684, \u89e3\u5bc6\u7684\u65f6\u5019\u9700\u8981\u5148\u7528\u8fd9\u4e2ablock\u7684key\u89e3\u5bc6, \u7136\u540e\u518d\u4e0e\u4e0a\u4e00\u4e2ablcok\u7684\u5bc6\u6587\u8fdb\u884c\u5f02\u6216\u64cd\u4f5c\u624d\u662f\u771f\u6b63\u7684\u539f\u6587                         - IV \u548c key k\u662f\u9700\u8981\u7684\u5171\u8bc6                         - \u5e38\u7528\u4e8eTLS/SSL\u4e2d \u63a8\u8350\u4f7f\u7528                         -                          -                      - CFB Cipher FeedBack \u5bc6\u6587\u53cd\u9988\u6a21\u5f0f                       collapsed:: true                         -                      - OFB Output FeedBack \u8f93\u51fa\u53cd\u9988\u6a21\u5f0f \u4e0d\u53ef\u4ee5\u5e73\u884c                       collapsed:: true                         - \u9700\u8981initialisation vector IV, \u7531IV\u6765\u751f\u6210\u4e00\u4e32key stream                            \\(Y=Y_1Y_2Y_3...Y_q\\),                            \\(Y_0 = IV\\) \\(Y_i = e_k(Y_{i\u22121})\\) \\(c_i =m_i\u2295Y_i\\) (\u5982\u679cm\u957f\u5ea6\u4e3a1\u7684\u8bdd\u90a3\u5c31\u662f\u4e2astream cipher\u4e86, \u5c31\u4e0d\u9700\u8981padding\u4e86)                         - \u52a0\u5bc6\u8fc7\u7a0b\u4e0eonetime-pad\u5c31\u5f88\u76f8\u4f3c, \u53ea\u4e0d\u8fc7OTP\u7528\u7684\u662f\u6bcf\u4e2aletter\u90fd\u968f\u673a\u751f\u6210key, \u800c\u8fd9\u91cc\u7684\u7684Yi\u90fd\u662f\u7531Y_i-1\u7528key\u751f\u6210\u7684                         - \u5e76\u6ca1\u6709perfect secret, \u56e0\u4e3a\u5e76\u4e0d\u662f\u5b8c\u5168\u968f\u673a\u5206\u5e03\u7684                         -                      - CTR counTeR \u8ba1\u6570\u5668\u6a21\u5f0f \u53ef\u4ee5\u5e73\u884c                         - \u9700\u8981initialisation vector IV as a counter: IV \u2243 binary encoding of counter value. \u5e8f\u5217\u4e2d\u6bcf\u6b21\u5f80IV\u91cc\u9762\u52a0\u4e00                         - \\(c_i = m_i \u2295 e_k(IV + i)\\)                         - Advantages:                           1. This is a recent US NIST Standard.                           2. We may compute all \\(c_i\\) in parallel, which is not possible in OFB, CFB, and CBC modes. \u53ef\u4ee5\u5e73\u884c\u5feb\u901f\u8ba1\u7b97                           3. If \\(m_i = m_j\\) , then \\(c_i != c_j\\) in general. So this improves over ECB. \u76f8\u540c\u7684\u660e\u6587\u5757\u4e0d\u4f1a\u5bfc\u5411\u76f8\u540c\u7684\u5bc6\u6587\u4e86                         - Requirement: For l-bit data blocks, the plaintext message m must not be                           longer than 2^l bits. #Question                         - \u63a8\u8350                         -                      -                  - AES (Rijndael)                   collapsed:: true                     -                      - 128-bit data blocks, key size of 128, 192, 256 bits                     - 512\u4f4d\u5df2\u7ecf\u523010^154\u7684\u6570\u91cf\u7ea7\u4e86, \u518d\u591a\u5c31\u6ca1\u5565\u5fc5\u8981\u4e86             - Symmetric ciphers take away               collapsed:: true                 - \u5bf9\u79f0\u5bc6\u7801\u4e2d, \u52a0\u5bc6\u7684\u5bc6\u94a5\u548c\u89e3\u5bc6\u7684\u5bc6\u94a5\u662f\u76f8\u7b49\u7684                 - \u8ba1\u7b97\u80fd\u529b\u518d\u9ad8, \u4e00\u6b21\u6027\u5bc6\u7801\u672c\u4e5f\u6ca1\u529e\u6cd5\u7834\u8bd1                 - \u957f\u5ea6\u4e3a56\u4f4d\u7684\u5bc6\u94a5\u5e73\u5747\u7834\u89e3\u6b21\u6570\u4e3a2^56\u7684\u4e00\u534a2^55             - ((63cdc18a-256a-444d-b9bb-5f5dd79bb5f1))               collapsed:: true                 - \u4e3a\u4e86\u4fdd\u8bc1\u4fe1\u606f\u7684\u5b8c\u6574\u6027\u548c\u4e0d\u88ab\u7be1\u6539, \u5373Integrity, \u6211\u4eec\u9700\u8981\u65b9\u6cd5\u6765\u9a8c\u8bc1\u4fe1\u606f                 - ((63cdc2a5-7373-4ed8-b7e1-0620b6993b2b)) h: capture the integrity of m as a hash h(m). \u5355\u5411\u6563\u5217\u51fd\u6570, \u6d88\u606f\u7684\u6307\u7eb9                   collapsed:: true                     - Input x is arbitrarily long. Output y is of fixed length n.                       \u4ece\u4efb\u610f\u957f\u5ea6\u7684\u6bd4\u7279\u5e8f\u5217\u5230\u56fa\u5b9a\u957f\u5ea6\u6bd4\u7279\u5e8f\u5217\u7684mapping                     - Security Properties \u533a\u522b\u4e8e\u54c8\u5e0c\u8868\u7684\u5b89\u5168\u5c5e\u6027                       collapsed:: true                         - Preimage resistance: Given y in the image of h, it should be compu- tationally infeasible to compute an x with h(x) = y.                            \u7ed9\u5b9a\u4e00\u4e2ay, \u5f88\u96be\u627e\u5230\u4e00\u4e2a\u53ef\u4ee5\u751f\u6210y\u7684x, \u81f3\u5c11\u5e94\u8be5\u662fO(2^n)                         - Collision resistance: It should be computationally infeasible to find                           two inputs x and x\u2032 (in particular, x \u0338= x\u2032) such that h(x) = h(x\u2032).                           \u96be\u4ee5\u627e\u5230\u4e86\u4e24\u4e2ax\u4f7f\u5f97\u8fd9\u4e2ahash\u51fd\u6570\u80fd\u751f\u6210\u540c\u6837\u7684y, \u81f3\u5c11\u662f\\(O(2^{n/2})\\), \u6ce8\u610f\u8fd9\u91cc\u6307\u7684\u662f\u627e\u5230\u6709\u4e24\u4e2a\u4eba\u540c\u4e00\u5929\u751f\u65e5, \u800c\u4e0d\u662f\u627e\u5230\u7279\u5b9a\u67d0\u4e00\u5929\u751f\u65e5\u7684\u4e24\u4e2a\u4eba, \u56e0\u6b64\u8981\u66f4\u5bb9\u6613\u4e00\u70b9, \u6240\u4ee5\u662fn/2. \u4e5f\u56e0\u6b64, \u6211\u4eec\u5982\u679c\u9700\u8981\u4fdd\u8bc12\u768480\u6b21\u7684\u8fd0\u7b97\u6210\u672c, \u81f3\u5c11\u9700\u89812\u7684160\u6b21\u7684\u603b\u91cf\u7a7a\u95f4, \u4e5f\u5c31\u662f160\u4f4d                         - Second preimage resistance: \u7ed9\u5b9a\u4e00\u4e2ax, \u627e\u5230\u80fd\u751f\u6210\u540c\u6837y\u7684x'                 - Message authentication codes (MACs): MACk(m) as a key-dependent                   hash \u6d88\u606f\u8ba4\u8bc1\u7801, \u6d88\u606f\u88ab\u6b63\u786e\u4f20\u9001\u4e86\u5417                 - N.B. Cannot say who created hash values. If this is an issue in an application, MACs are an alternative since a MAC is produced by someone who knows the used key.         - Notes             - ((63ce77e4-7243-4b92-9505-e550b80fad78))                 - Block ciphers trade off this tension between the degree of security and the                   practical utility of a cryptosystem. \u5757\u52a0\u5bc6\u5728\u5b89\u5168\u7a0b\u5ea6\u548c\u5b9e\u7528\u6027\u4e4b\u95f4\u627e\u5230\u4e86\u67d0\u4e2a\u5e73\u8861                 - \u660e\u6587\u5757\u7684\u957f\u5ea6\u4e3a128bits, \u5bc6\u6587\u5757\u7684\u957f\u5ea6\u4e5f\u4f1a\u662f128bits, \u4f46\u662f\u5bc6\u94a5\u7684\u957f\u5ea6\u53ef\u4ee5\u662f\u4efb\u610f\u7684, \u53d6\u51b3\u4e8e\u52a0\u5bc6\u7b97\u6cd5\u5982\u4f55\u5e94\u7528             - ((63ce7876-5dc2-42d8-8ab6-37f4f3583e8f))               collapsed:: true                 - ((63ce790e-7e63-492a-bce7-65b3440c1c0d))                 - DES is a instance of the above feistel cipher, \u8fd8\u6709key scheduling, Feistel funcion, pre-processing prior\u4ee5\u53capost-processing.                   collapsed:: true                     - \u7b2c\u4e00\u6b65\u9996\u5148\u662fpre-processing, \u5bf9\u660e\u6587\u6309\u7167\u4e00\u5b9a\u89c4\u5219\u8fdb\u884cpermutation, \u79f0\u4e4b\u4e3aInitial Permutation IP                       \\(L_0R_0 = IP(m)\\)                     - \u7b97\u6cd5\u5982\u4e0b, \u6bcf\u4e00\u8f6e\u7ed3\u675f\u540e, \u4f1a\u5bf9\u5de6\u53f3\u4e24\u8fb9\u4ea4\u6362, \u7ed9\u5230\u4e0b\u4e00\u8f6e, \u7531\u4e8e\u53f3\u8fb9R\u662f\u539f\u5c01\u4e0d\u52a8\u5730\u4f20\u9012\u4e0b\u6765\u7684, \u56e0\u6b64\u4e0b\u4e00\u8f6e\u7684\u5de6\u8fb9\u8f93\u5165L_i\u7b49\u4e8eR_i-1, \u800c\u4e0b\u4e00\u8f6e\u7684\u53f3\u8fb9\u8f93\u5165, \u5219\u8981\u5bf9\u4e0a\u4e00\u8f6e\u7684\u5de6\u8fb9\u8f93\u5165\u8fdb\u884c\u52a0\u5bc6\u64cd\u4f5c\u540e\u624d\u80fd\u5f97\u5230                     - ((63ce7a45-de3b-490c-9f14-14b4c83ac8bf))                     - \u8fd9\u91cc\u7684post-processing \u5305\u62ec\u4e86\\(L_16R_16 - R_16L_16\\)\u7684swap\u4ee5\u53caapply inverse\u7684initial permutation; \u5fc5\u987bswap\u624d\u80fd\u8ba9L1\u2018 \u7531R2\u66ff\u4ee3, R1\u2019\u7531L2\u66ff\u4ee3, \u56e0\u4e3a\u4e0a\u8ff0\u7b97\u6cd5\u7684\u8fc7\u7a0b\u662f\u7b2c\u4e00\u6b65\u5c31\u9700\u8981\u4ea4\u6362, \u89e3\u5bc6\u8fc7\u7a0b\u5982\u4e0b Exercise 16                     - ((63ce7d65-9b1b-48fb-aeb1-94c5b19eb3f7))                     - we can now reason that L\u20322 = R1 and R\u20322 = L1. Similarly, we get that L\u20323 = R0 and R\u20323 = L0.                     - Feistel function F: Does it contain enough non-linear behavior to ensure enough information-theoretic resiliency? #Question                     - Differential Cryptanalysis: a chosen-plaintext attack #\u672f\u8bed\u5361                       id:: 63ce8f73-25c8-48cc-817a-d17069e19ad3                       collapsed:: true                         - \u4e8b\u5148\u9009\u62e9\u4e00\u5b9a\u6570\u91cf\u660e\u6587, \u8ba9\u88ab\u653b\u51fb\u7684\u52a0\u5bc6\u7b97\u6cd5\u52a0\u5bc6, \u83b7\u5f97\u5bc6\u6587, \u901a\u8fc7\u8fd9\u4e2a\u8fc7\u7a0b\u83b7\u5f97\u52a0\u5bc6\u7b97\u6cd5\u7684\u4e00\u4e9b\u4fe1\u606f                           id:: 63ce99c0-c9dc-4243-be5c-10c053910d72                         - id:: 63ce8fcc-9af0-40f7-9e61-191827bad0d6                           1. choosing m and m\u2032 with particular differences                           2. computing c = ek(m) and c\u2032 = ek(m\u2032)                           3. analyzing the \u201cdifference\u201d c \u2295 c\u2032                           4. repeating the first three steps as often as needed                     - Linear Cryptanalysis: #\u672f\u8bed\u5361                       collapsed:: true                         - this approximates all non-linear components with linear functions; for example the S-boxes of DES; then it does a probabilistic analysis of this approximated cipher to infer information about the key.                           collapsed:: true                             -             - Hash Functions             - Preimage resistance               collapsed:: true                 - A hash function h is preimage resistant if, given an arbitrary y in the image of h, it should be computationally infeasible to compute an x with h(x) = y. \u7ed9\u5b9a\u4e00\u4e2a\u54c8\u5e0c\u503c, \u627e\u5230\u4e00\u4e2a\u80fd\u5bf9\u5e94\u4e0a\u7684x             - Collision resistance               collapsed:: true                 - A hash function h is collision-resistant if it is computationally infeasible to find two inputs x 6 = x\u2032 such that h(x) = h(x\u2032). \u627e\u5230\u4e24\u4e2a\u5177\u6709\u76f8\u540c\u54c8\u5e0c\u503c\u7684x             - second-preimage resistane               collapsed:: true                 - A hash function h is second-preimage resistant if, for an arbitrary input x, it is computationally infeasible to find an input x\u2032 with x != x\u2032 such that h(x) = h(x\u2032). \u7ed9\u5b9a\u4e00\u4e2ax, \u627e\u5230\u4e00\u4e2a\u5177\u6709\u76f8\u540c\u54c8\u5e0c\u503c\u7684x\u2018         - Exercises 16-23           collapsed:: true             - ((63ce9874-47a7-4c4a-986f-d580161a3b7e))             - ((63ce983b-fbce-4b4f-83f1-52134a26bb84))               collapsed:: true                 - ((63ce98c5-badc-4bc2-9b3c-12ee0111bd21))             - ((63ce98d6-b321-4e4f-820a-9a29343541eb))               collapsed:: true                 - 1. ECB\u6a21\u5f0f\u4e0b, \u67d0\u4e2aciphertext block\u7684\u4e00\u4e2abit\u6539\u53d8\u4e86, \u53ea\u4f1a\u5f71\u54cd\u5230\u8fd9\u4e00\u4e2ablock\u7684\u660e\u6587, \u4f46\u662f\u8fd9\u4e2a\u533a\u5757\u4e2d50%\u7684\u5185\u5bb9\u9700\u8981\u88ab\u6539\u53d8                   2. CBC\u6a21\u5f0f\u4e0b, avalanche effect\u4f1a\u5bfc\u81f4\u8fd9\u4e2a\u533a\u5757\u548c\u4e4b\u540e\u7684\u4e00\u4e2a\u533a\u5757\u90fd\u88ab\u5f71\u54cd                      3.ECB\u6a21\u5f0f\u4e0b, \u63d2\u5165\u4e86\u4e00\u4e2abit, \u6700\u540e\u5220\u9664\u4e00\u4e2a, \u4f1a\u5bfc\u81f4\u8fd9\u4e2ablock\u548c\u540e\u9762\u6240\u6709block\u90fd\u88ab\u5f71\u54cd                 - 4. CBC\u6a21\u5f0f\u4e0b\u540c\u6837\u5982\u6b64             - ((63ce9a5f-86b1-420d-b7b6-6521d3c9fcdd))               collapsed:: true                 - \u5982\u679cOFB\u7684key stream\u662f\u5b8c\u5168random\u7684, \u90a3\u5176\u5b9e\u5c31\u662fa instance of onetime-pad                 - \u4f46\u662fOFB\u53ea\u6709\u7b2c\u4e00\u4e2akey\u662frandom\u7684, \u540e\u9762\u90fd\u662fdepend\u4e8e\u8fd9\u4e2akey. \u56e0\u6b64\u5e76\u4e0d\u662f\u4e00\u6b21\u6027\u5bc6\u7801\u672c             - ((63ce9adc-1602-4899-8153-2af7c432d6af))               collapsed:: true                 - 1. CTR \u53ef\u4ee5parallel\u7684\u539f\u56e0: \u4e00\u65e6IV known\u4e86, \u6211\u4eec\u4e5f\u80fd\u591f\u77e5\u9053IV + i, \u5bf9\u4e8e\u6240\u6709\u7684i, \u800c\u5982\u679c\u6211\u4eec\u77e5\u9053m, \u90a3\u5c31\u76f4\u5230\u6240\u6709\u7684m blocks, \u5c31\u53ef\u4ee5evaluate\u540c\u65f6\u6240\u6709. \u4f46\u662f\u5982\u679cm\u662f\u4e00dcv\u4e2a\u4e2ablock\u63d0\u4f9b\u7684, \u5c31\u5f97\u4e00\u4e2a\u4e2a\u6765\u4e86                   2. CBC, OFB, CFB\u65e0\u6cd5parallel, \u56e0\u4e3aCBC \u4f9d\u8d56\u4e8e\u4e0a\u4e00\u4e2a\u5bc6\u6587, OFB\u4f9d\u8d56\u4e8e\u4e0a\u4e00\u4e2aY key, CFB\u4f9d\u8d56\u4e8e\u4e0a\u4e00\u4e2a\u5bc6\u6587                   3. OFB\u7684correctness for decryption:                       ((63ce9c52-a5a9-4160-9356-025f1b5591e1))                 - 4. \u52a0\u5165\u660e\u6587block\u957f\u5ea6\u4e3a a bits, \u7ed9\u5230\u4e00\u4e2aIV, CTR\u6a21\u5f0f\u6700\u9ad8\u652f\u6301\u7684block\u6570\u91cf\u662f                      \\(2^a - 1 - IV\\)             - Hash Functions             - ((63cea12d-5fc3-4136-ad82-a4df334fa476))               collapsed:: true                 - \u8981\u60f3\u8981\u627e\u5230\u4e24\u4e2a\u4eba\u7684\u751f\u65e5\u662f\u76f8\u540c\u7684, 23\u4e2a\u4eba, \u5c31\u6709\u8d85\u8fc750%\u7684\u6982\u7387\u4e86, \u8fd9\u662f\u56e0\u4e3a\u6211\u4eec\u7b971\u51cf\u53bb\u6240\u6709\u4eba\u751f\u65e5\u90fd\u662f\u4e0d\u540c\u7684\u6982\u7387, \u4e5f\u5c31\u662f1-(365364363...365-N+1)/365^N, 50%\u7684\u6982\u7387\u57fa\u672c\u4e0a\u5c31\u662f365\u5f00\u6839\u53f7, \u800c70\u4e2a\u4eba\u751a\u81f399.9%\u6709\u4fe9\u4eba\u540c\u4e00\u5929\u751f\u65e5, \u6240\u4ee5collision resistance\u7684compute time\u662fO(2^n/2), \u5373\u5f00\u4e86\u4e2a\u6839\u53f7             - ((63cea416-5f7d-4461-8f52-5ea3c3e11ec7))               collapsed:: true                 - \u4f8b\u5982 cannot performed in probabilistic polynominal time             - ((63cea45f-1475-4483-bf5b-7ffb8e884992))               collapsed:: true                 - \u7b2c\u4e8c\u539f\u8c61\u6307\u7684\u662f\u7ed9\u5b9a\u4e00\u4e2ax, \u627e\u5230\u4e00\u4e2a\u5177\u6709\u76f8\u540cy\u7684x'                 - ((63cea4b7-b0df-42a8-aba4-33c75e4bd5ba))         - \u5168\u5c40\u7406\u89e3             - Block ciphers \u5c06\u660e\u6587\u5206\u6210\u4e86\u7b49\u957f\u7684\u5757, \u4f20\u7edf\u7684\u52a0\u5bc6\u65b9\u5f0f\u4e5f\u53ef\u4ee5\u7406\u89e3\u4e3a\u5757\u7684\u5927\u5c0f\u4e3a\u4e00\u4e2a\u5b57\u6bcd, \u6216\u8005\u4e00\u4e2abit. \u4f46\u662f\u8fd9\u91cc\u7684Block cipher\u4f1a\u4f7f\u7528\u540c\u6837\u4e00\u4e2akey\u5bf9\u6bcf\u4e2a\u5757\u8fdb\u884c\u52a0\u5bc6, \u5e76\u4e14\u6bcf\u4e2a\u5757\u7684\u5927\u5c0f\u4f1a\u66f4\u5927\u6765\u63d0\u5347\u6548\u7387, \u4f8b\u5982DES\u768464, AES\u7684128. \u4e3a\u4e86\u4fdd\u8bc1\u52a0\u5bc6\u7b97\u6cd5\u7684non-deterministic, \u5404\u79cdmode\u4f1a\u88ab\u4f7f\u7528\u6765\u521b\u5efa\u4e00\u4e9b\u524d\u540e\u5757\u7684\u5173\u8054, \u6216\u662f\u4e00\u6b21\u6027\u7684\u4fe1\u606f\u6765\u8ba9\u4e00\u6837\u7684\u4fe1\u606f\u5757\u4e5f\u80fd\u88ab\u52a0\u5bc6\u6210\u4e0d\u4e00\u6837\u7684\u5bc6\u6587             - DES\u5bf9\u6bcf\u4e2a64\u4f4d\u7684\u5757\u752856\u4f4d\u7684key\u8fdb\u884c\u52a0\u5bc6, \u52a0\u5bc6\u65f6\u5206\u5de6\u53f3\u90e8\u5206, \u6709\u8f6e\u6570, \u6bcf\u4e00\u8f6e\u7684\u5b57\u5bc6\u94a5\u90fd\u7531key\u751f\u6210, \u53ef\u590d\u73b0. \u8fd9\u4e2a\u7ed3\u6784\u5f0fFeistel Cipher. \u4f18\u52bf\u662f \u52a0\u89e3\u5bc6\u7528\u7684\u540c\u4e00\u4e2a\u7ed3\u6784, \u65b9\u4fbf\u5feb\u901f.             - AES\u66f4\u4e3a\u590d\u6742, 256bit\u7684key\u5c31\u80fd\u5b9e\u73b0\u4e0d\u9519\u7684\u52a0\u5bc6\u6548\u679c             - \u5bc6\u6587\u7684\u957f\u5ea6\u548c\u660e\u6587\u4e00\u6837\u957f             - differential and linear cryptanalysis: chosen-text attack\u6765\u5206\u6790\u660e\u6587\u5bc6\u6587\u5bf9\u5e94\u5173\u7cfb; \u7528linear function \u4f30\u8ba1non-linear (\u8f6e\u51fd\u6570DES)             - ECB: \u5757\u4e0e\u5bc6\u6587deterministic \u5bf9\u5e94, \u6539\u53d8\u5206\u7ec4\u987a\u5e8f\u5f88\u5bb9\u6613             - CBC: \u63a8\u8350, \u52a0\u5bc6(\u4e0a\u5bc6 XOR m), \u672c\u5bc6\u6587\u7684\u6539\u53d8, \u4f1a\u5f71\u54cd\u81ea\u5df1\u7684\u89e3\u5bc6\u548c\u4e0b\u4e00\u4e2a\u7684\u89e3\u5bc6; \u89e3\u5bc6\u7684\u65f6\u5019\u7531\u4e8e\u77e5\u9053\u6240\u6709c, \u56e0\u6b64\u53ef\u4ee5\u5e76\u884c\u8fd0\u7b97, \u5e76\u89e3\u5bc6\u4efb\u610f\u5206\u7ec4; \u9700\u8981IV\u548ck             - OFB: \u5bf9IV\u8fdb\u884c\u8fed\u4ee3\u52a0\u5bc6\u751f\u6210\u6bcf\u4e2a\u5757\u7684XOR\u5bf9\u8c61, XOR\u5bf9\u8c61\u6d41\u53ef\u4ee5\u5355\u72ec\u751f\u6210, \u8fd9\u4e2a\u5bf9\u8c61\u6d41\u4e0eone-time pad\u7684key\u5f88\u76f8\u4f3c, \u4f46\u662f\u5e76\u4e0d\u662f\u5b8c\u5168\u968f\u673a\u7684. \u7531\u4e8e\u6309\u4f4d\u5f02\u6216, \u53ea\u6709\u5bf9\u5e94bit\u4f1a\u51fa\u9519             - CFB: \u52a0\u5bc6(\u4e0a\u5bc6) XOR m, \u5bf9\u4e0a\u4e2ablock\u7684\u5bc6\u6587\u52a0\u5bc6\u4e0em XOR             - CTR: \u63a8\u8350 \u52a0\u5bc6(\u8ba1\u6570\u5668) XOR m\u968f\u673aIV\u4f5c\u4e3a\u8ba1\u6570\u5668\u7684\u57fa, \u7b2c\u51e0\u4e2a\u5c31\u5f80IV\u52a0\u51e0, \u56e0\u6b64\u53ef\u4ee5\u5b8c\u5168\u5e73\u884c\u8fd0\u7b97. \u5757\u7684\u6570\u91cf\u88abIV\u5927\u5c0f\u9650\u5236, 2^l - 1 - IV\u4e2a\u5757\u6700\u591a. \u7531\u4e8e\u6309\u4f4d\u5f02\u6216, \u53ea\u6709\u5bf9\u5e94bit\u4f1a\u51fa\u9519     - Week4 Hash, MAC, RSA, DH       collapsed:: true         - 24-30, 53-54         - \u6570\u8bba           collapsed:: true             - Euler Totient               collapsed:: true                 - \u5c0f\u4e8en\u7684, \u4e0en\u4e92\u8d28\u7684\u6570\u7684\u4e2a\u6570                 - (p\u22121)\u00b7(q\u22121)             - \u539f\u6839 primitive root               collapsed:: true                 -                  -              - \u6b27\u62c9\u5b9a\u7406               collapsed:: true                 -              - FACTORING problem: given N, known to be the product of two               primes, find these primes:               given an RSA modulus N, find primes p and q with N = p \u00b7 q             - Discrete Logarithm Problem:\u201cFor appropriate choices of prime p and element g, it is hard to compute a knowing ga mod p.\u201d         - Slides           collapsed:: true             - \u56de\u987e\u4e00\u4e0bhash\u7684\u4e09\u4e2aresistence, \u6309\u7167\u96be\u5ea6\u4ece\u4f4e\u5230\u9ad8, second &gt;= preimage               collapsed:: true                 - Collision resistance: \u627e\u5230\u6709\u4e24\u4e2ax, \u62e5\u6709\u76f8\u540c\u7684hash                 - Preimage resistance: \u7ed9\u5b9ay, \u627ex                 - second preimage resistance: \u7ed9\u5b9ax, \u627e\u4e00\u4e2a\u6709\u76f8\u540cy\u7684x\u2018             - Hash function design               collapsed:: true                 - ((63d6e861-dca7-433a-973a-135b7947b0a1))                   collapsed:: true                     - \u5047\u8bbeOracle\u53ef\u4ee5\u5e2e\u6211\u4eec\u7ed9y\u627e\u5230\u4e00\u4e2a\u968f\u673a\u7684x, \u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u5229\u7528\u8fd9\u4e2aoracle\u7834\u574fcollision resistance                     - \u968f\u673a\u9009\u62e9\u4e00\u4e2ax, \u5e76\u8ba1\u7b97\u51fa\u5b83\u7684y                     - \u628ay\u7ed9\u5230oracle, \u5f97\u5230\u4e00\u4e2ax'                     - \u5982\u679cx\u2018\u548cx\u4e0d\u4e00\u6837\u5c31\u627e\u5230\u4e86collision, \u6ca1\u6709\u7684\u8bdd\u56de\u5230\u7b2c\u4e00\u6b65                 - ((63d6e8e2-4172-4787-803a-d431d54599cd))                   collapsed:: true                     - \u8981\u8bc1\u660e\u78b0\u649e\u62b5\u6297, \u5c31\u5f97\u8bc1\u660e\u76f8\u540c\u7684h, \u5bf9\u5e94\u7684\u662f\u76f8\u540c\u7684x                     -                        collapsed:: true                         -                 - ((63d6ea95-e4e9-4d8b-ba59-3bd5627ccdd8))                   collapsed:: true                     -                      - \u7528\u6709\u9650\u57df\u7684\u538b\u7f29\u51fd\u6570f, \u6765\u6784\u5efa\u4e00\u4e2a\u53ef\u62b5\u6297\u78b0\u649e\u7684hash\u51fd\u6570                 - ((63d6eb26-d4ce-4fab-9904-9126d5f922b6))                   collapsed:: true                     - Avalanche effect desired: small changes in the input should result in                       unpredictable changes in the output. \u9700\u8981\u6539\u53d8\u4e00\u70b9, \u5168\u5c40\u90fd\u80fd\u5927\u53d8\u5316                     - MD4, MD5, SHA-0, SHA-2, SHA-3                 - ((63d6ebca-906e-413a-bb46-b71eb2b8d74f))                   collapsed:: true                     - Davies-Meyer hash                     - ((63d6ebf6-011d-4bc5-9ef0-ff7c5eb075d0))                     - ((63d6ec04-60a3-4ca0-a2d1-44b4a252cdda))                     - \u5229\u7528\u8fed\u4ee3\u7684\u65b9\u5f0f, \u4e0a\u4e00\u5757\u7684\u7ed3\u679c, \u4f1a\u7528\u6765\u653e\u8fdb\u4e0b\u4e00\u5757\u91cc\u9762, \u538b\u7f29\u51fd\u6570\u662f\u7528block cipher\u52a0\u5bc6, \u5bc6\u94a5\u7528\u7684\u662f\u6570\u636ex                 - ((63d6f248-a7eb-4e35-a00d-4b329924f072))                   collapsed:: true                     - ((63d6f9d1-1e9c-4b6c-b528-811eae39c428))                     - the x is used now as \u201cdata\u201d and not as key and where we make use of a key                       derivation function (KDF) g             - ((63d6fa18-b74a-47c8-b807-7ea467970fc2))               collapsed:: true                 - ((63d6fa27-680a-43b6-9000-e425c7bc8ecb))                 - \u7531\u4e8eh\u5df2\u77e5, \u4e2d\u95f4\u4eba\u53ef\u4ee5\u5f88\u8f7b\u6613\u7684\u7528\u81ea\u5df1\u7684\u6d88\u606f, \u4fee\u6539. \u56e0\u6b64\u6211\u4eec\u9700\u8981\u8fd9\u4e2ahash key-dependent, \u6765\u4fdd\u8bc1\u53ea\u6709\u4ed6\u4eec\u4e24\u4e2a\u4eba\u53ef\u4ee5                 - ((63d6fa91-0bd5-4403-9533-a61378385a1f))                   collapsed:: true                     - MAC codes give us integrity, not confidentiality. An idea for both confi-                       dentiality and integrity:                     - \u7ed9\u5230\u5bc6\u6587\u548c\u5bc6\u6587\u7684mac, \u4fdd\u8bc1\u7684\u662f\u5bc6\u6587\u7684integrity                     - \u53ea\u6709\u62e5\u6709key\u7684\u4eba\u624d\u80fd\u521b\u5efaMAC, \u624d\u80fd\u9a8c\u8bc1MAC\u7684integrity; \u5f53\u7136key\u4e5f\u9700\u8981key management, \u5982\u4f55\u5b58\u50a8, \u8f6c\u79fb, \u4ea4\u6362\u5bc6\u94a5\u662f\u4e00\u4e2a\u5927\u95ee\u9898                 - ((63d7075b-574c-4050-9845-b6321fa91415))                   collapsed:: true                     - \\(H_{i+1} = f (H_i || m_i)\\)                     - \u901a\u8fc7\u8fed\u4ee3\u5173\u7cfb, \u8f7b\u677e\u7684\u4e0d\u9700\u8981key, \u53ea\u9700\u8981\u5229\u7528\u524d\u4e00\u4e2a\u7684\u5bc6\u6587, \u5c31\u53ef\u4ee5\u5f97\u5230\u52a0\u4e86\u4e00\u4e9b\u5185\u5bb9\u7684\u65b0MAC                     - ((63d7089c-f2ea-42b0-9a5b-42f4a1bae336))                 - ((63d70944-02c3-4929-82ff-193360d5a4e1))                   collapsed:: true                     - ((63d70951-968b-42ee-a3fe-0294689c2977))             - ((63d70a01-38fc-486a-8e2d-266810c416b0))               collapsed:: true                 - \u4e3a\u4ec0\u4e48\u60f3\u53d1\u660e\u516c\u94a5\u5bc6\u7801?                   collapsed:: true                     - \u5bf9\u79f0\u5bc6\u94a5\u9700\u8981\u88ab\u5b89\u5168\u5730\u4f20\u8f93\u5206\u4eab                     - \u516c\u94a5\u5bc6\u7801\u4e0d\u9700\u8981, \u516c\u5171\u5bc6\u7801\u662f\u516c\u5f00\u7684, \u4f46\u662f\u9700\u8981\u6ce8\u610f\u516c\u94a5\u7684\u8ba4\u8bc1\u95ee\u9898: \u600e\u4e48\u6765\u5224\u65ad\u8fd9\u4e2a\u516c\u94a5\u5c31\u662f\u4f60\u6216\u8005\u67d0\u7f51\u7ad9\u7684\u516c\u94a5\u5462\uff1f                 - [[RSA]] Factoring problem                   collapsed:: true                     - primes &gt; 1024 bits, key &gt; 2048 bits                     -                      -                      -                      -                      - Problem                       collapsed:: true                         - \u76f8\u540c\u7684\u660e\u6587\u610f\u5473\u7740\u76f8\u540c\u7684\u5bc6\u6587, \u4f1a\u51fa\u73b0ECB\u90a3\u79cd\u76f4\u63a5\u4e00\u5757\u5757\u52a0\u5bc6\u7684\u6a21\u5f0f\u4e00\u6837\u7684\u95ee\u9898, \u53ef\u4ee5\u901a\u8fc7\u7528\u968f\u673a\u6570blind m             - ((63d793c6-39df-434f-a2a4-d27e9efea6f2))               collapsed:: true                 - Diffie-Hellman Key Exchange protocol: Discrete Logarithm Problem                   collapsed:: true                     -                      - \u8fd9\u91cc\u7684g, \u751f\u6210\u5143\u5e94\u5f53\u4e3ap\u7684\u4e00\u4e2a\u539f\u6839, \u751f\u6210\u5143\u7684\u4e2a\u6570\u4e3aphi(p-1) \u4e5f\u5c31\u662fg\u76840-p-1\u6b21\u65b9\u80fd\u591fmod p\u5f97\u52301-p-1\u6240\u6709\u6570\u5b57, \u8fd9\u6837\u5b50\u624d\u8db3\u591f\u968f\u673a, \u56e0\u4e3a\u4e0d\u4f1a\u6709\u4e24\u4e2aa\u649e\u8f66\u5f97\u5230\u4e00\u6837\u7684g^a mod p                 - Discussion                   collapsed:: true                     - works in any mathematical group in which operation f (a) = ga is hard                       to \u201cinvert\u201d                     - both agents generate parts of the shared key, more trustworthy                     - satisfies forward secrecy: if any longterm keys of Alice or Bob get com-                       promised, it won\u2019t compromise secrecy of shared key K: \u4e00\u65b9\u7684key\u6cc4\u9732\u4e0d\u5f71\u54cd\u5171\u4eabkey                     - elliptic curves\u53ef\u4ee5\u7528\u66f4\u5c11\u7684bits \u751f\u6210\u5b89\u5168\u7684\u5bc6\u94a5                     - key \u901a\u5e38\u9700\u8981\u88abtrim, \u4f8b\u5982\u4f7f\u7528hash func                 - Man in the middle attack                   collapsed:: true                     -                        collapsed:: true                         -         - Notes             - second-preimage resistance \u662f\u6bd4collision resistance \u8981\u5f3a\u7684, \u56e0\u6b64\u80fd\u591fbreak second preimage resistance \u5c31\u53ef\u4ee5\u5f88\u6709\u6548\u7684\u53bb\u6253\u7834\u78b0\u649e\u62b5\u6297\u4e86             - \u600e\u6837\u8bc1\u660ecollision resistance\u5462? \u7ed9\u5b9a\u4e24\u4e2a\u4e0d\u540c\u7684x, \u4ed6\u4eec\u751f\u6210\u7684h\u662f\u76f8\u540c\u7684, \u5982\u679c\u6839\u636e\u8ba1\u7b97\u7ed3\u679c, \u4e24\u4e2ax\u5e94\u5f53\u662f\u76f8\u540c\u7684, \u90a3\u5c31\u662fcollision resistance\u4e86             - Hash Function Construction               collapsed:: true                 - ((63d7c620-3001-4b31-ae92-89d57dd14ffa))                   collapsed:: true                     - ((63d7c64f-0cb0-4ce5-9060-194700bd0723))                     - \u4ee5\u4e0a\u7684\u7b97\u6cd5\u5e76\u4e0d\u80fd\u591f\u62b5\u5fa1\u78b0\u649e, \u4f8b\u59828-&gt;4\u4f4d\u7684\u6620\u5c04, \u5982\u679cmi\u4f4d\u6570\u4e0d\u591f\u7684\u8bdd\u9700\u8981padding, \u4f8b\u5982m1=010, padding\u540e\u5c31\u662fm2=0100, \u4e24\u4e2a\u6570\u5b8c\u5168\u4e0d\u4e00\u6837, \u4f46\u662f\u7531\u4e8epadding\u7684\u5b58\u5728, \u4f1a\u8ba9\u4e24\u4e2a\u6570\u5b57\u7684hash\u503c\u4e00\u81f4                     - \u4f46\u662f\u53ef\u4ee5\u901a\u8fc7\u5728\u6700\u540e\u52a0\u4e00\u4e2ablock\u8bb0\u5f55\u4fe1\u606f\u7684\u957f\u5ea6\u6765\u907f\u514d\u8fd9\u4e2a\u95ee\u9898, \u4f8b\u5982 0011 \u548c0100, \u4e00\u4e2a3, \u4e00\u4e2a4, \u5c31\u4e0d\u4e00\u6837\u5566             - ((63d7cb8d-c849-43c0-bf03-ef14d31408c2))               collapsed:: true                 - \u6211\u4eec\u9700\u8981\u52a0\u5bc6\u7684hash\u6765\u4fdd\u8bc1\u4e0d\u77e5\u9053\u5bc6\u94a5\u7684\u4eba\u65e0\u6cd5\u4fee\u6539\u4fe1\u606f, \u4e5f\u53ea\u6709\u77e5\u9053\u5bc6\u94a5\u7684\u4eba\u624d\u80fd\u591f\u9a8c\u8bc1                 - ((63d7cbd9-42ad-4e70-901a-d051aa0f51e5))                 - MAC\u503c\u5c31\u662f\u6240\u8c13\u7684[[integrity tag]]                 - Security properties                   collapsed:: true                     - \u62e5\u6709MAC\u503c\u7684\u9ed1\u5ba2\u65e0\u6cd5\u627e\u5230\u4e00\u6761\u4e0e\u539f\u6587\u4e0d\u540c\u7684m\u2018\u4f46\u662f\u62e5\u6709\u76f8\u540c\u7684MAC\u503c.                     - \u552f\u4e00\u7684\u95ee\u9898\u5c31\u662fkey management                     - \u666e\u901a\u7684MAC, \u76f4\u63a5\u5bf9\u660e\u6587\u64cd\u4f5c\u7684\u8bdd\u662f\u4e0d\u4fdd\u8bc1confidentiality\u7684 , \u5982\u679c\u8981\u4fdd\u5bc6\u7684\u8bdd, \u90a3\u5c31\u5f97\u5bf9\u52a0\u5bc6\u7684\u5bc6\u6587\u8fdb\u884cMAC\u64cd\u4f5c                     -                        collapsed:: true                         - \u4e24\u90e8\u5206\u7ec4\u6210, \u5bc6\u6587\u548c\u5bc6\u6587\u7684MAC, \u4e24\u4e2a\u5bc6\u94a5\u8981\u4e0d\u540c, \u56e0\u4e3a\u52a0\u5bc6\u673a\u5236\u53ef\u80fd\u5c31\u662f\u4e0d\u540c\u7684, \u5e76\u4e14\u6211\u4eec\u8981limit the use of a key for a specific purpose, \u4f8b\u5982\u8fd9\u91cc\u5c31\u662fC\u548cI\u4e24\u4e2apurposes, \u8fd9\u6837\u5c31\u9700\u8981\u53cc\u91cd\u7834\u89e3\u4e86                         - \u4e0a\u8ff0\u7684\u505a\u6cd5\u90fd\u662f\u9488\u5bf9\u5bc6\u6587\u800c\u8a00\u7684, \u6709\u4e2a\u6f5c\u5728\u95ee\u9898\u5c31\u662f\u8fd9\u4e2aHorton\u2019s principle \u201csigns what is meant (the m1), not what is being said (the ek1 (m))\u201d. \u56e0\u6b64\u53ef\u4ee5\u6539\u8fdb\u4e3a                           MAC\u76f4\u63a5\u5bf9\u660e\u6587m\u8fdb\u884c\u64cd\u4f5c                     - Horton\u2019s Principle states that it is wise to sign what is meant and not what is being said. but not in blinding protocols for anonymity solutions.                     - This is a good principle. However, there are special use contexts were it                       is beneficial to sign what is being said, for example in blinding protocols for                       anonymity solutions. \u7531\u4e2d\u95f4\u4eba\u7684\u65f6\u5019\u5c31\u5e94\u8be5sign e\u4e86                 - MAC Construction                   collapsed:: true                     - ((63d7cede-df34-42da-9b1b-810d988a4b7c))                     - MAC post-processing:                       collapsed:: true                         - CBC-MAC \u7528\u4e86block cipher in CBC mode                         -                          - m_q \u9700\u8981padding, \u65b9\u5f0f\u67090 padding, \u6216\u8005\u518d\u52a0\u4e0a\u4e00\u4e2a\u957f\u5ea6block                         - \u7b2c\u4e00\u4e2a\u65b9\u6cd5\u81f4\u547d\u7f3a\u9677\u5728\u4e8e\u65e0\u6cd5\u77e5\u9053\u54ea\u4e9b\u5185\u5bb9\u771f\u7684\u662f\u539f\u6587                         - \u7b2c\u4e8c\u4e2a\u65b9\u6cd5\u5219\u53ef\u4ee5\u901a\u8fc7\u957f\u5ea6\u5f97\u77e5, \u4f46\u662f\u4f1a\u5bf9\u660e\u6587\u6709\u957f\u5ea6\u9650\u5236             - ((63d7d3c0-6cec-48b1-af88-d5b5ae1969e2)) Discrete Logarithm problem               collapsed:: true                 -                  - ((63d7d507-de34-461c-adbe-3c637d2bc608))                   collapsed:: true                     - ((63d7d515-a6dc-4521-bcf1-237e9ec8c965))                 - ((63d7d5f2-2e16-4c7d-9d9b-bc042d8e6cd1))                   collapsed:: true                     - ((63d7d613-d235-444e-865b-38a51bcd9271))                     - ((63d7d636-e4ef-48d1-86a1-7f7cd97b4730))                     -                     -             - Menti                 -                  - 1,2,5\u662f\u5bf9\u7684, \u4efb\u4f55\u9700\u8981\u8eab\u4efd\u8ba4\u8bc1, \u786e\u8ba4\u8eab\u4efd\u8d23\u4efb\u7684\u5730\u65b9\u9700\u8981\u7528\u5230MAC                 -                      - 1         - \u5168\u5c40\u7406\u89e3           collapsed:: true             - Collision resistance does not imply preimage resistance, \u56e0\u4e3a\u4f60\u53ef\u4ee5\u6784\u5efa0\u5f00\u5934\u540e\u9762\u6570\u5b57\u90fd\u662f\u4e0d\u4e00\u6837\u7684\u76f4\u63a5\u7528\u539f\u6587\u8868\u793a\u548c\u4e00\u4e2a1\u5f00\u5934\u540e\u9762\u662fcollision resistance\u7684hash fun, \u4f46\u662f\u53ea\u8981\u662f0\u5f00\u5934\u5c31\u80fd\u77e5\u9053\u539f\u6587             - \u4e09\u79cdresistance \u6ca1\u6709\u5b70\u5f3a\u5b70\u5f31, \u751f\u65e5\u6096\u8bba\u5bf9\u4e8e\u8ba1\u7b97\u91cf\u7684\u51cf\u5c11\u662f\u6839\u53f72\u7684             - DM \u548cMMO hash \u90fd\u7528\u5230\u4e86block cipher, \u4f46\u662f\u52a0\u5bc6\u7684\u5bc6\u94a5\u9009\u7528\u4e0a\u6709\u533a\u522b, \u4e00\u4e2a\u662f\u7528data x, \u4e00\u4e2a\u662f\u7528key derivation function             - MAC \u8ba9\u6709key\u7684\u4eba\u624d\u80fd\u9a8c\u8bc1\u6d88\u606f\u7684MAC\u5bf9\u4e0d\u5bf9, \u4e5f\u662f\u53ef\u4ee5\u7528block cipher \u6765\u5b9e\u73b0, \u56e0\u4e3a\u90fd\u662f\u5f88\u957f\u7684\u4fe1\u606f\u6d41. MAC\u7684\u4f5c\u7528\u662f\u5728\u672a\u7ecf\u8ba4\u8bc1\u7684\u901a\u9053\u4f20\u8f93\u7528\u4e8e\u9a8c\u8bc1\u8eab\u4efd             - RSA \u5c31\u662fmod pq\u7684\u7684\u4e00\u4e2a\u52a0\u5bc6\u624b\u6bb5, \u5229\u7528\u4e86\u4efb\u4f55\u6709\u9650\u7fa4\u91cc\u7684\u5143\u7d20\u7684phi(N)\u65b9\u90fd\u662f1\u8fd9\u4e2a\u62c9\u683c\u6717\u65e5\u5b9a\u7406\u5e26\u6765\u7684\u7279\u6027, \u56e0\u4e3ad\u00b7e=1+s(p-1)(q-1)     - Week5 Elliptic Curves, Finite Fields       collapsed:: true         - 63, 41-42, 55-58, 60         - Slides             - Elliptic Curves for Public Key Cryptography               collapsed:: true                 - \\(y^2 = x^3 + ax + b\\)                 - Elliptic Curve Discrete Logarithm Problem: Given two points P and Q on an elliptic curve such that Q is a multiple of P , it is a hard problem to find some k such that Q = k \u22c6 P .                 -                  - \u7b80\u5355\u6765\u8bf4, \u692d\u5706\u66f2\u7ebf\u76f8\u5bf9\u4e8e\u4e4b\u524d\u76f4\u63a5\u4f7f\u7528g\u7684a\u6b21\u65b9mod N \u96be\u4ee5\u77e5\u9053a\u662f\u591a\u5c11\u6765\u8bf4, \u5229\u7528\u4e86\u692d\u5706\u66f2\u7ebf\u51fd\u6570, \u5728\u692d\u5706\u66f2\u7ebf\u4e0a\u9762\u533a\u4e00\u4e2a\u57fa\u70b9, \u901a\u8fc7\u4e0d\u65ad\u505a\u5207\u7ebf\u4ea4\u70b9\u5bf9x\u5bf9\u79f0\u64cd\u4f5c, \u4ee5\u53ca\u548c\u57fa\u70b9\u8fde\u7ebf\u53d6\u4ea4\u70b9\u7684x\u5bf9\u79f0\u64cd\u4f5c\u5f97\u5230\u65b0\u7684k * P \u70b9, \u8fd9\u4e2a\u70b9\u4f1a\u4f5c\u4e3a\u5bc6\u94a5\u4ea4\u6362\u7684\u516c\u5171\u90e8\u5206, \u522b\u4eba\u77e5\u9053\u8fd9\u4e2a\u662f\u5f88\u96be\u5f97\u77e5\u8fd9\u4e2ak\u5230\u5e95\u662f\u51e0, \u4e5f\u5c31\u662f\u5230\u5e95\u505a\u4e86\u51e0\u6b21P + P\u7684\u64cd\u4f5c, \u4f46\u662f\u5bf9\u65b9\u5728\u524d\u9762\u4e58\u4e2aj, \u548c\u81ea\u5df1\u4e58\u4e0akP\u5f97\u5230\u7684\u90fd\u662fjkP, \u8fbe\u6210\u4e86\u76ee\u7684                 - \u4f46\u662f, \u5728\u5bc6\u7801\u5b66\u4e2d, \u8fd9\u4e2a\u5e94\u7528\u7684\u662f\u6709\u9650\u57df\u4e2d\u7684\u692d\u5706\u66f2\u7ebf\u79bb\u6563log\u95ee\u9898, \u51fd\u6570\u7684\u8f93\u51fa\u88ab\u9650\u5b9a\u5728\u4e86\u4e00\u4e2aprime\u5927\u5c0f\u7684\u6709\u9650\u57df\u4e2d, \u8981\u6c42\u7684\u4e0d\u662f\u7b49\u5f0f\u5de6\u53f3\u4e24\u8fb9\u76f4\u63a5\u7b97\u51fa\u6765\u7684\u503c\u76f8\u7b49, \u800c\u662fmod p\u7684\u503c\u76f8\u7b49, \u692d\u5706\u4e0a\u7684\u70b9\u4e5f\u5c31\u88ab\u8868\u793a\u6210\u4e86\u7b26\u5408\u4e0b\u9762\u8fd9\u4e2amod \u516c\u5f0f\u7684\u70b9\u96c6\u5408, \u753b\u5728\u56fe\u4e2d \u5c06\u662f\u79bb\u6563\u7684, \u6cbfx\u8f74\u5bf9\u79f0\u7684                 - ((63e03e4a-7ec3-4918-bbe5-c8b68e324caa))                 - k * P \u5c31\u662f\u8fdb\u884c\u4e86k\u6b21 P + P\u7684\u64cd\u4f5c, \u52a0\u6cd5\u662fgroup\u5b9a\u4e49\u7684; given points P and Q on curve, it is hard to determine whether there is some k with Q = k \u22c6 P, and to compute such k; \u96be\u4ee5\u77e5\u9053\u8fdb\u884c\u4e86\u51e0\u6b21P+P\u7684\u64cd\u4f5c\u5f97\u5230Q\u70b9             - ECDSA Signature Generation               collapsed:: true                 - \u751f\u6210\u8fc7\u7a0b:                 - Let G be a generator of prime order q &gt; 2^160 in E\u2032\u2032(K). The private key for agent A is a random 256-bit number k in N. The corresponding public key for agent A is K = k \u22c6 G. The security requirement is that nobody should be able to learn the private key from public information \u2013 including the public key K. m message to be signed                 - points P on elliptic curve can be mapped to integer interval [0, q \u2212 1]:                   collapsed:: true                     - f (P ) = \u201cx-coordinate of point P \u201d mod q                 - ((63e0443b-49de-423b-a49c-9446e2f308a1))                 - \u9996\u5148\u7528\u4e86\u4e24\u904dhash, \u518d\u9009\u62e9\u4e00\u4e2a\u4e00\u6b21\u6027\u7684\u968f\u673a\u6570\u5b57\u4e58\u4e0a\u57fa\u70b9G\u5f97\u5230\u7ecf\u8fc7u\u6b21\u53d8\u6362\u540e\u7684\u70b9P, \u518d\u5bf9\u8fd9\u4e2a\u70b9P\u8fdb\u884c0-q-1\u7684map\u5de5\u4f5c\u5f97\u5230\u5904\u7406\u5b8c\u6210\u7684r, s\u7531hash, \u79c1\u94a5k, r, u\u64cd\u4f5cmod q\u5f97\u5230 (r,s)\u5c31\u662fsignature                 - \u9a8c\u8bc1\u8fc7\u7a0b                 - ((63e045b1-a22f-41c2-a350-24f16af72cfe))                 - \u751f\u6210\u540c\u6837\u7684hash, \u9a8c\u8bc1f(uG) = f(aG + bK)                 - To hedge against one of the 2 hash functions being broken                 - \u91cd\u8981\u5b89\u5168\u63d0\u793a: \u4e0d\u80fd\u4f7f\u7528\u76f8\u540c\u7684ephemeral key u\u5bf9\u4e0d\u540c\u6d88\u606f\u8fdb\u884c\u7b7e\u540d, \u56e0\u4e3aattacker\u53ef\u4ee5\u901a\u8fc7\u8fd9\u4e2a\u8ba1\u7b97\u51fa\u79c1\u94a5k                   collapsed:: true                     -             - EC math               collapsed:: true                 - [[Fields]]                   collapsed:: true                     - ((63e046b7-2572-4e89-b870-3e4f6d2c003a))                     - \u57df\u9650\u5b9a\u4e86\u8f93\u5165\u7684\u53d6\u503c\u8303\u56f4, \u4e58\u6cd5\u662f\u52a0\u6cd5\u7684\u53e0\u52a0,                     - ((63e04744-ad99-470e-b64b-4d27551b83bf))                     - Fp\u662f\u8d28\u6570\u7fa4, \u5305\u62ec\u4e860-p-1, \u6709\u9650\u7684\u503c\u57df, \u901a\u8fc7mod\u64cd\u4f5c\u6765\u91cd\u65b0\u6620\u5c04                 - [Finite fields]                   collapsed:: true                     - Let (K, +, \u2217, 0, 1) be a finite field.                     - the [[order]] of K is the size of set K, K\u7684set\u5927\u5c0f\u5c31\u662fK\u7684\u9636 order                       collapsed:: true                         - the [[characteristic]] of K is the smallest n &gt; 1 such that the nfold sum                           1 + \u00b7 \u00b7 \u00b7 + 1 equals 0, \u7279\u5f81\u662f\u51e0\u4e2a1\u52a0\u8d77\u6765\u80fdmod N\u7b49\u4e8e0; \u53ea\u6709prime power order \u7684\u57df\u624d\u6709characteristic, 2\u76843\u6b21size\u7684\u57df, characteristic\u8fd8\u662f2, \u4f46\u5176\u4e2d\u4e58\u7684\u64cd\u4f5c\u4f1a\u5728\u591a\u9879\u5f0f\u4e0a\u8fdb\u884c\u4ee5\u8fbe\u5230\u95ed\u73af\u76ee\u7684, \u5143\u7d20\u90fd\u662f\u591a\u9879\u5f0f, \u4ed6\u4eec\u7684degree \u4e5f\u5c31\u662f\u51e0\u6b21\u4f1a\u5c0f\u4e8e\u7b49\u4e8e2, \u4f1a\u6709\u4e00\u4e2aIdeal, \u4f5c\u4e3a\u7c7b\u4f3c\u4e8e\u4e4b\u524d(mod 7)\u7684\u4e00\u4e2a\u6a21\u6570, \u53ea\u4e0d\u8fc7\u8fd9\u91cc\u662f\u4e00\u4e2a\u591a\u9879\u5f0f, \u53ef\u4ee5\u662fp(x)=1+x+x^3, \u5176\u4ed6\u591a\u9879\u5f0f\u76f8\u4e58mod\u8fd9\u4e2ap(x)\u5f97\u5230\u7684\u7ed3\u679c\u4f1a\u5728\u8fd9\u4e9b\u5143\u7d20\u4e4b\u5185, \u5f62\u6210\u4e00\u4e2a\u73af, \u4e14\u662f\u4e00\u4e2a\u5927\u5c0f\u4e3a8\u7684\u57df, characteristic\u4e3a2, \u56e0\u4e3a\u5927\u5bb6\u7684\u7cfb\u6570a\u90fd\u662f\u4ece(0,1)\u4e2d\u53d6\u7684. \u4e3a\u4ec0\u4e48\u6709\u516b\u4e2a: a+bx+cx^2, abc\u4ece0\u30011\u4e2d\u53d6\u4e00\u5171\u6709\u516b\u79cd\u7ec4\u5408                     - Z7 \u5c31\u662f0-6\u7684\u57df, order \u548ccharacteristic \u90fd\u662f7                     - Z7\u5c31\u662f\u4e00\u4e2a\u57df\u7684\u4e58\u6cd5\u53ef\u4ea4\u6362\u7fa4, order\u4e3a6, \u56e0\u4e3a\u9700\u8981\u5143\u7d20\u90fd\u548c7\u4e92\u8d28\u4e140\u4e0d\u53ef                     - \u57df\u7684\u5927\u5c0fsize\u6216\u662forder\u5fc5\u987b\u8981prime power\u7684, \u6bd4\u5982size\u4e3a2^3=8\u7684\u57df\u4e2d\u76848\u4e2a\u5143\u7d20\u5c31\u662f(1, x, x^2)                     - \u57df\u4e2d\u7684\u52a0\u6cd5inverse\u5c31\u662f\u52a0\u51e0\u53ef\u4ee5mod N\u52300, \u4e58\u6cd5inverse\u5219\u4e3a\u4e58\u51e0\u53ef\u4ee5mod N\u4e3a1                     - Facts about finite fields k                       collapsed:: true                         - \u2022 there is some prime p and n \u2265 1 such that the order of K is p^n                           \u2022 two finite fields with the same order are isomorphic as fields                           \u2022 fields of characteristic 2 are exactly those of order 2n for some n \u2265 1                     -                      -                      - \u8fd9\u91cc\u5bf9\u6709\u9650\u57df\u5b9a\u4e49\u4e86\u7fa4\u64cd\u4f5c, \u7b2c\u4e00\u4e2ae\u662f0, \u5199\u9519\u4e86, \u6211\u6ca1\u6539\u6389, \u52a0\u6cd5\u662f\u6210\u7acb\u7684, \u4e0d\u7ba1\u600e\u4e48\u6837\u90fd\u80fd\u627e\u5230inverse                     - \u4f46\u662f\u4e58\u6cd5\u5fc5\u987b\u8981\u5728\u4e0b\u9762\u8fd9\u4e2asubgroup\u4e2d\u5b9e\u73b0, \u56e0\u4e3a\u4e0d\u4e00\u5b9a\u53ef\u4ee5\u4e58\u8d77\u6765mod\u51fa1\u6765                 - ((63e049cf-5808-4f0f-81eb-c8e79f49712b))                   collapsed:: true                     - ((63e049c8-877c-457c-96a9-bc2e113d42a0))                     - \u6295\u5f71\u5e73\u9762\u5c31\u662f\u4e00\u4e2a\u591a\u7ef4\u5750\u6807\u96c6\u5408, \u5750\u6807\u53d6\u503c\u8303\u56f4\u662fK                     - ((63e04a1b-66fc-4821-a1d0-834659d0c867))                     - \u4e09\u7b49\u7b26\u53f7\u7684\u542b\u4e49\u662f\u4e24\u4e2a\u5750\u6807\u53ef\u4ee5\u901a\u8fc7\u4e00\u4e2ascalar \u7f29\u653e\u6210\u4e00\u6837                 - ((63e04a6f-abd3-4238-a4a0-a0fb8a0ed789))                   collapsed:: true                     - \u9f50\u6b21\u692d\u5706\u66f2\u7ebf\u65b9\u7a0b                     - ((63e04aa2-5b26-401f-902b-b171da7dc1e5))                     - E(K)\u662f\u5728K\u57df\u4e2d\u7684\u70b9\u96c6\u5408, mod\u4e86K\u7684\u5927\u5c0f                 - ((63e04b37-7f76-428a-bf10-f426023f6790))                   collapsed:: true                     - \u5c06Z\u770b\u4f5c\u662f1, \u8fdb\u884c\u4e00\u4e2a\u7f29\u653e\u5f97\u5230\u76f8\u901a\u4f46\u7b80\u5355\u7684\u8868\u8fbe                     - ((63e04b75-3feb-42fc-8e85-f2f08b9dbd5a))                     - \u7531\u4e8e\u6ca1\u6709z\u4e86, \u8981\u8868\u8fbe\u65e0\u9650\u8fdcO = (0, 1, 0), \u5c31\u9700\u8981\u7279\u5730\u62ff\u51fa\u6765\u4e00\u4e2a                 - ((63e04bbb-e1b1-409b-b257-649a87c40f6a))                   collapsed:: true                     - ((63e04bde-a129-4926-bc2c-7dbab4120043))                     - 3\u503c\u8868\u8fbe\u66f4\u4e3a\u9ad8\u6548, \u4f46\u662f2\u503c\u8868\u8fbe\u66f4\u5bb9\u6613\u7406\u89e3                 - ((63e04eff-7b6c-40cd-bed1-2b96e4965460))                   collapsed:: true                     - \u5f53finite field\u7684characteristic\u4e3a2 \u6216\u80053\u7684\u65f6\u5019, \u66f2\u7ebf\u7684\u5f62\u5f0f\u5c31\u4e0d\u80fd\u7b80\u5316\u4e3ashort Weierstrass form\u4e86. \u5e76\u4e14\u5373\u4fbf\u5f0f\u5b50\u4e0d\u540c, \u4f46\u662f\u6240\u8868\u8fbe\u7684\u66f2\u7ebf\u4e0a\u7684\u70b9\u5728field\u4e2d\u4f1a\u5b58\u5728\u91cd\u590d, \u4e5f\u5c31\u662f\u4e0d\u540c\u7684coefficient, \u76f8\u540c\u7684\u70b9\u96c6. \u56e0\u6b64\u4e0d\u80fd\u591f\u4f7f\u75282\u6216\u80053                     - ((63e04eeb-9ffb-4fe4-b892-f51738c73a4f))                 - Chord-tangent process, Group law of EC                   collapsed:: true                     -                  - ((63e04fac-dad3-4e78-89f1-a09111fe481a))                   collapsed:: true                     - \u5982\u4f55\u8ba1\u7b97P3, \u6839\u636eP1 P2\u5f97\u5230P1 + P2                     - ((63e04fdb-7ef9-4e3c-b022-e92f86295598))         - Notes         - \u5b8f\u89c2\u7406\u89e3             - \u7b80\u5355\u6765\u8bf4, \u692d\u5706\u66f2\u7ebf\u76f8\u5bf9\u4e8e\u4e4b\u524d\u76f4\u63a5\u4f7f\u7528g\u7684a\u6b21\u65b9mod N \u96be\u4ee5\u77e5\u9053a\u662f\u591a\u5c11\u6765\u8bf4, \u5229\u7528\u4e86\u692d\u5706\u66f2\u7ebf\u51fd\u6570, \u5728\u692d\u5706\u66f2\u7ebf\u4e0a\u9762\u533a\u4e00\u4e2a\u57fa\u70b9, \u901a\u8fc7\u4e0d\u65ad\u505a\u5207\u7ebf\u4ea4\u70b9\u5bf9x\u5bf9\u79f0\u64cd\u4f5c, \u4ee5\u53ca\u548c\u57fa\u70b9\u8fde\u7ebf\u53d6\u4ea4\u70b9\u7684x\u5bf9\u79f0\u64cd\u4f5c\u5f97\u5230\u65b0\u7684k * P \u70b9, \u8fd9\u4e2a\u70b9\u4f1a\u4f5c\u4e3a\u5bc6\u94a5\u4ea4\u6362\u7684\u516c\u5171\u90e8\u5206, \u522b\u4eba\u77e5\u9053\u8fd9\u4e2a\u662f\u5f88\u96be\u5f97\u77e5\u8fd9\u4e2ak\u5230\u5e95\u662f\u51e0, \u4e5f\u5c31\u662f\u5230\u5e95\u505a\u4e86\u51e0\u6b21P + P\u7684\u64cd\u4f5c, \u4f46\u662f\u5bf9\u65b9\u5728\u524d\u9762\u4e58\u4e2aj, \u548c\u81ea\u5df1\u4e58\u4e0akP\u5f97\u5230\u7684\u90fd\u662fjkP, \u8fbe\u6210\u4e86\u76ee\u7684             - E(\u6709Z)\u4e2d\u8ba1\u7b97\u6bd4\u8f83\u9ad8\u6548(\u56e0\u4e3a\u6ca1\u6709inverse), E'\u6ca1\u4e86Z\u8981\u53e6\u5916\u5305\u62ecO, \u4ed6\u4fe9isomorphic. E''\u662f\u7279\u5f81\u975e2,3\u7684\u57df\u4e0a\u7684\u692d\u5706\u66f2\u7ebf, \u53ef\u4ee5\u5199\u4f5c\u6700\u7b80\u5f62\u5f0f             -              - EC\u88ab\u7528\u5728\u4e86\u6570\u5b57\u7b7e\u540d, ECDSA                 - G\u662fEC\u7684\u57fa, K\u662f\u5bf9G\u505a\u4e86k(\u79c1\u94a5)\u6b21\u8fd0\u7b97\u540e\u7684\u516c\u94a5, h\u662f\u4fe1\u606f\u7684hash, u\u662f\u4e00\u6b21\u6027\u5bc6\u94a5\u5bf9G\u505au\u6b21\u8fd0\u7b97\u751f\u6210\u4e86\u4e00\u4e2ar, s\u4f5c\u4e3a\u7b7e\u540d, \u6c47\u603b\u4e86h, kr\u548cu\u7684\u9006. (r, s)\u662f\u7b7e\u540d. \u9a8c\u8bc1\u7b7e\u540d\u65f6\u8ba1\u7b97v\u662f\u5426\u7b49\u4e8er.             - RSADSA RSA\u7684\u6570\u5b57\u7b7e\u540d                 - \u7528\u79c1\u94a5\u5bf9hash\u540e\u7684m\u52a0\u5bc6, \u53d1\u9001(m, s), \u9a8c\u8bc1\u8005\u5bf9\u6bd4\u516c\u94a5\u89e3\u5bc6\u540e\u7684hash\u662f\u5426\u4e0e\u81ea\u5df1hash\u7684m\u76f8\u540c                 - preimage resistance: can compute a preimage m of the decrypted s, but is an existential forgery since the attacker cannot control the content                 - collision resistance: sender can be a potential attacker, can repudiate the m with m', which has the same hash                 - second-preimage resistance: attacker can find another m'     - Week6 ECs Weierstrass form, EC efficiency, secure sharing, Reed-Solomon Codes, Shamir Secret Sharing       collapsed:: true         - 78-82         - Slides           collapsed:: true             - Elliptic Curves               collapsed:: true                 - \u5728\u4e00\u4e2a\u5c0f\u7684\u6709\u9650\u57df\u4e0a\u7684EC\u7684\u5f62\u8c61\u8868\u8fbe in short Weierstrass form                   collapsed:: true                     - ((63e05059-ba36-4521-866e-3d73d5405565))                     - ((63e05060-8886-4038-813a-c7e496835e73))                     - \u53ef\u4ee5\u770b\u51fa\u70b9\u7684\u4e2a\u6570\u662f\u6709\u9650\u7684, \u800c\u67d0\u4e2a\u70b9\u7684\u8fde\u52a0\u6700\u7ec8\u90fd\u4f1a\u8d70\u5230infinity                 - ((63e050b6-54de-4b39-b50f-93e85858fd71))                   collapsed:: true                     - \u7528\u6570\u5b66\u7684\u65b9\u6cd5evaluate \u8fd9\u4e2acurve\u7684\u5b89\u5168\u6027                     - ((63e90c42-f520-4692-88e5-fd47fbcbc922))                     - \u67d0\u4e2afinite field\u7684order\u7684trace of frobenius\u53ef\u4ee5\u88ab\u5b9a\u4e49\u51fa\u6765, \u4e0eorder \u672c\u8eab\u548c\u63cf\u7ed8\u51fa\u7684\u66f2\u7ebf\u70b9\u96c6\u5927\u5c0f\u6709\u5173                     - \u5f53n=1\u7684\u65f6\u5019, p^n\u662fprime, t=1, \u8fd9\u4e2a\u65f6\u5019\u5c31\u4e0d\u5b89\u5168                     - n&gt;1\u7684\u65f6\u5019\u4e5f\u6709\u7c7b\u4f3c\u7684\u60c5\u51b5, \u8981\u5c3d\u91cf\u907f\u514d                     - \u6211\u4eec\u9700\u8981|E(K)| \u80fd\u591f\u88ab\u4e00\u4e2a\u5927\u7684prime\u6574\u9664                       collapsed:: true                         -                 - ((63e050a8-7194-4732-8eb3-6e5a1114ea63))                   collapsed:: true                     - 1. may choose values of curve coefficients, may choose finite field: this                          gives us a very large number of possible groups for Group Law \u692d\u5706\u66f2\u7ebf\u53c2\u6570, \u57df\u7684\u5927\u5c0f, \u6709\u5f88\u591a\u53ef\u4ee5\u9009\u62e9\u7684\u53c2\u6570\u6765\u6784\u5efa\u4e0d\u540c\u7684group law                     - 2. finding E and K with strong cryptographic properties is relatively easy \u5bb9\u6613\u627e\u5230\u9ad8\u5f3a\u5ea6\u7684\u692d\u5706\u66f2\u7ebfgroup law                     - 3. curves E(K) with strong cryptographic properties much more secure                          than, say, a 7 \u2192 ga mod p in terms of the bit-size of p \u6bd4\u7279\u6570\u91cf\u8981\u6c42\u51cf\u5c11\u4e86, \u4f46\u662f\u4fdd\u6301\u4e86\u76f8\u8fd1\u7684\u5b89\u5168\u7a0b\u5ea6                     - 4. thus we may decrease size of p when using Elliptic Curves, without                          losing security when compared to operating in multiplicative group                          ({1, 2, . . . , p \u2212 1}, \u03bbx\u03bby : x \u00b7 y mod p, 1) with suitable generator g \u53ef\u4ee5\u4f7f\u7528\u5c0f\u4e00\u70b9\u7684p\u4e86                     -                  - ((63e90fba-8101-4937-909a-f9412fad2218))                   collapsed:: true                     - \\(K^2\\) \u662faffine form, \u53ea\u6709(X, Y), \u8ba1\u7b97\u4e2d\u4f1a\u6709\u9664\u6cd5\u7684\u5b58\u5728, \u800c\u9664\u6cd5\u5728group\u5185\u505a\u7684\u4e8bmultiplicative inverse, \u5373\u5bfb\u627e\u548c\u4ed6\u4e58\u8d77\u6765mod N\u4e3a1\u7684\u503c, \u8ba1\u7b97\u4e0a\u9762\u5f88\u6602\u8d35                     - \u89e3\u51b3\u65b9\u6848\u662f\u4f7f\u7528projective coordinates (X, Y, Z) instead of (X, Y ); One good method is to transform (0, 1, 0) back to (0, 1), and (X, Y, Z) back to (X/Z^2, Y /Z^3) for Z  != 0. \u53ea\u6709\u5728\u6700\u540e\u4e00\u6b65\u6709\u9664\u6cd5\u4e86, \u4e2d\u95f4\u5c31\u6ca1\u6709\u4e86                 - ((63e9148e-9454-42e6-a067-79591a0ac1bc))                 - ((63e9168e-e6c6-4eef-9b60-29bd95f7d866))                   collapsed:: true                     - ((63e9170f-c137-414b-b639-6f5a7d5d886c))                     - \u53ef\u4ee5\u901a\u8fc7\u5feb\u901f\u5e42\u6765\u8ba1\u7b97mod, \u589e\u52a0\u901f\u5ea6                 - ((63e9176f-d18d-4b13-b7fe-ae8f9acac9b7))                   collapsed:: true                     - ((63e91786-a0ba-45ca-b611-5543084bcd8e))             - Secrete Sharing               collapsed:: true                 - ((63e917d5-5755-4757-a228-7ef3e29c461b))                   collapsed:: true                     - Error-correcting codes used in CD/DVD, BluRay, WiMAX, and in space missions such as the famous interstellar mission Voyager.                     - \u9519\u8bef\u4fee\u6b63\u4ee3\u7801,                     - Mathematical setting:                     - ((63e91909-58ad-4783-89a0-93bf866994a5))                     - \u8fd9\u91cc\u7684P\u662f\u4e00\u4e2a\u591a\u9879\u5f0f\u96c6\u5408, \u96c6\u5408\u4e86\u5404\u79cd\u53ef\u80fd\u7684\u591a\u9879\u5f0f, \u591a\u9879\u5f0f\u7684\u7cfb\u6570f_i\u662f\u4ecefinite field\u4e2d\u53d6\u7684, \u56e0\u4e3a\u6709\u4e00\u5171t+1\u4e2afi, \u56e0\u6b64P\u7684\u5927\u5c0f\u4e5f\u5c31\u662f\\(q^{t+1}\\).                     - X\u5219\u662f\u5404\u4e2aparties, \u4e0d\u540c\u7684sharing \u8005, \u4ed6\u4eec\u5404\u81ea\u62e5\u6709\u81ea\u5df1\u7684\u5bc6\u94a5, \u5373\u5982\u679c\u81ea\u5df1\u662f1, \u90a3\u4e48\u81ea\u5df1\u5c06\u62e5\u6709\u628ax=1\u4ee3\u5165\u8fd9\u4e2a\u5f0f\u5b50\u540e\u7684\u503c\u4f5c\u4e3a\u6211\u7684share \u5bc6\u94a5. \u800c\u8fd9\u4e2a\u591a\u9879\u5f0f\u7684\u7cfb\u6570\u5c31\u662f\u96c6\u9f50\u51e0\u4e2aparties\u624d\u80fd\u591f\u5f97\u5230\u7684\u516c\u5171\u5b9d\u5e93                 - ((63e91c25-7cad-4cf3-a536-20dec8f02d76))                   collapsed:: true                     - Evaluate code word f in P at all points in X = {x1, x2, . . . , xn}:                     - \\(C = {(f (x_1), f (x_2), . . . , f (x_n)) | f \\in P}\\)                     - \u6bcf\u4e2aparty\u4f1a\u62e5\u6709code words\u4e2d\u7684\u4e00\u4e2a                     - C\u867d\u7136\u6709n\u4e2a\u5143\u7d20, \u6bcf\u4e2a\u5143\u7d20\u7684\u957f\u5ea6\u662f\\(log_2 q\\), \u4f46\u662f\u53ea\u9700\u8981t+1\u4e2a\u5143\u7d20\u5c31\u591f\u4e86, \u56e0\u4e3a\u6211\u4eec\u8bbe\u8ba1\u7684\u65f6\u5019, \u5c31\u662ft\u4e2a\u5143\u7d20, \u591a\u4f59\u7684\u5c31\u662fredundency for error correction t &lt; n                 - ((63e931e4-b775-46ee-8746-d9f7ecadb9e5))                   collapsed:: true                     - q = 101                       t = 2                       n = 7                       X = {1, 2, 3, 4, 5, 6, 7}                     - q\u662f\u4e00\u4e2aprime number, \u7528\u6765mod\u7684\u4e00\u4e2afield order\u5927\u5c0f, t\u662f\u591a\u9879\u5f0f\u7684order, \u51b3\u5b9a\u4e86\u6700\u5c11\u9700\u8981t+1\u4e2aparties\u6765\u89e3, n\u662f\u6240\u6709\u7684\u591a\u9879\u5f0fx\u4ee3\u5165\u7ed3\u679c\u5197\u4f59, x\u662fq\u7684\u5b50\u96c6, \u5305\u62ec\u4e86\u4ee3\u8868parties\u7684\u51e0\u4e2ax\u503c, \u7528\u6765\u4ee3\u5165\u591a\u9879\u5f0f\u83b7\u5f97f(x)                     - ((63e93448-4b47-4d0b-8043-eb15eae36845))                 - ((63e9345c-a186-46bb-b069-544ed2d08e4b))                   collapsed:: true                     - if t &lt; n and no transmission error occurred in communication of c, we can fully recover f from c.                     - ((63e93491-7d56-48e0-9a5a-86fafb16f0df))                     - \u6839\u636eFundamental Theorem of Algebra, \u89e3\u552f\u4e00, t+1\u4e2a\u70b9\u65f6                 - ((63e934c1-7214-493d-8fc2-3113c1c9c042)) Lagrange interpoltion                   collapsed:: true                     - ((63e93627-3035-4110-b59b-7b533f3ef1a6))                 - ((63e93631-f0e2-4604-bdf4-4389c1a3fcb4))                   collapsed:: true                     - (t + 1)-out-of-n secret sharing scheme                     - Security property: t or less than t parties, when colluding, should not                       learn anything about the secret, which is an element s in field F_q.                     - ((63e936ed-8db1-4061-b729-c6ecd1752eff))                     - \u73b0\u5728\u7684s\u662f\u51fd\u6570\u7684\u5e38\u6570\u9879, \u6b63\u5e38\u89e3\u7684\u8bdd, \u5c31\u662f\u5f97\u5230\u53c2\u6570\u4ee5\u540e\u6c42f(0)                     - \u5982\u679c\u7528Lagrange\u7684\u8bdd, \u5c31\u53ef\u4ee5\u76f4\u63a5\u7b97\u51fa\u6765                     - ((63e9379c-1c26-4e02-a0af-699454595569))                     - \u8fd9\u91cc\u7684recombinant vector\u53ea\u4e0eX\u6709\u5173, \u53ef\u4ee5\u91cd\u590d\u4f7f\u7528                     - Example                       collapsed:: true                         - ((63e938e5-8529-44bb-8e5c-d8addbc9ce26))                         - \u6bcf\u4e2aparty\u6839\u636e\u4ed6\u7684id, \u4f1a\u5f97\u5230f(id)\u7684\u503c, \u5373\u4e3a\u4e00\u4e2ashare, \u56e0\u4e3a\u6211\u4eec\u7684\u51fd\u65702\u6b21\u65b9, \u6240\u4ee5\u67093\u4e2ashares\u5c31\u53ef\u4ee5\u4e86                         - ((63e93955-07f6-41f4-bea2-bd69d4ec48ec))                     -                     -                 - ((63e93972-4a16-47b4-b075-46cbd9961bdf))                   collapsed:: true                     - \u53ea\u8981\u5c55\u793ashow this for the case when | Y | = t\u7684\u60c5\u51b5\u6ca1\u529e\u6cd5\u8fd8\u539f\u5c31\u591f\u4e86                     - \u7531\u4e8e\u53ea\u6709t\u4e2a, \u6240\u4ee5\u53ea\u80fd\u627e\u51fa\u6765\u4e00\u4e2ag, order\u4e3at-1, \u6781\u7aef\u60c5\u51b5\u4e0b\u80fd\u5f97\u5230\u4e00\u4e2ag(0)\u521a\u597d\u7b49\u4e8es, \u4f46\u662f\u5373\u4fbf\u5f97\u5230\u4e86, \u4ed6\u4eec\u4e5f\u4e0d\u77e5\u9053\u8fd9\u4e2a\u5c31\u662fs                     - ((63e93a49-0fde-4d7d-a4b8-0e4cd4c736a7))                     - \u53ef\u80fd\u6027\u592a\u591a\u4e86, \u6839\u672c\u65e0\u6cd5\u786e\u5b9a\u662f\u4e0d\u662f\u771f\u7684s         - Notes             -              - ECDSA \u692d\u5706\u66f2\u7ebf\u6570\u5b57\u7b7e\u540d\u7b97\u6cd5             - \u5229\u7528\u7684\u662f\u90e8\u5206\u597d\u7528\u7684\u692d\u5706\u66f2\u7ebf, \u5e76\u975e\u6240\u6709\u66f2\u7ebf\u90fd\u662f\u597d\u4f7f\u7684             - \u7531\u4e8e\u692d\u5706\u66f2\u7ebf\u7684\u6027\u8d28, \u76f8\u540c\u5b89\u5168\u6027\u9700\u8981\u7684key\u7684size\u8981\u5c0f\u4e8eRSA             - \u7531\u4e8eephemeral\u7684\u5b58\u5728, \u662fnon-deterministic\u7684             - private key\u662f\u4e00\u4e2along term key, \u4f7f\u7528\u4e00\u6b21\u6027\u7684ephemeral keys\u6765\u8ba9\u7b7e\u540d\u7ed3\u679c\u4e0d\u5177\u786e\u5b9a\u6027, \u5373\u6bcf\u6b21\u90fd\u4f1a\u4e0d\u4e00\u6837             -              - ((63ea32eb-8b78-43f7-b40a-6eff5b6d8cf8))             - \u4e0a\u9762\u7684Figure\u5c31\u7ed9\u5230\u4e86\u7531\\(p(x) = 1 + x+x^3\\)\u7ed9\u5230\u7684quotient ring, \u662f\u4e00\u4e2afinite field, size\u662f2^3 = 8, \u5373\u67098\u4e2a\u5143\u7d20, \u4e14\u6bcf\u4e2a\u5300\u901f\u90fd\u6709\u80fd\u53d8\u62101\u7684multiplicative inverse             - quotient ring \u662f\u9664\u4e86p(x)\u4ee5\u53ca\u4ed6\u7684\u9ad8\u9636\u5ef6\u4f38\u7684\u6240\u6709coefficient \u7ec4\u5408\u7684\u591a\u9879\u5f0f\u7684\u96c6\u5408, \u5982\u8868\u4e2d\u6240\u793a, \u56e0\u6b64\u7b2c\u4e00\u4e2a\u9009\u9879\u662f\u6b63\u786e\u7684             - \u7b2c\u4e8c\u4e2a\u9009\u9879\u53ef\u4ee5\u89c1\u8868\u4e2d\u7684x(x+1)\u5f97\u5230\u4e86x^2+x\u4e5f\u662ffield\u5185\u7684\u5143\u7d20, \u56e0\u6b64\u662f\u9519\u8bef\u7684             - \u7b2c\u4e09\u4e2a\u9009\u9879, The ring of polynomials over the rational numbers is a field \u5e94\u8be5\u662f\u5bf9\u7684               collapsed:: true                 - Given the polynomial ring F_K[x] for a field K, rational functions are quotients p(x)/q(x) - as formal expressions - where p(x) and q(x) are in F_K[x] and q(x) is not 0. It turns out that this set of rational functions is then a field where 0, 1,  and * is what you might expect.                 - In particular, this is true for the field K = Q of rational numbers. The ring F_K[x] is not a field only because non-unit elements p(x) that are not 0 do not have a multiplicative inverse. This is a problem that rational functions solve; and note that the ring F_K[x] embeds into this field where p(x) is mapped to p(x)/1.                 - The ring F_K[x] is infinite when K is infinite, as K embeds into F_K[x] where field elements are identified with their constant polynomial. For K = Q, the field of rational functions is infinite for the same reason.             - \u7b2c\u56db\u4e2a\u9009\u9879, AES\u4e2d\u4e5f\u8fd0\u7528\u5230\u4e86finite fields\u548c\u591a\u9879\u5f0f             - \u7b2c\u4e94\u4e2a\u9009\u9879, fields\u7684\u5927\u5c0f\u5f97\u662fprime power             -              - Shamir\u2018s secret sharing \u662f\u5b89\u5168\u7684, \u4fe1\u606f\u5b66\u4e0a\u7684\u5b89\u5168\u7684             - Recombinant vector\u662f\u7531party\u96c6 X\u51b3\u5b9a\u7684             - \u53ef\u4ee5\u7528\u6765\u7ba1\u7406key, \u4f8b\u5982\u591a\u5c11\u4e2a\u4eba\u6ee1\u8db3\u624d\u80fd\u83b7\u53d6\u5bc6\u94a5             - \u52a0\u6cd5\u662f\u540c\u6784\u7684, f+g (n) = f(n) + g(n), \u4f46\u662f\u4e58\u6cd5\u662f\u4e0d\u6ee1\u8db3\u7684         - \u5b8f\u89c2\u7406\u89e3             - Secret share \u4e3b\u8981\u7528\u7684\u662fShamir scheme, \u591a\u9879\u5f0f\u6b21\u6570+1\u4e2agroup\u7684\u503c\u53ef\u4ee5\u89e3\u51fa\u8fd9\u4e2a\u591a\u9879\u5f0f\u7684\u7cfb\u6570, \u901a\u5e380\u6b21\u7684\u5e38\u6570\u9879\u4f5c\u4e3asecret;             -              - \u901a\u8fc7Lagrange interpolation \u6765\u7b80\u5316\u8ba1\u7b97, \u7528recombinant vector\u6765\u8ba1\u7b97\u5f97\u51fa0\u6b21\u9879\u7684\u503c             -              - \u4f8b\u59823\u4e2aparties, \u5c31\u9700\u8981\u8ba1\u7b97\u4e09\u904d\u8fd9\u4e2a\u503c, \u7136\u540e\u5404\u81ea\u7684\u503c\u4e58\u4e0a\u5404\u81ea\u7684secret\u4ee5\u540e\u7684\u548c\u5c31\u662fs0\u4e86. \u8fd9\u4e2a\u7b97\u51fa\u6765\u7684\u5176\u5b9e\u662fg(0), \u4f46\u7531\u4e8e\u6211\u4eec\u7684party\u6570\u6bd4\u6b21\u6570\u9ad8, \u6240\u4ee5g(0)=f(0); \u5982\u679c\u4e2a\u6570\u4e0d\u591f, determine\u7684g\u7684degree\u5c31\u4e0d\u591f, 0\u70b9\u7684\u53ef\u80fd\u6027\u5c31\u5728\u57df\u5185\u90fd\u6709\u53ef\u80fd.             - If parties 1 and 3 pool together their shares, they can determine a polynomial g(x) of degree 1 but the polynomial f(x) has degree 2 and so for each value z in Z31 there is a polynomial h(x) of degree 2 with h(0) = z, and where h(1) = g(1) and h(3) = g(3). Therefore, parties 1 and 3 cannot learn any information about the secret s from combining their shares.             - Reed-Solomon codes generalize this to the case in which some of the shares may be erased s (i.e., no longer available) or may be corrupted e. e&lt;t&lt; (n\u2212s)/ 3. n\u662ftotal number of shares, t\u662f\u591a\u9879\u5f0f\u7684degree. \u5f53error\u6570\u5927\u4e8e\u6216\u7b49\u4e8edegree\u7684\u65f6\u5019\u5c31recover\u4e0d\u4e86\u4e86.     - Week7 Complex numbers, Quantum secret exchange       collapsed:: true         - 83-91         - Slides           collapsed:: true             - ((63f25aa5-c3ca-4b2d-839e-ddf50d2ce7c9))               collapsed:: true                 -                  - a is real part, b\u00b7i is the imaginary part, where \\(i^2 = -1\\)             - ((63f25b02-ae81-4061-be87-12e3940b933b))               collapsed:: true                 - ((63f25b12-deba-4172-a08b-260f10065848))                 - \u5b9e\u6570\u90e8\u5206\u548c\u865a\u6570\u90e8\u5206\u5206\u522b\u76f8\u52a0\u5373\u53ef             - ((63f25b36-0755-4bac-b07c-43be217dbaf5))               collapsed:: true                 - \u5206\u914d\u5f8b Distributivity Law \u4f9d\u7136\u6210\u7acb                 - ((63f25b69-fc4c-4191-b27a-c89864e6efa5))             - ((63f25b80-ba64-458f-8860-ef5130cabeb4))               collapsed:: true                 - (C, +, 0) is a commutative group that + and \u00b7 satisfies the distributivity laws of a field.                 - 1 is the unit for \u00b7 in C, \u4efb\u4f55\u975e0\u7684c\u90fd\u6709multiplicative inverse                 -                  - ((63f25bdd-1477-493b-8b61-3279415dbe78))                   collapsed:: true                     - The conjugate of c = a + bi negates the imaginary part c^bar = a - bi                       id:: 63f25bdf-2006-4b29-b611-f54794677b34                     - ((63f25c7e-975d-4a1b-a9ae-21d9b5d97752))                     - Norm\u662f\u6839\u53f7\u4e0b\u7684\u7cfb\u6570\u5e73\u65b9\u548c, inverse\u53ef\u4ee5\u7528 conjugate \u7b80\u5199             - ((63f25cc3-4e8b-4531-bb78-a38e5ba9e02b))               collapsed:: true                 - \u865a\u6570\u4e5f\u53ef\u4ee5\u6784\u6210vector, \u4f8b\u5982\u4e8c\u7ef4\u7684\u865a\u6570vetor                 -                  - ((63f25cf7-1839-417f-8eca-6ea22fff42f9))                 - ((63f25d09-6b84-4b32-a02f-442866c6e692))                   collapsed:: true                     - Vector\u76f8\u52a0, \u5c31\u662f\u5bf9\u5e94\u7ef4\u5ea6\u9879\u7684\u76f8\u52a0                     -                      - scalar vector\u76f8\u4e58, \u5c31\u662f\u6bcf\u4e2a\u7ef4\u5ea6\u90fd\u4e58\u4e0a\u8fd9\u4e2ascalar                     -                  - ((63f25ddf-8c2d-47e1-ba9a-6ba87d61fa50))                   collapsed:: true                     - vector space operations are component-wise:                     -                  - ((63f25e16-6d52-479f-9721-3e856893699f))                   collapsed:: true                     - 2x2 complex matrices \u53ef\u4ee5\u4f5c\u4e3atransformation matrix \u53d8\u6362quantum bits                     -                  - ((63f25e57-15f7-44d7-a73f-38887795990b))                   collapsed:: true                     - \u4e24\u4e2a2x2\u7684\u77e9\u9635\u76f8\u4e58, \u548c\u666e\u901a\u77e9\u9635\u4e58\u6cd5\u662f\u4e00\u6837\u7684                     - ((63f25e7f-c7c4-4c62-924b-edc58fe5acc2))                       collapsed:: true                         -             - ((63f25ebf-166e-43b4-a116-31fe7e029d03)) (two bases)               collapsed:: true                 - A \u201cqubit\u201d q is an element of C^2 with norm 1                 - qubit \u9700\u8981norm\u4e3a1, \u957f\u5ea6\u4e3a1, \u4e00\u4e2aqubit\u7531\u4e24\u4e2acomplex number\u7ec4\u6210, \u6bcf\u4e2acomplex number\u6709\u4e24\u4e2a\u7cfb\u6570, \u4e00\u5171\u56db\u4e2a\u7cfb\u6570, \u4ed6\u4eec\u7684\u5404\u81ea\u7684\u5e73\u65b9\u7684\u548c\u8981\u7b49\u4e8e1                 - Since qubits are elements of C2 with norm 1, they correspond to the points on Bloch\u2019s sphere, a 2-dimensional sphere. Therefore, there are uncountable many qubits! This is in striking contrast to the fact that there are only 2 classical bits: 0 and 1. qubits\u7684\u6570\u91cf\u548c\u53ef\u80fd\u6027\u662f\u65e0\u7a77\u7684                 -                  - qubits\u53ef\u4ee5\u88ab\u5199\u6210ket notation form, \u4e24\u4e2a\u5e38\u7528\u7684qubits\u662f [[ket right]] and [[ket up]]                   collapsed:: true                     - ((63f25f54-2eb8-43ef-a6b8-ed907a670aa5))                     - \u7531\u8fd9\u4e24\u4e2aqubits\u7ec4\u6210\u7684set \u2018+\u2019, \u5373plus basis, \u662fC^\u2006\u7684\u4e00\u4e2abasis                     - ((63f25fa1-2ebb-4576-8725-d255a84453ed))                 - Another basis we will use for quantum-key exchange uses \u201cket 45 degrees\u201d and \u201cket \u221245 degrees\u201d and is \u00d7 = {| \u2197\u3009, | \u2196\u3009} where                   collapsed:: true                     - ((63f25fed-a49e-415d-85e9-05ab98a1f668))                 - States of 2-dimensional quantum systems are elements of C2. \u5373\u4e8c\u7ef4\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684states \u662fC2\u7684\u5143\u7d20\u4eec\u90fd\u662fstates                 - In quantum physics, we cannot \u201cread\u201d those exact states, but we can measure them in a given basis. The measurement collapses the measured state to a basis vector.                 - \u5728\u91cf\u5b50\u529b\u5b66\u7cfb\u7edf\u4e2d, \u6211\u4eec\u6ca1\u6709\u529e\u6cd5\u5bf9\u8fd9\u4e9bstate\u8fdb\u884cread\u64cd\u4f5c\u800c\u4e0d\u7834\u574f\u4ed6\u4eec\u7684\u539f\u672c\u72b6\u6001, \u6211\u4eec\u53ea\u53ef\u4ee5measure, \u6d4b\u91cf\u4ed6\u4eec\u7684\u72b6\u6001, \u4f1a\u622a\u83b7\u5e76\u5f97\u5230\u4e00\u4e2a\u7ed3\u679c, \u8fd9\u4e2a\u7ed3\u679c\u4f9d\u636e\u6211\u4eecmeasure\u4f7f\u7528\u7684basis\u4f1a\u6709\u4e0d\u540c, \u91c7\u7528\u53d1\u9001\u8005\u76f8\u540c\u7684basis\u4f1a\u5f97\u5230\u76f8\u540c\u7684state, \u4f46\u662f\u5982\u679c\u4f7f\u7528\u7684\u662f45\u5ea6\u5dee\u7684\u5219\u4f1a\u670950%\u7684\u6982\u7387\u5f97\u5230\u5f53\u524d\u4f7f\u7528\u7684basis\u4e2d\u7684\u4e24\u4e2aqubits\u4e2d\u7684\u4efb\u610f\u4e00\u4e2a. \u8fd9\u79cd\u6d4b\u91cf\u540e\u624d\u5f97\u5230\u7ed3\u679c\u5e76\u4e14\u7ed3\u679c\u4f9d\u7167basis\u800c\u5b9a\u7684\u60c5\u51b5\u5c31\u53eb\u505ameasurement collapse             - ((63f263bf-cde5-48aa-ba2d-18f99382f770))               collapsed:: true                 - measurement collapse                   collapsed:: true                     - ((63f2920d-4712-495e-a6df-2bdf7531c6fb))                 - qubit\u7684\u7cfb\u6570\u4f1a\u5bf9measurement\u6700\u540e\u7684basis\u7ed3\u679c\u6709\u6982\u7387\u5f71\u54cd                   collapsed:: true                     - ((63f265af-fecc-4a89-aa67-24a23270e0d1))             - ((63f29291-eb16-465d-9ac6-e9cfd6b518c5))               collapsed:: true                 - Creating a source of uncertainty: We have two bases of the complex vector space of qubits, + and \u00d7 and a qubit does not reveal in which basis it was prepared.                   collapsed:: true                     - \u4e24\u4e2abasis\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u7684source, \u4e0d\u77e5\u9053\u7528\u7684\u662f\u54ea\u4e2abase, \u7b2c\u4e00\u91cduncertainty                 - No information gain: Measuring any vector from one of those bases in the other basis has an equal probability of observing any of the vectors of that other basis.                   collapsed:: true                     - For example, measuring | \u2196\u3009 in the + basis gives us no information                       gain as to which vector | \u2192\u3009 or | \u2191\u3009 of + we might observe.                     - \u4e0d\u6b63\u786e\u7684basis\u4f1a\u5bfc\u81f4\u7ed3\u679c\u4e5f\u4e0d\u786e\u5b9a, 50 50, \u6ca1\u6709info gain                 - Probabilistic certainty: Measuring a basis vector in the same basis is guaranteed to observe that same basis vector with probability 1. For example, measuring | \u2196\u3009 in basis \u00d7 observes | \u2196\u3009 with probability 1.                   collapsed:: true                     - \u4f46\u662f\u540c\u6837\u7684basis\u4f1a\u7ed9\u5230\u4e00\u6837\u7684\u7ed3\u679c             - ((63f29542-729f-49d3-8203-598a96e714d6))               collapsed:: true                 - Players: Alice and Bob share a quantum channel and an authenticated public channel. Eve wants to intercept or manipulate the key in transit.                 - Aim: Alice and Bob want to securely exchange a bitstring of length m so that any tampering or analysis of Eve will be detected.                 - ((63f29ab8-5335-4b53-95b2-d1678cdb4359))                 - ((63f29ac9-6180-40ec-bf10-ae54690d921d))                 - Steps:                   collapsed:: true                     - collapsed:: true                       1. Alice generates n = 8 \u00b7 m bits of randomness x1y1x2y2 . . . x4my4m.                         - For sake of illustration, let us say that m = 2 and that Alice generates                           1011100101001010 and so x = 11100011 and y = 01011000.                         - \u6839\u636e\u4e0a\u9762\u7684encode scheme, \u6765\u751f\u6210qubits, xy\u751f\u6210\u4e00\u4e2a, \u6240\u4ee5\u67094m\u4e2a                     - 2.From this randomness of 8 \u00b7 m bits x1y1x2y2 . . . x4my4m, Alice prepares 4m                       collapsed:: true                       qubits | x1\u3009, | x2\u3009, . . . , | x\u30094m.                         -                      - collapsed:: true                       3. Alice sends the qubits | x1\u3009, | x2\u3009, . . . , | x4m\u3009 in that order to Bob over the                          quantum channel. \u4ece\u91cf\u5b50\u901a\u9053, \u53d1\u9001\u7ed9bob\u8fd9\u4e9bqubits                         - | \u2191\u3009, | \u2196\u3009, | \u2191\u3009, | \u2197\u3009, | \u2197\u3009, | \u2192\u3009, | \u2191\u3009, | \u2191\u3009                     - 4.Bob will generate a classical bitstring z = z1z2 . . . z4m of bitlength 4m.                       collapsed:: true                       Bob will then measure each of the 4m qubits | x\u2032_j \u3009he receives as follows:                         -                          -                          - \u9047\u5230basis\u5339\u914d\u9519\u8bef\u7684\u60c5\u51b5, bob\u5c31\u4f1a\u5f97\u5230\u4e00\u4e2a\u5728measure basis\u4e0a\u7684\u968f\u673aqubit                     - collapsed:: true                       5. Alice and Bob share their bitstrings y = y1y3 . . . y4m and z = z1z2 . . . z4m                          on the authenticated, public, and classical channel.                         - Step 5.1: Alice will delete in x1x2 . . . x4m all xj for which yj != zj .                         - Step 5.2: Bob will delete in x\u20321x\u20322 . . . x\u20324m all x\u2032j for which yj != zj .                         - ((63f29ea9-825b-4884-bba4-cbfc434d2657))                         - \u8fd9\u4e2a\u60c5\u51b5\u4e0b, \u9664\u975e\u6709\u901a\u8baf\u9519\u8bef, \u6216\u662f\u6076\u610f\u64cd\u5f04, \u8fd9\u91cc\u76845 bits \u5bf9\u4e8e\u53cc\u65b9\u90fd\u662f\u4e00\u6837\u7684                     - collapsed:: true                       6. ((63f29f2e-d134-46c2-8f5a-2429ed7a860a))                         - \u8981\u9a8c\u8bc1\u4e24\u4e2a\u4eba\u5f97\u5230\u76845 bits\u662f\u4e0d\u662f\u4e00\u6837\u7684, \u9996\u5148B\u751f\u6210\u4e00\u4e2ak/2\u5927\u5c0f\u7684\u6d4b\u8bd5\u96c6index\u96c6 I, I\u4f1a\u88ab\u516c\u5f00, \u4e5f\u5c31\u662f\u7b2c\u4e94\u6b65\u91cc\u6700\u540e\u53bb\u6389\u4e0d\u4e00\u81f4\u7684bits\u540e\u7684\u957f\u5ea6\u7684\u4e00\u534a\u5927\u5c0f\u7684\u6d4b\u8bd5\u96c6T, K_B\u5c31\u662fx\u2018\u4e2d\u53bb\u6389T\u2019\u4e2d\u6709\u7684\u5143\u7d20\u4ee5\u540e\u7684bits, \u516c\u5f00T                         - Alice\u7528\u516c\u5f00\u7684\u4fe1\u606fI, \u521b\u5efaT\u6d4b\u8bd5\u96c6, \u4f7f\u7528\u76f8\u540c\u65b9\u6cd5\u5f97\u5230K_A, \u516c\u5f00T                         - \u4ed6\u4eec\u4e24\u4e2a\u4eba\u518d\u90fd\u770b\u4e00\u4e0b\u4e24\u8005\u7684T\u6709\u591a\u5c11\u4e0d\u4e00\u81f4\u7684bits                         - \u5c11\u7684\u8bdd\u5c31\u6ca1\u95ee\u9898, \u591a\u7684\u8bdd, \u5c31\u6709\u95ee\u9898\u4e86, \u8fd9\u91cc\u7684key\u53ea\u662flikely\u4e00\u81f4, \u5982\u679c\u4f7f\u7528\u8fc7\u7a0b\u4e2d\u53d1\u73b0\u4e0d\u4e00\u81f4, \u8fd8\u9700\u8981\u91cd\u65b0\u8fdb\u884c\u8fd9\u4e2a\u8fc7\u7a0b                 - ((63f2a45f-9374-4dbc-ba2c-7c0e314e0f2d))                   collapsed:: true                     - \u4f20\u7edf\u7684\u901a\u9053\u4e2d, Eve\u53ef\u4ee5\u4fdd\u5b58\u6570\u636e, \u4e2d\u95f4\u7be1\u6539, \u80fd\u591f\u8bfb\u4f46\u662f\u4e0d\u4fee\u6539\u6570\u636e, \u4f46\u662f\u91cf\u5b50\u901a\u9053\u4e2d, \u56e0\u4e3a\u6570\u636e\u4e0d\u53ef\u8bfb, \u53ea\u80fd\u6d4b, \u800c\u4e00\u65e6\u6d4b\u91cf\u5c31\u4f1a\u6709\u574d\u584c, \u65e0\u6cd5\u5f97\u77e5\u6e90\u6570\u636e\u771f\u6b63\u7684\u6837\u5b50, \u56e0\u6b64Eve\u53ea\u80fd\u62e6\u622a, \u526a\u5207, \u800c\u65e0\u6cd5\u62f7\u8d1d                     - However, in quantum computations:                       \u2022 Cut, paste, and analyze: Eva can transfer a qubit (a \u201ccut and paste\u201d in computer science) but this transfer must collapse or destroy the original bit (\u201ccopy and paste\u201d is impossible). This follows from the no-cloning theorem of quantum physics.                       \u2022 Reading quantum states: This can only be done through measurements, which will change the quantum state to one of the basis vectors with respect to which the measurement is made. Therefore, it is physically impossible to read a quantum state without modifying it.                 - ((63f2a5dc-4cbc-4599-a95d-1d03cc032999))                   collapsed:: true                     - ((63f2a5e5-0ea8-4658-8a76-8244d708f684))                     - ((63f2a5fb-94b6-4d2f-b703-a63dfc5f0d9b))                       collapsed:: true                         -         - Notes             -                  - 1. NO, \u77ed\u6682\u5373\u901d\u7684, \u6ca1\u6709\u957f\u671f\u7684storage, quantum states \u4f1a\u88ab\u522b\u7684\u7c92\u5b50\u5f71\u54cd                   2. NO, \u5f97\u8981\u75282 complex numbers, 4 real numbers \u53ef\u4ee5\u7528\u6765\u8868\u793a\u4e24\u4e2acomplex numbers, \u56db\u4e2a\u6570\u5b57squared norm\u7b49\u4e8e1                   3. YES, \u53ef\u4ee5\u6709\u591a\u79cdqubits\u7684\u8868\u73b0\u5f62\u5f0f, \u6bd4\u5982energy state, low \u662f0, high\u662f1, \u4e4b\u7c7b\u7684; quantum tech\u7684\u4e00\u4e2a\u95ee\u9898\u5c31\u662f\u627e\u5230\u4e00\u79cd\u5408\u9002\u7684\u8868\u73b0\u5f62\u5f0f\u6765\u8868\u8fbe This system may be realised in different ways: as a single photon whose state is either in vertical or horizontal polarization, as an electron that has either spin up or spin down, as an atom that is either in its ground energy state or at an excited energy state, and so forth.                   4. YES, \u53ef\u4ee5\u901a\u8fc7\u7f16\u7801\u8fdb\u884c\u8868\u793a\u7ecf\u5178\u6bd4\u7279                   5. NO, \u4e0d\u80fd\u88abread, \u53ea\u80fd\u88abmeasure, measure \u4ee5\u540e\u4f1a\u6539\u53d8\u6210one of vectors in the eigenspace,             -                  - 1. protocol\u6700\u540e\u4f1a\u68c0\u6d4b\u76f8\u540c\u7684\u4e2a\u6570, \u9519\u8bef\u5c11\u7684\u8bdd\u8ba4\u4e3a\u6ca1\u5565\u95ee\u9898                   2. \u662f\u7684, \u9700\u8981authenticated \u516c\u5171\u901a\u9053                   3. \u6ca1\u6709\u88ab\u5e7f\u6cdb\u4f7f\u7528                   4. \u53ef\u4ee5\u7528\u4e0d\u540c\u7684basis                   5. \u5bf9\u7684, \u4e0drandom\u7684\u8bdd\u5c31\u51fa\u95ee\u9898\u4e86             - \u5173\u4e8eEve\u4f5c\u4e3a\u4e2d\u95f4\u4ebaattack\u7684\u95ee\u9898, \u8c8c\u4f3c\u91cf\u5b50\u901a\u9053\u5185, Eve\u80fd\u505a\u7684\u53ea\u6709\u62e6\u622a, \u6ca1\u529e\u6cd5\u505a\u5230\u91cd\u65b0\u53d1\u9001, \u6240\u4ee5\u6ca1\u529e\u6cd5\u5047\u88c5\u662f\u53cc\u65b9         - \u5b8f\u89c2\u7406\u89e3             - \u5173\u4e8equbits               collapsed:: true                 - qubit \u6ca1\u6709longterm storage, \u53ea\u80fd\u88ab\u89c2\u6d4b, \u65e0\u6cd5\u88ab\u50a8\u5b58, \u9605\u8bfb\u800c\u4e0d\u6539\u53d8\u5176state                 - qubit\u7684\u8868\u793a\u7528\u4e86\u4e24\u4e2acomplex number, \u4f5c\u4e3a\u4e00\u4e2a\u4e8c\u7ef4\u590d\u6570\u7a7a\u95f4\u7684\u5355\u4f4d\u5411\u91cf, \u5176\u6a21\u5fc5\u987b\u4e3a1, \u4e5f\u5c31\u662f\u5176\u590d\u6570\u8868\u793a\u7684\u56db\u4e2a\u7cfb\u6570\u7684\u5e73\u65b9\u548c\u5fc5\u987b\u4e3a1, \u5f53\u7136\u524d\u9762\u53ef\u4ee5\u6709\u4e2ascalar. \u56e0\u6b64\u6240\u6709qubits\u4f1a\u5f62\u6210\u4e00\u4e2aBloch\u2018s sphere, \u4e00\u4e2a\u4e8c\u7ef4\u7684\u7403, \u7531\u65e0\u6570\u4e2aqubits\u7ec4\u6210, \u8fd9\u4e0e\u4f20\u7edf\u76842\u4f4d\u6709\u5f88\u5927\u533a\u522b                 - \u62e5\u6709\u591a\u79cdphysical manifestations, \u4f8b\u5982polaristion, spin up down, energy state                 - \u53ef\u4ee5\u4ee5\u67d0\u79cd\u4eba\u4e3a\u7684\u5b9a\u4e49\u8868\u793a\u7ecf\u5178\u76840\u548c1                   collapsed:: true                     - \u4e24\u4e2alinearly independent qubits\u53ef\u4ee5\u7ec4\u6210\u4e00\u4e2abasis, \u6211\u4eec\u7528\u5230\u4e86\u4e24\u7ec4basis, \u5206\u522b\u662f+ = {\u53f3, \u4e0a}\u548cx = {\u53f3\u4e0a, \u53f3\u4e0b}, \u7528\u51e0\u4f55\u5e73\u9762\u7684\u7c7b\u4f3c\u7684\u5411\u91cf\u8868\u793a             - \u5173\u4e8emeasurements               collapsed:: true                 - measure\u4e00\u4e2aqubit\u9700\u8981\u5b9a\u4e49\u4e00\u4e2abasis\u6765\u6d4b\u91cf, \u4efb\u4f55\u91cf\u5b50\u4f1acollapse to\u8fd9\u4e2abasis\u7684\u4e24\u4e2aqubits, \u91cf\u5b50\u574d\u584c\u7684\u6982\u7387\u662f\u5b9e\u6253\u5b9e\u7684\u968f\u673a. \u5177\u4f53\u574d\u584c\u5230\u54ea\u4e00\u4e2aqubit, \u7531\u88ab\u6d4b\u91cf\u7684\u8fd9\u4e2aqubit\u5199\u4f5c\u4ee5\u8fd9\u4e2abasis\u4e3a\u5e95\u4ee5\u540e\u7684\u4e24\u4e2a\u5355\u4f4d\u5411\u91cf\u7684\u6a21\u65b9\u51b3\u5b9a|\u03c8\u27e9 = \u03b1 \u00b7 |\u2192\u27e9 + \u03b2 \u00b7 |\u2191\u27e9, \u5219\u5176\u6982\u7387\u4e3aa^2 \u548c b^2             - BB84 protocol               collapsed:: true                 - \u7528quantum channel\u5efa\u7acb\u4e00\u4e2ashared bitstring K, \u540c\u65f6\u53ef\u4ee5\u53d1\u73b0\u901a\u8baf\u9519\u8bef\u548c\u6076\u610f\u4fee\u6539                 - \u9700\u8981\u4e00\u4e2aauthenticated public channel                 - 0 = \u53f3+, \u53f3\u4e0ax; 1 = \u4e0a+, \u53f3\u4e0bx                 - m\u957f\u5ea6\u7684key, \u9700\u89818m\u957f\u5ea6\u7684\u6bd4\u7279\u4e32\u6765\u8fd0\u884c. \u5206\u4e3a4m\u957f\u7684x\u8868\u793abit, \u548c4m\u957f\u7684y\u8868\u793abasis, \u7ec4\u5408\u5f97\u52304m\u4e2aqubits                 - Bob\u4e5f\u751f\u62104m\u957f\u7684z\u8868\u793a\u4ed6\u7684basis\u6765\u6d4b\u91cfAlice\u53d1\u8fc7\u6765\u76844m\u4e2aqubits, \u4e0d\u4e00\u6837\u7684basis\u4f1a\u5bfc\u81f4\u7ed3\u679c\u968f\u673a, \u5f97\u52304m\u957f\u7684bitstring x                 - A\u548cB\u5728\u516c\u5171\u9891\u9053share\u4e86z\u548cy, \u6bd4\u8f83\u4e86\u4e24\u8005\u4e0d\u540c, \u628a\u4e0d\u540c\u7684\u4f4d\u7f6e\u7684 x\u5220\u6389, \u4e24\u4e2a\u4eba\u7684bitstring\u5c31\u4e00\u6837\u4e86, \u7531\u4e8e\u6d4b\u4e0d\u51c6\u6982\u7387\u4e3a1/2, \u5220\u6389\u540e\u7684\u671f\u671b\u957f\u5ea6\u5c31\u662f2m                 - \u7531\u4e8e\u4ed6\u4fe9\u4e0d\u77e5\u9053\u4ed6\u4eec\u7684x\u662f\u5426\u4e00\u6837\u4e86, \u6240\u4ee5\u8fd8\u6709\u4e00\u6b65. B\u968f\u673a\u4ecex\u91cc\u9762\u9009\u51fa1/2size\u4e2a\u6570\u7684bits, \u5c06(i, xi)\u4eec\u53d1\u9001\u5230\u516c\u5171\u9891\u9053, \u9a8c\u8bc1, \u5982\u679c\u4e24\u4e2a\u4eba\u7684\u8fd9\u4e2a\u4e00\u6837\u6216\u8005\u5dee\u522b\u5f88\u5c0f, \u5c31\u9a8c\u8bc1\u901a\u8fc7                 - \u9a8c\u8bc1\u8fc7\u7a0b\u53c8\u6709\u4e00\u534a\u7684\u4e22\u4e86, \u6240\u4ee5\u6700\u540e\u5f97\u5230\u7684share key size \u4e3a8m/2/2/2 = m                 - \u6ce8: \u6781\u5c0f\u6982\u7387\u8fd8\u662f\u4e0d\u540c\u4f46\u4f1a\u88ab\u53d1\u73b0, \u91cd\u65b0run; bit\u4f4d\u6570\u53ef\u80fd\u4e0d\u591f, hash\u4e00\u4e0b             - Eve               collapsed:: true                 - \u65e0\u6cd5\u514b\u9686, \u65e0\u6cd5\u4e0d\u6d4b\u91cf\u7684\u60c5\u51b5\u4e0b\u9605\u8bfb                 - \u91cf\u5b50\u901a\u9053\u662f\u5355\u5411\u901a\u9053, \u6bd4\u5982\u4e00\u6839\u5149\u7ea4, \u53ea\u80fd\u4e2d\u95f4\u89c2\u6d4b\u4e00\u4e0b, \u4efb\u7531qubit\u8fdc\u53bb                 -         - Questions             - What is an authenticated public channel? Why is it authenticated?             - Is it still secure If Eve acts as a man-in-the-middle who pretends to be Bob and measures all the qubits from Alice and sends her own message to Bob? In other words, Alice may consider Eve as Bob, and Bob may consider Eve as Alice.     - Week8 Commitments and Oblivious Transfer P121       collapsed:: true         - 65-67         - Slides             - \u4e3a\u4ec0\u4e48\u9700\u8981commitments?               collapsed:: true                 - Security protocols are wise to assume that their participants may not be                   honest.             - Aim               collapsed:: true                 - Design protocols in which parties can only cheat by also exposing                   that they cheated. \u8bbe\u8ba1\u4e00\u79cdprotocol, \u5982\u679c\u67d0\u4e00\u65b9cheat\u4e86, \u662f\u80fd\u591f\u53d1\u73b0\u5b83cheat\u4e86\u7684             - ((63fa7a53-6afb-4834-a45f-62d728239093))               collapsed:: true                 - \u8fdc\u7a0b\u8fdb\u884c\u77f3\u5934\u526a\u5b50\u5e03\u6e38\u620f, \u5148\u51fa\u7684\u4e00\u65b9\u4eba\u4f1a\u5403\u4e8f, \u56e0\u4e3a\u540e\u9762\u90a3\u4e2a\u4eba\u53ef\u4ee5\u9009\u62e9\u51fb\u8d25\u7684\u9009\u9879, \u4f46\u662f\u5982\u679c\u5148\u51fa\u7684\u4e00\u65b9\u5148\u7ed9\u51fa\u4e00\u4e2acommitment\u6765\u8868\u793a\u4ed6\u51fa\u4e86\u67d0\u4e00\u4e2a, \u4f46\u662f\u53e6\u4e00\u65b9\u5e76\u4e0d\u80fd\u77e5\u9053\u8fd9\u4e2a\u662f\u4ec0\u4e48(concealing), \u53e6\u4e00\u65b9\u51fa\u5b8c\u4e86\u4ee5\u540e, \u7b2c\u4e00\u65b9\u518d\u628a\u771f\u6b63\u7684\u9009\u9879\u7ed9\u51fa\u6765, \u7b2c\u4e8c\u65b9\u53ef\u4ee5\u8fdb\u884c\u7b97\u6cd5\u8fd0\u7b97\u5f97\u5230commitment, \u5982\u679c\u548c\u4e00\u5f00\u59cb\u4e00\u6837(binding), \u5373\u9a8c\u8bc1\u6210\u529f                 - Root of the problem                   collapsed:: true                     - asynchronous nature of play. \u5f02\u6b65\u9020\u6210\u4e86\u8fd9\u4e2a\u95ee\u9898, \u5982\u679c\u5927\u5bb6\u540c\u65f6, \u5c31\u4e0d\u4f1a\u6709\u8fd9\u4e2a\u95ee\u9898\u4e86                 - ((63fa7aeb-abfa-43e6-a83f-e97d1a56a266))                   collapsed:: true                     - hA defined as H(RA || paper)                     - H\u662fhash fucntion                     - Ra\u662f\u968f\u673a\u6570, \u7528\u6765\u963b\u6b62B\u4ecehA\u53cd\u5411\u63a8\u51fa\u539f\u503c\u4ec0\u4e48 (concealing)                     - hash function\u7684\u96be\u4ee5\u627e\u5230second preimage\u7279\u6027\u7528\u6765bind\u539f\u503c\u548ccommitment (binding)             - ((63fa7b9a-ead1-408f-bd50-e812e8df92cb))               collapsed:: true                 - Concealing Property of Commitment Scheme: (Hiding Property) protect sender                   collapsed:: true                     - Bob won\u2019t know which value A has committed to, since Bob does                       not know RA after step 1. B \u65e0\u6cd5\u63d0\u524d\u77e5\u9053commitment\u80cc\u540e\u7684\u771f\u5b9e\u503c                     - \u63a5\u53d7\u8005 \u6ca1\u6cd5\u63d0\u524d\u77e5\u9053                 - Binding Property of Commitment Scheme: protect receiver                   collapsed:: true                     - A \u65e0\u6cd5\u9003\u907f\u5176\u6240commit\u7684\u503c, \u6ca1\u529e\u6cd5\u627e\u5230\u65b0\u7684\u503c, \u5f97\u5230\u4e00\u6837\u7684commiment. \u4f8b\u5982\u5728\u4e0a\u9762\u7684\u77f3\u5934\u526a\u5200\u5e03\u7684\u6e38\u620f\u4e2d, A\u9700\u8981\u627e\u5230\u4e00\u4e2arandom R'A \u4f7f\u5f97hash\u5b8c\u7684\u7ed3\u679c\u4e00\u6837.                     - \u53d1\u9001\u8005 \u6ca1\u6cd5\u62b5\u8d56             - ((63fa7c24-be54-48d8-8d67-08b67dac89e8))               collapsed:: true                 - information-theoretic security: attacker with infinite computing power                   cannot break the scheme                 - computational security: attacker with specified computation resources                   (e.g. probabilistic polynomial time) cannot break the scheme             - ((63fa7dad-e182-40dc-89f3-d5acb506ab78))               collapsed:: true                 - A commitment scheme is a public algorithm C which takes a value x and                   collapsed:: true                   some randomness r as input to produce a commitment                     - \\(c = C(x,r)\\)                 - ((63fa8010-d349-4cf9-9d6a-bcfc6d2ca9d1)) \u80fd\u4e0d\u80fd\u627e\u5230\u53e6\u4e00\u5bf9x r\u5f97\u5230\u76f8\u540c\u7684commitment\u503c                   collapsed:: true                     - ((63fa8031-57b4-4ff5-983c-9b3b882d842e))                 - ((63fa805b-eeaa-4497-b9f3-293292c951d5)) \u80fd\u4e0d\u80fd\u901a\u8fc7commitment \u5f97\u5230\u539f\u6765\u7684\u503c                   collapsed:: true                     - ((63fa80f3-38e4-4604-b888-387a8eb834a6))                     - attacker \u5c31\u662fadversary, \u53d1\u9001\u4e86\u4e24\u4e2a\u6d88\u606f, \u6536\u5230\u4e86commitment, \u8981\u731c\u5230\u662f\u54ea\u4e2a                 - Lemma 20.3: There is no commitment scheme that is both information-theoretically binding and information-theoretically concealing.                   collapsed:: true                     - \u65e0\u6cd5\u505a\u5230\u5b8c\u7f8e\u7684\u53c8\u4fdd\u5bc6, \u53c8\u7ed1\u5b9a                     - \u5982\u679c\u7edd\u5bf9\u7684\u7ed1\u5b9a\u7684\u8bdd, \u6bcf\u4e2acommitment\u503c\u53ea\u5bf9\u5e94\u4e00\u7ec4r\u548cx, \u65e0\u9650\u8ba1\u7b97\u80fd\u529b\u5c31\u53ef\u4ee5\u627e\u5230\u8fd9\u7ec4                     - \u5982\u679c\u7edd\u5bf9\u7684\u4fdd\u5bc6\u7684\u8bdd, \u6bcf\u4e2acommitment\u5e94\u8be5\u6709\u65e0\u9650\u79cd\u53ef\u80fd, \u8fd9\u4e2a\u65f6\u5019\u4e5f\u5c31\u4e0d\u7ed1\u5b9a\u4e86                 - Lemma 20.4: For a hash function H, the commitment scheme is \u201cat best\u201d computationally binding and computational concealing.                   collapsed:: true                     - \u7528hash function\u7684\u8bdd, \u56e0\u4e3a\u503c\u57df\u662f\u6709\u9650\u7684, \u6240\u4ee5binding\u53ea\u80fd\u662f\u5f88\u96be\u627e\u5230\u7b2c\u4e8c\u4e2a\u539f\u50cf.                     - concealing \u627e\u7684\u662f\u67d0\u4e00\u4e2a\u5b58\u5728\u7684\u539f\u50cf, \u4e5f\u662f\u53ea\u6709\u8ba1\u7b97\u4e0a\u7684\u5b89\u5168, \u4f46\u662f\u5982\u679c\u6709\u5f88\u591acollision \u5b58\u5728\u7684\u8bdd, \u90a3\u5c31\u662finformation- theoretically secure\u4e86, \u8981\u6c42\u662f\u8fd9\u4e2aR\u4ece\u4e00\u4e2a\u5f88\u5927\u5f88\u5927\u7684\u96c6\u5408\u4e2d\u53d6, \u5c31\u4f1a\u6709\u5f88\u591acollision, \u65e0\u6cd5\u786e\u5b9a\u54ea\u4e2a\u662f\u771f\u7684                 - ((63fa840a-b0c5-4f75-820c-331441de87a9))                   collapsed:: true                     - ((63fa8414-e62c-4d02-bd6b-ced5a2cf8869))                     - G\u662f\u4e00\u4e2a\u53ef\u4ea4\u6362\u7fa4, \u662f\u7531prime q\u4f5c\u4e3asize\u7528\u4e00\u4e9b\u4e1c\u4e1cgenerate\u7684, \u6bd4\u5982\u8bf4\u9009\u5b9a\u4e00\u4e2a\u539f\u6839g, g\u7684123456\u6b21\u65b9 \u6a21q \u53ef\u4ee5\u83b7\u5f971 - q-1\u4e4b\u95f4\u7684\u6570\u5b57, \u4e5f\u53ef\u4ee5\u7528\u692d\u5706\u66f2\u7ebf\u6765\u505a\u8fd9\u4ef6\u4e8b\u60c5, \u5dee\u4e0d\u591a\u5c31\u662f\u6362\u4e86\u4e2a\u8fd0\u7b97\u65b9\u6cd5\u800c\u5df2                     - g\u7684x\u6b21\u65b9\u5f97\u5230\u7684\u503c\u57fa\u672c\u4e0a\u5c31\u662fq\u91cc\u9762\u968f\u673a\u53d6\u4e00\u4e2a\u6570\u7684\u90a3\u79cd\u968f\u673a, \u628a\u8fd9\u4e2a\u4f5c\u4e3ah\u516c\u5f00, \u522b\u4eba\u4e5f\u5f88\u96be\u77e5\u9053\u5230\u5e95\u662f\u54ea\u4e2ax                 - ((63fa8945-2d12-48e5-a528-7858e0cb64cc))                   collapsed:: true                     - \u5982\u4f55\u627e\u5230\u4e00\u4e2asafe prime: Let p and q be prime with p = 2 \u00b7 q + 1. Then p is called a \u201csafe prime\u201d.                     - ((63fa89b5-e936-48fb-85f9-627b2e9cac25))                     - Fp\u662f\u4e00\u4e2aorder\u4e3ap-1\u7684\u4e58\u6cd5\u4ea4\u6362\u7fa4, \u56e0\u4e3a\u53bb\u6389\u4e860                     - \u5982\u4f55\u627e\u5230Generator g                       collapsed:: true                         - ((63fa8a04-3d25-4b6a-85bc-4f1c60601971))                           collapsed:: true                             - \u7531random r\u627e\u4e00\u4e2a\u4e0d\u7b49\u4e8e1\u7684g, g\u548cr\u662f\u7ed1\u5b9a\u7684; \u8fd9\u91cc\u7684g\u7531\u5b89\u5168\u8d28\u6570p\u63a8\u5f97, \u5c06\u53ef\u4ee5\u7528\u6765\\(g^x\\ mod\\ p\\) \u6765\u751f\u6210\u4e00\u4e2a\u5927\u5c0f\u4e3aq\u7684subgroup G, x\u4e3a0~q-1\u7684\u60c5\u51b5\u4e0b\u5f97\u5230\u7684\u503c\u5e94\u8be5\u90fd\u4e0d\u4e00\u6837, \u4f1a\u904d\u5386G\u4e2d\u7684\u6240\u6709\u5143\u7d20.                             - \u4e3e\u4e2a\u4f8b\u5b50, sefe prime p=5, q=2, g\u7531\u4e0a\u9762\u7684\u7b97\u6cd5\u53ef\u4ee5\u627e\u52304                             - 4^0 mod 5=1, 4^1 mod 5=4, 4^2 mod 5=1, ...                             - \u8fd9\u4e2aG\u7684\u5927\u5c0f\u5c31\u662f2=q, \u4e5f\u5c31\u662forder\u4e3aq, \u5305\u542b1\u548c4\u4e24\u4e2a\u5143\u7d20                             - \u4e5f\u5c31\u662f\u540e\u9762\u7684\u627f\u8bfa\u7b97\u6cd5\u4e2d, \u4efb\u4f55\u7528g\u751f\u6210\u7684\u6570\u5b57mod p\u4ee5\u540e\u5f97\u5230\u7684\u7ed3\u679c\u90fd\u4f1a\u662f\u5728G\u4e4b\u4e2d\u7684, \u4e14q\u6709\u591a\u5927, \u7528\u6765\u751f\u6210\u8fd9\u4e2a\u7ed3\u679c\u7684power\u5c31\u6709\u591a\u96be\u627e                               collapsed:: true                                 -                     - \u5982\u4f55\u627e\u5230\u4e00\u4e2a\u516c\u5f00\u5bc6\u94a5 h                       collapsed:: true                         - ((63fa8b1a-9dbe-4eba-81ee-64832d1fcc3e))                         - h\u9664\u4e86\u4e0d\u80fd\u7b49\u4e8e1\u4ee5\u5916, \u8fd8\u4e0d\u80fd\u7b49\u4e8eg                         - h\u7531\u4e8e\u79cd\u79cd\u539f\u56e0, \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u751f\u6210\u7684\u8bdd, \u4e00\u5b9a\u53ef\u4ee5\u88abg\u751f\u6210                         - \u5728\u627f\u8bfa\u7cfb\u7edf\u4e2d, h\u4f5c\u4e3a\u516c\u5171\u6570\u636e, \u662f\u9700\u8981\u88ab\u968f\u673a\u751f\u6210\u7684, \u4e0d\u80fd\u6709\u4eba\u77e5\u9053h\u5230\u5e95\u662fg\u7684\u51e0\u6b21\u65b9                         - \u5e76\u4e14h\u975e\u5e38\u968f\u673a, \u53ef\u4ee5\u662fp\u4ee5\u5185\u7684\u4efb\u4f55\u6570, \u65e0\u6cd5\u77e5\u9053\u8fd9\u4e2az\u5230\u5e95\u662f\u54ea\u4e2a                         - \u53c2\u8003\u4e0b\u9762\u7684\u516c\u5f0f, \u5982\u679c\u6709\u4eba\u80fd\u4eceh\u5f97\u77e5g\u7684\u51e0\u6b21\u65b9(z)\u7b49\u4e8eh, \u4ed6\u5c31\u53ef\u4ee5\u7ed9c1, \u4e5f\u5c31\u662f\\(g^a\\)\u8fdb\u884c\u4e00\u4e2ato power of z, \u6c42\u5f97\u8fd9\u4e2a\u503c\u7684\u4e58\u6cd5\u9006, \u7528\u8fd9\u4e2a\u4e58\u6cd5\u9006\u4e58\u4e0ac2, x\u5c31\u88ab\u8fd8\u539f\u4e86                     - ((63fa8b99-ac21-4e85-a3a8-2c47a0169155))                       collapsed:: true                         - \\(C(x, a) = E_a(x) = (g^a, x \u00b7 h^a)\\)                         - \u627f\u8bfa\u7684\u503c\\((x, a)\\)                         - \u57fa\u4e8eElGamal\u7684\u627f\u8bfa\u7cfb\u7edf, \u4e0a\u9762\u7684\u5f0f\u5b50\u4e2d, x\u662fmessage, \u662f\u88ab\u627f\u8bfa\u7684\u4fe1\u606f, a\u662f\u4e00\u4e2arandom number, \u7528EG\u6765\u52a0\u5bc6\u7684\u8bdd\u5c31\u662f\u4f1a\u751f\u6210\u4e00\u7ec4\u5bc6\u94a5\u5bf9,                         - c1 (\\(g^x\\)) \u662f\u6211\u4f5c\u4e3a\u53d1\u9001\u8005\u53d1\u9001\u7684\u4e00\u6b21\u6027\u516c\u5171\u5bc6\u94a5, \u9664\u4e86\u6211\u4ee5\u5916\u6ca1\u4eba\u77e5\u9053a\u662f\u591a\u5c11, \u4e00\u6b21\u6027\u7684\u6765\u4fdd\u8bc1\u6bcf\u6b21\u52a0\u5bc6\u7684\u7ed3\u679c\u90fd\u4e0d\u540c                         - c2 (\\(x \u00b7 h^a\\)) \u662f\u6211\u4f5c\u4e3a\u53d1\u9001\u8005, \u5c06\u4fe1\u606fembedding\u8fdb\u53bb\u7684\u90e8\u5206, \u7528\u6765\u5c06\u8fd9\u90e8\u5206\u52a0\u5bc6\u6df7\u4e71\u7684\u5bc6\u94a5\u662f\u63a5\u53d7\u8005\u63d0\u4f9b\u7684\u516c\u5171\u5bc6\u94a5\u7684a\u6b21\u65b9, \u5982\u679c\u5bf9\u65b9\u7684\u79c1\u6709\u5bc6\u94a5\u662fb\u7684\u8bdd, h\u53ef\u4ee5\u8868\u793a\u4e3a\\(h = g^b\\), \u4e5f\u5c31\u662f\u8bf4\\(h^a = g^{ab}\\)                         - \u63a5\u53d7\u8005\u53ea\u8981\u628a\u81ea\u5df1\u7684\u79c1\u94a5\u548cc1\u7ed3\u5408\u53d6modulo \u9006\u4e58\u4e0a\u4e0a\u9762\u7684c2\u5c31\u80fd\u628a\u8ff7\u60d1\u9879\u6d88\u9664, \u4ece\u800c\u5f97\u5230x                         -                          - \u4ee5\u4e0a\u662fElGamal\u4f5c\u4e3a\u52a0\u5bc6\u6216\u8005\u7b7e\u540d\u7684\u65f6\u5019\u7684\u7528\u6cd5, \u5728\u627f\u8bfa\u7684\u65f6\u5019, h, \u8fde\u540cg, p, q\u90fd\u4f1a\u662f\u516c\u5f00\u7684\u6570\u5b57, \u552f\u4e00\u5927\u5bb6\u4e0d\u77e5\u9053\u7684\u662f\u968f\u673a\u751f\u6210\u7684a, \u548c\u627f\u8bfa\u7684\u6d88\u606fx, \u5f53\u5b83\u4eec\u4e24\u4e2a\u653e\u51fa\u6765\u7684\u65f6\u5019, \u5c31\u53ef\u4ee5\u9a8c\u8bc1\u627f\u8bfa\u503c\u5bf9\u4e0d\u5bf9\u4e86                         - ((63fa91bc-94b3-4bcf-b2c4-d2a95b8c0205))                         - \u8fd9\u91cc\u8981\u6ce8\u610f\u7684\u662fx\u548ca\u7684\u53d6\u503c, \u4ed6\u4eec\u7684\u53d6\u503c, \u5176\u5b9e\u662f\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u5b9a\u7684. \u5728\u5b89\u5168\u8003\u8651\u4e0b\u4e00\u5b9a\u662f\u4f1a\u629b\u53bb\u4e00\u4e9b\u4e0d\u5b89\u5168\u7684\u5c0f\u6570\u5b57.x = 0\u662f\u4e0d\u5141\u8bb8\u7684\u3002\u8fd9\u5c06\u4ea7\u751fg^x = g^0 = 1\uff0c\u5176\u4e2d1\u662f\u7ec4\u7684identity\u5143\u7d20\uff0c\u8fd9\u5b8c\u5168\u7834\u574f\u4e86\u5b89\u5168\u6027\u3002\u8bf7\u6ce8\u610f\uff0cx = 1\u4e5f\u662f\u4e0d\u5b89\u5168\u7684\uff0c\u56e0\u4e3a\u8fd9\u5c06\u4ea7\u751fg^1 = g\uff0c\u8fd9\u662f\u516c\u5f00\u7684\u3002a\u548cq\u7684\u53d6\u503c\u90fd\u5fc5\u987b\u5728q\u4ee5\u5185, \u4e0d\u7136\u5c31\u91cd\u590d\u4e86, \u4e0d\u518d\u552f\u4e00\u4e86                     - ((63fa91ff-4755-4d11-840e-cec439c72d34))                       collapsed:: true                         - Lemma 20.6: The commitment scheme Ea(x) is information-theoretically                           binding and computationally concealing.                         - \u4f7f\u7528ElGamal\u7684\u627f\u8bfa\u7cfb\u7edf\u56e0\u4e3a\u7528\u7684\u662f\u52a0\u5bc6\u64cd\u4f5c, \u80fd\u591f\u4fdd\u8bc1\u6570\u636e\u4e00\u5b9a\u80fd\u591f\u8fd8\u539f, \u56e0\u6b64\u662f\u5fc5\u5b9abinding\u7684, \u5fc5\u987b\u662f\u4e00\u5bf9\u4e00\u7684, \u5f97\u5230commitment\u662f\u4e00\u5b9a\u80fd\u591f\u8ba1\u7b97\u51faa\u548cx\u7684(\u53ea\u8981\u6709\u65e0\u9650\u7684\u7b97\u529b), \u8fd9\u4e5f\u5bfc\u81f4\u4e86\u8fd9\u4e2a\u7cfb\u7edf\u53ea\u80fd\u662fcomputationally concealing.                         - Proof: Ea(x) \u201cis\u201d ElGamal encryption with respect to public key h,                           and where Ea(x) does not need associated private key (some z with gz = h                           mod p).                         - Computational concealing follows from semantic security of ElGamal en-                           cryption, which depends on G satisfying the decisional Diffie-Hellman as-                           sumption (we won\u2019t cover this topic of \u201csemantic security\u201d here).                         - Since decryption is correct for ElGamal, and so a ciphertext has a unique                           matching plaintext, the commitment scheme Ea(x) is information-theoretically                           binding.                     - ((63fa92f9-7887-4f94-b5c6-d5769b7c5225)) Pedersen Commitment Scheme                       collapsed:: true                         - \\(C(x, a) = B_a(x) = h^x \u00b7 g^a\\)                         - This is the Pedersen Commitment Scheme:                           \u2022 again, a is random \u2013 a blinding number                           \u2022 x is committed value                           \u2022 all exponentiations are modulo p                           Commitment is revealed as (a, x), the same as for scheme Ea(x).                         - \u8fd9\u4e2acommitment\u7cfb\u7edf\u4e2d, a\u53ef\u4ee5\u8499\u853d\u4f4fx, \u5373\u4fbf\u62e5\u6709\u65e0\u9650\u7b97\u529b\u4e5f\u65e0\u6cd5\u5f97\u5230\u786e\u5207\u7684x, \u4e0d\u540c\u4e8eElGamal\u7684\u4fdd\u8bc1binding\u7684\u7edd\u5bf9\u5b89\u5168, \u8fd9\u4e2a\u7cfb\u7edf\u4fdd\u8bc1\u7684\u662fconcealing\u7684\u7edd\u5bf9\u5b89\u5168, \u76f8\u5bf9\u7684, binding\u5c31\u662fcomputationally secure\u7684 (\u5982\u679c\u4e0d\u77e5\u9053g\u7684\u51e0\u6b21\u65b9\u662fh, \u5c31\u662f\u8ba1\u7b97\u4e0abinding\u7684, \u5f88\u96be\u88ab\u5426\u8ba4, \u56e0\u6b64h\u662f\u8981\u88ab\u968f\u673a\u751f\u6210\u7684, \u4e0d\u80fd\u901a\u8fc7g\u7684\u51e0\u6b21\u65b9\u4eba\u5de5\u9009\u5b9a)                         - Information-theoretically concealing:                           collapsed:: true                             - ((63fa94ae-17b4-46e8-a186-abae677539f0))                             - \u56e0\u4e3a\u662f\\(h^x \u00b7 g^a\\) \u4f1a\u6709\u5f88\u591a\u5f88\u591ax\u548ca\u7684\u7ec4\u5408\u5f97\u5230\u76f8\u540c\u7684\u7ed3\u679c, \u5b8c\u5168\u968f\u673a\u7684\u7ed3\u679c\u5c31\u662f\u975e\u5e38\u4fdd\u5bc6, concealing                         - Computationally binding:                           collapsed:: true                             - ((63fa95cc-827f-42ca-a4c1-c6a3ac570d1b))                             - \u6839\u636e\u63a8\u5bfc, \u95ee\u9898\u53ef\u4ee5\u88ab\u7b80\u5316\u4e3a\u627e\u8fd9\u4e2az, \u53ea\u8981\u627e\u5230z\u5c31\u53ef\u4ee5\u627eb\u548cy, \u6765\u8ba9\u7b97\u5f0f\u7684\u503c\u8fd8\u662fz\u4e86, \u4f46\u662fz\u7684\u627e\u662f\u5f88\u96be\u7684\u4e00\u4ef6\u4e8b\u60c5, \u6240\u4ee5\u8bf4\u662f\u8ba1\u7b97\u4e0a\u82e6\u96be                         - ((63fa9643-cd52-4a2f-9f5f-e9ecd9984616))                           collapsed:: true                             - ((63fa9661-3f50-4cba-b7eb-1a7500c2950d))                             - \u8fd9\u4e2a\u53ea\u662f\u5229\u7528\u4e86\u5f88\u96be\u5728\u5927\u6570\u7684\u60c5\u51b5\u4e0b\u627e\u5230log\u503c\u800c\u5df2, g\u4f1a\u751f\u6210\u4e00\u904d\u6240\u6709\u7684\u57df\u4e2d\u7684\u503c, \u6240\u4ee5\u8981\u4e00\u4e2a\u4e2a\u7b97\u8fc7\u53bb, \u4f46\u662f\u662f\u552f\u4e00\u7684, \u6240\u4ee5\u4f9d\u7136binding; \u53ea\u4e0d\u8fc7\u4fdd\u5bc6\u6027\u6ca1\u90a3\u4e48\u5f3a                             - \u4e3a\u4ec0\u4e48\u540c\u6837\u90fd\u662f\u6c42discrete logarithm, \u5c31\u4e0d\u9009\u7528\u8fd9\u4e2a\u65b9\u6848\u5462? \u56e0\u4e3aElGamal \u5f15\u5165\u4e86\u968f\u673a\u7684\u53d8\u91cfa, \u80fd\u8ba9\u8fd9\u4e2a\u7b97\u6cd5\u4e0d\u90a3\u4e48deterministic         - Notes         - \u5168\u5c40\u7406\u89e3             - Commitment\u7528\u6765\u53d1\u9001\u65b9\u627f\u8bfa\u5176\u53d1\u9001\u7684\u4fe1\u606f\u4e0d\u4f1a\u53d8\u66f4, \u540c\u65f6\u4fdd\u8bc1\u63a5\u53d7\u65b9\u65e0\u6cd5\u63d0\u524d\u77e5\u6653\u53d1\u9001\u7684\u4fe1\u606f, \u76f4\u5230\u660e\u6587\u53d1\u51fa\u624d\u80fd\u591f\u9a8c\u8bc1             - \u4e24\u4e2a\u6982\u5ff5, \u4e24\u79cd\u7a0b\u5ea6               collapsed:: true                 - concealing: \u4fdd\u62a4\u53d1\u9001\u8005\u4fe1\u606f\u4e0d\u88ab\u63d0\u524d\u83b7\u53d6, \u7531\u7b97\u6cd5\u7a7a\u95f4\u7684\u968f\u673a\u6027\u51b3\u5b9a, \u6216\u662f\u57fa\u4e8e\u7684problem\u7684\u56f0\u96be\u7a0b\u5ea6\u51b3\u5b9a                 - binding: \u4fdd\u62a4\u63a5\u6536\u8005\u5f97\u5230\u7684\u4fe1\u606f\u4e0e\u627f\u8bfa\u7684\u4e00\u81f4: \u7531\u7b97\u6cd5\u7a7a\u95f4\u7684\u968f\u673a\u6027\u51b3\u5b9a                 - information-theoretic: \u5f53x\u7684\u53ef\u80fd\u6027\u65e0\u7a77, \u6216\u662f\u65e0\u6cd5\u533a\u5206\u65f6, \u5c31concealing\u4e86; \u5f53x\u4e0e\u627f\u8bfa\u5b8c\u5168\u53ea\u6709\u4e00\u4e2a\u5bf9\u5e94, \u5c31binding\u4e86                 - computational: \u5b8c\u5168binding\u7684\u65f6\u5019\u7a77\u5c3d\u53ef\u4ee5\u627e\u5230\u90a3\u4e2ax, \u5b8c\u5168concealing\u7684\u65f6\u5019, \u53ef\u4ee5\u627e\u5230\u66ff\u4ee3\u7684x; \u56e0\u6b64\u53ea\u57fa\u4e8e\u4e00\u5b9a\u7684\u7b97\u529b\u4e0b.                 - \u4e24\u8005\u6c38\u8fdc\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u5b8c\u7f8esecure             - Hash-based commitment schemes                 - \\(C(x, r) = H(r || x)\\)                 - \u76f8\u5bf9 binding, \u7edd\u5bf9 concealing.                 - \u65e0\u7a77\u7b97\u529b\u7684\u60c5\u51b5\u4e0b, hash\u7279\u6027\u51b3\u5b9a\u53ef\u4ee5\u627e\u5230\u53e6\u5916\u4e00\u7ec4r, x\u5f97\u5230\u4e00\u6837\u7684commitment. \u4e5f\u56e0\u6b64\u63a5\u6536\u8005\u5373\u4fbf\u627e\u5230\u4e00\u7ec4\u4e5f\u65e0\u6cd5\u786e\u5b9a\u662f\u4e0d\u662f\u53d1\u9001\u8005\u63d0\u4f9b\u7684\u90a3\u4e2a, concealing\u4e86             - ElGamal based commitment schemes                 - \\(C(x, a) = E_a(x) = (g^a, x \u00b7 h^a)\\)                 - \u76f8\u5bf9concealing, \u7edd\u5bf9binding                 - \u63d0\u4e00\u53e5, ElGamal\u4f5c\u4e3a\u516c\u94a5\u52a0\u5bc6\u7b97\u6cd5\u65f6, h=g^b, \u8fd9\u91cc\u7684\u6307\u6570b\u662f\u79c1\u94a5, g^-ab\u53ef\u4ee5\u7528\u6765\u6062\u590dx                 - \u627f\u8bfa\u7cfb\u7edf\u4e2d, (a,x)\u4f5c\u4e3a\u4fe1\u606f, \u56e0\u4e3ah\u7684\u90a3\u4e2ab\u96be\u4ee5\u5f97\u77e5(h\u662f\u81ea\u52a8\u751f\u6210\u7684), \u56e0\u6b64\u65e0\u6cd5\u5f97\u5230x, \u76f8\u5bf9conceiling                 - x\u548ca\u7684\u8303\u56f4\u90fd\u9501\u5b9a\u5728\u4e86q\u4ee5\u540e, \u56e0\u6b64\u53ea\u6709\u4e00\u4e2ax\u548ca\u53ef\u4ee5\u5f97\u5230\u8fd9\u4e2a\u7ed3\u679c, \u7edd\u5bf9binding                 - \u80cc\u666f\u77e5\u8bc6, g\u662f2q+1\u5927\u5c0f\u7684\u7fa4\u7684\u4e00\u4e2a\u5b50\u5faa\u73af\u7fa4\u7684generator, g\u5199\u4f5cf^2, \u4ece\u800cg^q\u4e00\u5b9a\u53ef\u4ee5\u6a21p\u4e3a1 (\u56e0\u4e3af^2q = f^p-1 \u4e00\u5b9a\u7b49\u4e8e1), \u56e0\u6b64\u5f97\u5230\u7684\u8fd9\u4e2ag\u4e00\u5b9a\u662forder \u4e3aq\u7684\u5faa\u73af\u7fa4, \u56e0\u4e3aorder\u4e3aprime, \u5176\u4e2d\u6240\u6709\u5143\u7d20\u90fd\u662fg, \u6240\u4ee5g\u548ch\u7684\u9009\u62e9\u53ef\u4ee5\u505a\u5230\u968f\u673a             - Pedersen Commitment Scheme with dual properties \u4e0e\u4e0a\u8fb9\u8fd9\u4e2a\u76f8\u53cd\u7684properties                 - \\(C(x, a) = B_a(x) = h^x \u00b7 g^a\\)                 - \u76f8\u5bf9binding, \u7edd\u5bf9concealing                 - concealing: \\(h^x \u00b7 g^a = g^{zx+a} = c\\), for any x', some a' can be found; \\(g^{a'} = c\u00b7(h^{x'})^{-1}\\)                 - binding: suppose a pair(b, y), \\(z=\\frac{a-b}{y-x}\\), solving z is assumed to be very hard, so computationally binding                 - \u5b9e\u7528\u7684\u5c5e\u6027: \\(B_{a1} (x_1) \u00b7 B_{a2} (x_2) = B_{a_1+a_2} (x_1 + x_2)\\)             - Deterministic commitment scheme                 - \\(C(x, r) = B(x) = g^x\\)                 - \u76f8\u5bf9concealing, \u7edd\u5bf9binding                 - concealing \u4f9d\u8d56\u4e8eHardness of discrete logarithm problem in G                 - binding \u6e90\u4e8e\u6bcf\u4e2ag^x\u90fd\u4e0d\u540c                 - \u6ca1\u6709\u7528r\u6765blinding, \u4f1a\u5bfc\u81f4\u53d1\u9001\u8fc7\u7684\u4fe1\u606f\u91cd\u65b0\u53d1\u9001\u5c31\u5931\u53bb\u4e86concealing             - Commitment based on quadratic residues                 - \\(f(b,x)=m^b\u00b7x^2(mod\\ N)\\)                 - \u76f8\u5bf9binding, \u7edd\u5bf9concealing                 - \u53ef\u4ee5\u628a\\(m^b\u00b7x^2\\)\u770b\u4f5c\\(y^2\\)\u8fd9\u4e2a\u8981\u70b9\u5728\u4e8e\u5f88\u96be\u7b97\u51fay\u662f\u4ec0\u4e48, \u5373\u4fbf\u80fd\u7b97\u51fa\u6765, \u56e0\u4e3a\u6709b\u7684\u5b58\u5728, \u65e0\u6cd5\u786e\u5b9a\u8fd9\u4e2ay\u662f\u5355\u7eaf\u7684x\u8fd8\u662fx\u4e58\u4e86\u6839\u53f7m(r)\u7684\u7ed3\u679c (\u53ef\u4ee5\u6784\u9020\u51fa\u76f8\u540c\u7ed3\u6784\\(x=x*r^{-1}\\)). \u56e0\u6b64\u662f\u7edd\u5bf9concealing\u7684                 - \u5982\u679c\u80fd\u8ba1\u7b97QR\u7684\u8bdd, \u5c31\u53ef\u4ee5\u9009\u62e9\u4efb\u610f\u4e00\u4e2ab\u6765\u4fee\u6539\u7b54\u6848\u4e86, \u7528\u7684\u89e3\u6cd5\u5c31\u662f\u4e0a\u9762\u8fd9\u4e2a     - Week9 Zero Knowledge proof       collapsed:: true         - 68-69, 43, 71-74         - Slides           collapsed:: true             -         - Notes             -                collapsed:: true                 - 1. hash\u7684\u65b9\u5f0f\u8bc1\u660e\u6211\u77e5\u9053\u5bc6\u7801                   2. \u901a\u8fc7\u591a\u6b21\u9a8c\u8bc1\u4fdd\u8bc1\u6982\u7387\u4e58\u79ef\u5f88\u5c0f                   3. NP\u95ee\u9898\u5c31\u53ef\u4ee5\u7528ZKP prove                   4. V\u7684honest\u662f\u4e00\u4e2a\u524d\u63d0                   5. \u6b63\u7ecf\u7684ZKP\u8fc7\u7a0b\u662f\u4e0d\u4f1a\u8ba9V\u77e5\u9053P\u7684\u4fe1\u606f\u7684         - \u5b8f\u89c2\u7406\u89e3             - NP\u95ee\u9898, \u53ef\u4ee5\u88ab\u8f7b\u677everify\u7684\u624d\u53ef\u4ee5, Everything in NP has zero knowledge proofs             - Commitment -&gt; Challenge -&gt; Response -&gt; Verify             - Completeness,  Soundness, Zero-knowledge             - Completeness \u53ea\u8981\u63a8\u4e00\u904d\u5982\u679c\u77e5\u9053knowledge, \u5c31\u4e00\u5b9a\u80fd\u591f\u5728\u4e24\u79cd\u4e0d\u540c\u60c5\u51b5\u4e0bresponse\u5230\u80fd\u591f\u88abverify\u7684\u7b54\u6848\u5373\u53ef             - Soundness \u662fP\u53ea\u6709\u5f88\u5c0f\u7684\u6982\u7387\u53ef\u4ee5cheat, \u53ef\u4ee5\u7531 special soundness \u6765imply, \u5177\u4f53\u662f\u4e24\u4e2a\u76f8\u540c\u7684commitment with \u4e24\u4e2a\u4e0d\u540c\u7684challenge, \u5728honest V\u7684\u60c5\u51b5\u4e0b\u53ef\u4ee5\u89e3\u51fa\u77e5\u8bc6x; \u666e\u901a\u60c5\u51b5\u4e0b\u5c31\u662f\u6839\u636echeat\u7684\u6982\u7387\u6765\u6c42\u5f97\u591a\u6b21\u7684joint prob             - Zero-knowledge \u9700\u8981\u6784\u5efasimulation, \u5177\u4f53\u6d41\u7a0b\u662f\u5148\u8bbe\u5b9a\u597dresponse\u7684\u503c\u548cchallenge\u7684\u503c (\u4f8b\u5982\u968f\u673a\u7684b = 0 or 1), \u518d\u6839\u636ecommitment\u7684\u7b97\u5f0f\u7b97\u51facommitment. \u7531\u4e8er\u548cc\u90fd\u662f\u968f\u673a\u7684, \u770b\u4e0d\u51fa\u6765\u662f\u5047\u7684             - \u6848\u4f8b\u6709:               collapsed:: true                 - 3-colourability                   collapsed:: true                     - P\u6bcf\u6b21\u90fd\u751f\u6210\u4e00\u4e2a\u65b0\u7684permutation, \u5bf9\u539f\u672c\u7684colour\u65b9\u6848\u8fdb\u884c\u4e00\u4e2a\u6253\u4e71, \u4e00\u6b21\u6027commit\u6240\u6709\u7684vertex v, \u4ee5\u4fdd\u5bc6\u7684\u5f62\u5f0f, V\u9009\u62e92\u4e2avertex\u8ba9P\u63ed\u9732, \u9a8c\u8bc1\u662f\u4e0d\u662f\u4e24\u4e2a\u4e0d\u540c\u7684\u989c\u8272. \u591a\u6b21\u91cd\u590d. \u56e0\u4e3acommit\u4e86\u6240\u6709vertex, \u6240\u4ee5\u6ca1\u6cd5\u9003\u907f (1-1/E)^k                     - ZK\u7684\u6a21\u62df, \u56e0\u4e3acommit\u4fdd\u5bc6\u6027, \u6240\u4ee5\u53ef\u4ee5\u4e8b\u5148\u628a\u9700\u8981\u7684\u4e24\u4e2av\u7ed9\u8bbe\u5b9a\u4e3a\u4e0d\u4e00\u6837, \u522b\u7684\u4e0d\u7ba1\u662f\u5565\u90fd\u6ca1\u4e8b                 - graph isomorphism,                   collapsed:: true                     - G1-&gt;G2-&gt;H                     - P\u7528\u81ea\u5df1\u77e5\u9053\u7684pi\u968f\u673a\u9009\u62e9\u4e00\u4e2aG1\u6216\u8005G2\u751f\u6210H, V\u4e5f\u968f\u673a\u9009\u62e9\u4e00\u4e2aG1 or G2 \u8ba9P\u7ed9\u51fa\u5230\u8fbeH\u7684\u65b9\u6848. V\u53ef\u4ee5\u8f7b\u677e\u8ba4\u8bc1, P\u4e00\u8f6e\u53ea\u67091/2\u6982\u7387\u9003\u907f                     - ZK \u7684\u6a21\u62df, \u81ea\u5df1\u9009\u54ea\u4e2aG\u751f\u6210, \u4e00\u5f00\u59cbH\u5c31\u4ece\u54ea\u91cc\u751f\u6210, \u90a3\u4e48\\(\\psi\\)\u5c31\u4e00\u5b9a\u53ef\u4ee5                 - Schorr\u2019s identification protocol                   collapsed:: true                     - \\(g\\ and\\ y=g^x\\) \u662f\u516c\u5171\u4fe1\u606f, \\(x\\)\u53ea\u6709P\u77e5\u9053, P \u968f\u673a\u9009k commit \u4e00\u4e2a\\(r=g^k\\), V \u53d1\u9001\\(e\\), P\u8ba1\u7b97\\(s=k+x\u00b7e\\), V \u8ba1\u7b97\u662f\u5426\\(r = g^s\u00b7y^{-e}\\)                     - ZK\u7684\u6a21\u62df, \u968f\u673a\u751f\u6210\u4e00\u4e2as\u548ce, \u8ba9\\(r = g^s\u00b7y^{-e}\\) \u5373\u53ef                 - Chaum-Pedersen protocol                   collapsed:: true                     - \u4e0a\u9762\u7684\u6269\u5c55, (r1, r2) = (g^k, h^k), \u4f7f\u7528\u76f8\u540c\u7684e\u5bf9\u4ed6\u4fe9challenge                 - Quadratic residue                   collapsed:: true                     - P\u8981\u8bc1\u660e\u4ed6\u77e5\u9053u, \u8fd9\u4e2a \u8fd9\u4e2a\\(u^2 = x\\); \\(x\\) \u662f\u516c\u5f00\u7684; P\u968f\u673a\u9009\u4e86\\(v\\) commit \\(y = v^2\\), V\u968f\u673a\u9009\u4e860\u6216\u80051, \u6765\u51b3\u5b9a\\(u\\)\u9910\u4e0d\u53c2\u4e0e, P\u8ba1\u7b97\u53d1\u9001\\(z=u^b\u00b7v\\), V\u9a8c\u8bc1\\(z^2=x^b\u00b7y\\)                     - ZK \u6a21\u62df: \u73b0\u51b3\u5b9a\\(b\\)\u548c\\(z\\), \u518d\u8ba1\u7b97\\(y=z^2\u00b7(x^b)^{-1}\\)             - \u7591\u95ee:               collapsed:: true                 - \u5e73\u65b9\u5269\u4f59\u91cc\u8fd8\u6709\u5fc5\u8981b?     - Revision       collapsed:: true         - \u6570\u8bba             - [[\u7fa4]]\u662f\u4e00\u79cd\u4ee3\u6570\u7ed3\u6784 algebraic structure, \u6ee1\u8db3\u975e\u7a7a\u96c6\u5408, \u4e8c\u5143\u8fd0\u7b97\u548c\u5c01\u95ed\u6027                 -                  -                  -                  -                  -                  - \u6700\u540e\u7684\u5faa\u73af\u7fa4\u662f\u6709\u751f\u6210\u5143\u7684\u963f\u8d1d\u5c14\u7fa4             - \u4f8b\u5982(G, ), G\u662f\u4e00\u4e2a\u96c6\u5408, \u4f46\u662f\u4e3a\u4e86\u65b9\u4fbf\u8d77\u89c1, \u4e4b\u540e\u4e5f\u4f1a\u628a\u4ed6\u79f0\u4e4b\u4e3a\u7fa4             - \u7fa4\u7684\u5927\u5c0f\u53eb\u505a\u7fa4\u7684\u9636, \u5373order             - \u7fa4\u8981\u6ee1\u8db3\u56db\u4e2a\u7279\u6027                 - \u5c01\u95ed\u6027: \u7fa4\u5185\u5143\u7d20\u505a\u4efb\u610f\u64cd\u4f5c\u8fd8\u5728\u7fa4\u5185                 - \u7ed3\u5408\u6027: \u80fd\u7ed3\u5408\u7387                 - \u5355\u4f4d\u5143: \u6709\u4e2ae,                 - \u9006\u5143: \u6bcf\u4e2a\u5143\u7d20\u90fd\u6709             - [[\u4e58\u6cd5\u7fa4]]: \u7531\u4e8e\u6bcf\u4e2a\u5143\u7d20\u90fd\u8981\u6709\u548c\u5b83\u4e58\u8d77\u6765\u662f1\u7684\u4e1c\u897f, \u56e0\u6b64\u9700\u89811~ n-1\u4e2d\u7684\u6240\u6709\u5143\u7d20\u90fd\u4e0en\u4e92\u7d20             - [[\u5b50\u7fa4]]: \u5b50\u7fa4\u4e5f\u662f\u7fa4, \u5176\u96c6\u5408\u662f\u67d0\u4e2a\u7fa4\u7684\u975e\u7a7a\u5b50\u96c6, \u5982\u679c\u662f\u6709\u9650\u96c6, \u6ee1\u8db3\u5c01\u95ed\u6027, \u90a3\u5c31\u4e00\u5b9a\u662f\u5b50\u7fa4 (\u5b50\u7fa4\u4e0d\u4e00\u5b9a\u662f\u6709\u9650\u96c6, \u6bd4\u5982\u6709\u7406\u6570\u52a0\u7fa4\u5c31\u662f\u5b9e\u6570\u52a0\u7fa4\u7684\u5b50\u7fa4)                 - \\(G^m := \\{a^m|a \\in G\\}, m \\in Z\\)                     - \\((Z_n^*)^m\\)\u662f\\(Z_n^*\\)\u7684\u5b50\u7fa4 (m\u6b21\u5269\u4f59\u5b50\u7fa4)                 - \\(mZ_n := \\{mz\\ mod\\ n\\ |\\ z \\in Z_n \\}\\)             - [[\u963f\u8d1d\u5c14\u7fa4]]\u662f\u6ee1\u8db3\u4ea4\u6362\u5f8b\u7684\u7fa4, \u56e0\u4e3a\u5e76\u4e0d\u662f\u6240\u6709\u7fa4\u90fd\u6ee1\u8db3\u4ea4\u6362\u5f8b\u7684             - [\u966a\u96c6] \u662fG\u4e2d\u7684\u67d0\u4e2a\u5143\u7d20a, \u4e58\u4e0a\u67d0\u4e2a\u5b50\u7fa4H\u5f97\u5230\u7684\u96c6\u5408, \u5b50\u96c6\u662f\u5212\u5206\u7684\u6807\u51c6, \u966a\u96c6\u5199\u4f5c\\([a]_H\\). \u662f\u4ee5\u5b50\u7fa4H\u4e3a\u57fa\u51c6, \u53bb\u89c2\u5bdf\u5927\u7fa4\u4e2d\u6bcf\u4e00\u5143\u7d20\u4e0e\u5b50\u7fa4\u7684\u5173\u7cfb                 - \u4f8b\u5982\u8bf43Z, \u5c31\u662f\u5212\u5206\u4e86Z\u4e2d\u6240\u6709\u80fd\u88ab3\u6574\u9664\u7684\u5143\u7d20. \u6ce8\u610f\u8fd9\u662f\u4e2a\u52a0\u6cd5\u7fa4                     - \\([2]_{3Z} = \\{2, 2+-3, 2+-6, ...\\}\\)                 - \u5728\u966a\u96c6\u7684\u6982\u5ff5\u4e2d, \u5b50\u7fa4\u5c31\u50cf\u662f\u4e00\u4e2a\u5206\u7c7b\u5668, \u628a\u4e0d\u540c\u7684\u4e3b\u7fa4G\u5143\u7d20\u5206\u7c7b\u5230\u4e00\u4e2a\u4e2a\u966a\u96c6\u4e2d, \u6bcf\u4e2a\u5143\u7d20\u53ea\u5b58\u5728\u4e8e\u552f\u4e00\u7684\u4e00\u4e2a\u966a\u96c6\u4e2d. \u6709\u4e00\u4e2a\u7279\u6b8a\u7684\u966a\u96c6\u662f\u5b50\u7fa4H\u672c\u8eab, \u53ea\u6709\u8fd9\u4e2a\u966a\u96c6\u62e5\u6709\u5355\u4f4d\u5143, \u56e0\u6b64\u624d\u662f\u4e00\u4e2a\u7fa4, \u5176\u4ed6\u7684\u90fd\u53ea\u662f\u96c6\u5408                     -              - [[\u62c9\u683c\u6717\u65e5\u5b9a\u7406]]:                 - |H| | |G| . \u5982\u679cG\u662f\u6709\u9650\u7fa4, H\u7684\u9636\u4e00\u5b9a\u662fG\u7684\u9636\u7684\u56e0\u5b50. \u6bd4\u5982G\u5185\u670915\u4e2a\u5143\u7d20, \u5b50\u7fa4\u7684\u9636\u53ea\u53ef\u80fd\u662f1,3,5,15. \u800c\u7d20\u6570\u9636\u7684\u7fa4, \u5c31\u53ea\u80fd\u6709e\u548c\u5b83\u672c\u8eab\u4e86                 - \u5728\u966a\u96c6\u6982\u5ff5\u4e2d, G\u4e2d\u7684\u5143\u7d20\u4f1a\u6839\u636e\u4e0e\u5b50\u7fa4H\u7684\u5173\u7cfb\u88ab\u5206\u7c7b\u5230\u4e0d\u540c\u7684\u966a\u96c6\u4e2d, \u8fd9\u4e9b\u966a\u96c6\u7684\u5e76\u96c6\u5c31\u662f\u6574\u4e2a\u7fa4G, G\u7684\u5927\u5c0f\u7b49\u4e8e\u8fd9\u4e9b\u966a\u96c6\u7684\u5927\u5c0f\u4e4b\u548c, \u800cH\u6784\u9020\u7684\u966a\u96c6\u7684\u5927\u5c0f\u90fd\u548cH\u76f8\u7b49, \u6240\u4ee5H\u7684\u5927\u5c0f\u5fc5\u5b9a\u8981\u80fd\u6574\u9664G\u7684\u5927\u5c0f                     -                      -              - \u5546\u7fa4: G/N := \\(\\{[a]_N|a\\in G\\}\\) (G/N,) \u4e8c\u5143\u8fd0\u7b97\u5b9a\u4e49\u4e3a\\([a*b]_N\\)                 - G\u7684\u5b50\u7fa4N\u5f62\u6210\u7684\u966a\u96c6\u4eec\u7684\u96c6\u5408, \u91cd\u70b9\u5173\u6ce8\u7684\u662f, G\u5185\u9664\u4e86N\u8fd9\u4e2a\u5b50\u7fa4\u5916\u7684\u5143\u7d20                     -                      -              - [[\u5faa\u73af\u7fa4]]                 - \u5faa\u73af\u7fa4G\u6ee1\u8db3\u7fa4G\u5185\u7684\u4e00\u4e2a\u5143\u7d20g, \u53ef\u4ee5\u4e0e\u81ea\u8eab\u64cd\u4f5c, \u5f97\u5230\u7fa4\u5185\u6240\u6709\u7684\u5143\u7d20.                 - \u4f8b\u5982Z\u8fd9\u4e2a\u52a0\u6cd5\u7fa4\u4e2d\u7684+-1\u5c31\u662f                 - Z5 \u8fd9\u4e2a\u4e58\u6cd5\u7fa4\u4e2d\u76842\u548c3\u90fd\u53ef\u4ee5\u901a\u8fc7\u51e0\u6b21\u65b9\u5f97\u52301-4\u8fd9\u56db\u4e2a\u6570\u5b57                     -                  - \u5b9a\u7406: \u4efb\u610f\u5faa\u73af\u7fa4\u90fd\u662f\u963f\u8d1d\u5c14\u7fa4                 - \u5b9a\u7406: \u5faa\u73af\u7fa4\u7684\u5b50\u7fa4\u4e5f\u662f\u5faa\u73af\u7fa4                     - \u4f8b\u5982\u6211\u4eec\u6709\u4e2a\u5faa\u73af\u7fa4Z5, \u751f\u6210\u5143\u662f2, \u4ed6\u6709\u4e2a\u6a215\u7684\u5b50\u7fa4{1,4}, \u751f\u6210\u5143\u662f4, \u4e5f\u662f\u5faa\u73af\u7fa4. \u53ef\u4ee5\u770b\u5230\u539f\u751f\u6210\u5143\u7684\u5e42\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u751f\u6210\u5143\u751f\u6210\u5b50\u7fa4\u5faa\u73af\u7fa4. g\u7684\u5e42\u751f\u6210\u7684\u6709\u53ef\u80fd\u662fG\u81ea\u5df1, \u4e5f\u53ef\u80fd\u662f\u5b50\u7fa4             - \u5143\u7d20\u7684\u9636 order:                 - \u7fa4\u6709order, \u7fa4\u5185\u7684\u5143\u7d20\u4e5f\u6709order                 - \\(a^n = e\\), \u6700\u5c0f\u7684\u90a3\u4e2an\u5c31\u662f\u5143\u7d20a\u7684\u9636                 - \u5143\u7d20\u7684order\u53ef\u4ee5\u7406\u89e3\u4e3a\u5176\u79bb\u5355\u4f4d\u5143\u7684\u8ddd\u79bb, \u5373\u7ecf\u8fc7\u51e0\u6b21\u4e0e\u81ea\u5df1\u7684\u8fd0\u7b97\u53ef\u4ee5\u5230\u8fbee                 - \u5e76\u4e0d\u662f\u6240\u6709\u5143\u7d20\u90fd\u6709order, \u6c38\u8fdc\u5230\u4e0d\u4e86\u7684, \u90a3\u5c31\u662f\u65e0\u9650\u9636\u7684, \u8fd9\u79cd\u5143\u7d20\u4e5f\u6210\u4e3a\u4e0d\u4e86gen                 - \u4f46\u662f\u5bf9\u4e8e\u6709\u9650\u5faa\u73af\u7fa4\u800c\u8a00, G\u7684\u9636\u5c31\u5f97\u7b49\u4e8e\u5176\u751f\u6210\u5143\u7684\u9636, \u56e0\u4e3aG\u5c31\u662fg\u751f\u6210\u7684                 - \u6027\u8d282\uff1a\u6709\u9650\u5faa\u73af\u7fa4\u7684\u9636\u662fn\uff0c\u5219\u751f\u6210\u5143g\u7684\u9636\u4e5f\u662fn\uff0c \u4e14\u7fa4\u91cc\u5143\u7d20 g,g2,g3,\u2026,gn (=g0 =e)\u5404\u4e0d\u76f8\u540c\u3002                     -                 - \u6027\u8d28 3: d\u662fn\u7684\u4e00\u4e2a\u56e0\u5b50, n\u9636\u7684\u6709\u9650\u5faa\u73af\u7fa4, \u53ea\u6709\u552f\u4e00\u4e00\u4e2a\u5b50\u7fa4\u7684\u9636\u4e3ad                     - \u4f8b\u5982\u8bf4, Z5 \u8fd9\u4e2a\u4e58\u6cd5\u5faa\u73af\u7fa4\u7684\u751f\u6210\u5143\u662f2\u6216\u80053, \u9636\u662f4, 1-4\u8fd9\u56db\u4e2a\u6570\u5b57, \u56e0\u5b50\u67091, 2, 4 \u6240\u4ee5\u521a\u597d\u5c31\u662f\u6709\u4e09\u4e2a\u5faa\u73af\u5b50\u7fa4, \u751f\u6210\u5143\u662fG\u7684\u751f\u6210\u5143\u7684\u5e42                     - \u518d\u4e3e\u4e2a\u4f8b\u5b50, commitment\u4e2d\u6211\u4eec\u53d6\u4e86\u4e2asafe prime = 2q+1, \u5176\u4e58\u6cd5\u7fa4\u7684order\u662f2q, \u90a3\u4e48\u5f88\u663e\u7136\u5176\u5b50\u7fa4\u7684\u9636\u5fc5\u987b\u4e3a1, 2, q, 2q. \u5373\u53ea\u6709\u56db\u4e2a\u5b50\u7fa4                     -                 - \u6027\u8d28 4: n\u9636\u6709\u9650\u5faa\u73af\u7fa4(\u6307g\u751f\u6210\u7684\u5faa\u73af\u7fa4), \u5bf9\u4e8e\u6240\u6709\u6574\u6570k, \\(g^k\\)\u7684\u9636(\u5373\\(g^k\\)\u4f5c\u4e3a\u751f\u6210\u5143\u751f\u6210\u7684\u5b50\u7fa4\u5927\u5c0f)\u4e3a\\(n/gcd(n,k)\\). \u7406\u89e3\u4e00\u4e0b\u5c31\u662f, \u5f53k\u548cn\u4e92\u7d20\u7684\u65f6\u5019, \\(g^k\\)\u7684\u9636\u5c31\u548cn\u4e00\u6837\u5927, \u56e0\u4e3a\u6700\u5c0f\u516c\u56e0\u6570\u662f1, \u5176\u4ed6\u60c5\u51b5\u4e0b\u751f\u6210\u7684\u5b50\u7fa4\u90fd\u4f1a\u6bd4\u8fd9\u4e2a\u8981\u5c0f, \u4e14\u5b50\u7fa4\u7684\u5927\u5c0f\u4e3an\u7684\u56e0\u5b50                     -                  - \u6027\u8d285: \u751f\u6210\u5143\u7684\u4e2a\u6570\u4e0e\u6709\u9650\u5faa\u73af\u7fa4\u7684\u9636\u6709\u5173. n\u9636\u7684\u6709\u9650\u5faa\u73af\u7fa4\u6709\\(\\phi (n)\\)\u4e2a\u751f\u6210\u5143. \u8fd9\u4e2a\u5c31\u662f\u4e0a\u4e00\u4e2a\u6027\u8d28\u7684\u7406\u89e3\u63a8\u5bfc\u7ed3\u8bba, \u516c\u56e0\u6570\u4e3a1\u7684\u60c5\u51b5\u751f\u6210\u7684\u5c31\u662fG\u672c\u8eab, \u5373\u4e3an\u9636\u6709\u9650\u5faa\u73af\u7fa4\u7684\u751f\u6210\u5143. \\(\\phi (n)\\)\u662f\u6b27\u62c9\u51fd\u6570, \u8ba1\u7b97\u7684\u662f1-n-1\u4e2d\u6709\u591a\u5c11\u4e0en\u4e92\u7d20\u7684\u6570 (\u8d28\u6570\u7684\u6b27\u62c9\u51fd\u6570\u5c31\u662f\u8d28\u6570\u51cf\u4e00)                     -                      - \u8fd8\u662f\u4ee5Z5 \u8fd9\u4e2a\u4e58\u6cd5\u5faa\u73af\u7fa4\u4e3a\u4f8b, \u5176\u9636\u4e3a4, \\(\\phi(4) = 2\\) (\u6709\u4e24\u4e2a\u4e0eorder 4 \u4e92\u7d20\u7684\u6570), \u56e0\u6b64\u8fd9\u4e2a\u7fa4\u67092\u4e2a\u751f\u6210\u5143. g\u7684power k\u53ef\u4ee5\u53d61\u548c3\u4e24\u4e2a\u6570\u5b57, \u56e0\u4e3a\u53ea\u67091, 3\u4e0e4\u4e92\u7d20. \u5bf9\u4e8e\u4efb\u610fg, \u4f8b\u59822\u76841\u6b21\u548c3\u6b21\u5c31\u662f2\u548c3; \u800c3\u76841\u6b21\u548c3\u6b21\u4e5f\u662f3\u548c2. \u56e0\u6b64\u6211\u4eec\u53ea\u8981\u6709\u4e00\u4e2ag, \u5c31\u53ef\u4ee5\u5f97\u5230\u8fd9\u4e2a\u5faa\u73af\u7fa4\u7684\u6240\u6709\u751f\u6210\u5143.                     - \u975e\u4e92\u7d20\u7684power\u5f97\u5230\u7684g^k\u5219\u662f\u5faa\u73af\u5b50\u7fa4\u7684\u751f\u6210\u5143(\u6839\u636e\u4e0a\u4e00\u6761\u6027\u8d28, \u9636\u4e3a4\u7684\u5faa\u73af\u7fa4, \u67091, 2, 4\u4e09\u4e2a\u5b50\u7fa4\u5927\u5c0f, \u56e0\u6b64\u8fd8\u6709\u4e00\u4e2a\u5faa\u73af\u5b50\u7fa4\u7684\u5927\u5c0f\u4e3a2, \u53ef\u4ee5\u75312^2=4\u751f\u6210                 - \u6027\u8d28 6: G\u662f\u6709\u9650\u7fa4, \u5143\u7d20a\u7684\u9636\u662f|G|\u7684\u56e0\u5b50. \u6ce8\u610f\u8fd9\u91cc\u7684\u7fa4\u53ea\u9700\u8981\u662f\u4e2a\u6709\u9650\u7fa4\u5373\u53ef. \u5982\u679c\u67d0\u4e2a\u5143\u7d20a\u6709order, \u5219\u8fd9\u4e2aorder\u662f\u662f|G|\u7684\u56e0\u5b50. \u8fd9\u4e2a\u4e5f\u7b26\u5408\u62c9\u683c\u6717\u65e5\u5b9a\u7406\u7684\u76f4\u89c9, \u56e0\u4e3a\u5982\u679ca\u80fd\u591f\u751f\u6210\u5b50\u7fa4\u7684\u8bdd, \u5b50\u7fa4\u7684\u5927\u5c0f\u4e5f\u4e00\u5b9a\u662fG\u7684\u5927\u5c0f\u7684\u56e0\u5b50.                     - \u8fd9\u91cc\u4e5f\u63a8\u51fa\u4e86\u4e00\u4e2a\u7ed3\u8bba: \u5047\u8bbea\u7684\u9636\u662fk, \u5373a\u7684k\u6b21\u4f1a\u5f97\u5230\u5355\u4f4d\u5143, \u56e0\u4e3ak | |G|, \u6240\u4ee5\u4f5c\u4e3ak\u7684\u500d\u6570\u7684G\u6b21\u7684a\u4e5f\u80fd\u5f97\u5230\u5355\u4f4d\u5143, \\(a^{|G|}=e\\), [[\u6b27\u62c9\u5b9a\u7406]]                     -                  - \u6027\u8d287: \u7d20\u6570\u9636\u7684\u7fa4\u5fc5\u7136\u662f\u6709\u9650\u5faa\u73af\u7fa4                     - \u7fa4\u7684\u5927\u5c0f\u4e3a\u7d20\u6570\u7684\u8bdd, \u90a3\u8fd9\u4e2a\u7fa4\u4e00\u5b9a\u662f\u6709\u9650\u5faa\u73af\u7fa4. \u5e73\u65f6\u6211\u4eec\u9047\u5230\u7684\u5355\u7eaf\u7684\u6a21p\u7684\u4e58\u6cd5\u7fa4\u7684\u9636\u662fp-1, \u5e76\u4e0d\u4e00\u5b9a\u662f\u5faa\u73af\u7fa4. \u4f46\u662f\u5982\u679c\u7fa4\u7684\u9636\u4e3a\u7d20\u6570\u7684\u8bdd, \u90a3\u5c31\u4e00\u5b9a\u662f\u4e86.                     - \u539f\u56e0\u5728\u4e8e\u5047\u8bbeG\u4e2d\u5143\u7d20a\u662f\u6709\u9636\u7684, \u90a3\u4ed6\u7684\u9636\u5c31\u53ea\u80fd\u662fp\u7684\u56e0\u5b50, \u53731 \u6216\u8005 p, \u5982\u679ca\u4e0d\u662fe\u7684\u8bdd, \u90a3k\u5c31\u53ea\u80fd\u662fp, \u90a3\u5c31\u610f\u5473\u7740, a\u4e00\u5b9a\u662f\u751f\u6210\u5143\u4e86, \u56e0\u4e3a\u53ef\u4ee5\u751f\u6210p\u4e2a\u5143\u7d20                         -                      - \u62ffcommitment\u4e2d\u7684\u4f8b\u5b50, safe prime (2q+1)\u7684\u4e58\u6cd5\u7fa4\u4e0d\u4e00\u5b9a\u662f\u5faa\u73af\u7fa4, \u56e0\u4e3a\u5176\u9636\u4e3a2q, \u4f46\u662f\u6839\u636e\u62c9\u683c\u6717\u65e5\u5b9a\u7406, \u5b83\u4e00\u5b9a\u4f1a\u6709\u9636\u4e3a1, 2, q, 2q\u7684\u56db\u4e2a\u5b50\u7fa4, \u5176\u4e2d\u53ea\u6709q\u662f\u6709\u7528\u7684. \u800c\u4e14\u8fd9\u4e2a\u5b50\u7fa4\u7684\u9636\u4e3a\u4e00\u4e2aprime q, \u90a3\u4e48\u8fd9\u4e2a\u7fa4\u5c31\u4e00\u5b9a\u662f\u4e2a\u6709\u9650\u5faa\u73af\u7fa4, \u4e0d\u4ec5\u5982\u6b64, \u5176Euler Totient\u4e3a(q-1), \u56e0\u4e3a\u6bd4\u8d28\u6570\u5c0f\u7684\u6570\u5168\u90fd\u548c\u4ed6\u4e92\u7d20, \u90a3\u4e48\u5176\u751f\u6210\u5143\u7684\u4e2a\u6570\u4e5f\u5c06\u4f1a\u6709q-1\u4e2a, \u4e5f\u5c31\u662f\u8bf4\u9664\u4e86\u5355\u4f4d\u51431\u4ee5\u5916\u5176\u4ed6\u7684\u7fa4\u5185\u5143\u7d20\u90fd\u53ef\u4ee5\u4f5c\u4e3a\u751f\u6210\u5143. \u6839\u636e\u6027\u8d285, g\u7684k=1~q-1\u90fd\u662f\u751f\u6210\u5143, \u4e5f\u5c31\u662f\u6240\u6709\u975e1\u90fd\u662f\u751f\u6210\u5143                         - \u5bfb\u627e\u80fd\u591f\u751f\u6210q\u5927\u5c0f\u7684\u5b50\u7fa4\u7684g\u7684\u8fc7\u7a0b\u4e2d, \u76ee\u6807\u662f\u627e\u5230g^q = 1(\u610f\u5473\u7740\u5176\u80fd\u751f\u6210\u4e00\u4e2aq\u5927\u5c0f\u7684\u5faa\u73af\u7fa4\\(a^{|G|}=e\\)),  generator g\u7684\u5bfb\u627e, \u4e5f\u662f\u5145\u5206\u5229\u7528\u4e86safe prime\u7684\u7279\u6027: p = 2q+1, 2q = p-1, \u800cp-1\u6070\u597d\u662fp\u8fd9\u4e2a\u6709\u9650\u7fa4\u7684order, \u4efb\u4f55\u7fa4\u5185\u5143\u7d20(1~p-1)\u7684(p-1)\u6b21\u65b9\u90fd\u7b49\u4e8e\u5355\u4f4d\u51431(\u56e0\u4e3a\u6027\u8d286\u53ca\u5176\u63a8\u8bba), (p-1)\u6b21\u65b9\u53c8\u53ef\u4ee5\u5199\u4f5c(2q)\u6b21\u65b9. \u6211\u4eec\u5c31\u60f3\u7740\u53bb\u6784\u9020\u4e00\u4e2a2q, \u56e0\u6b64\u5c31\u628ag\u5199\u505a\u4e86f^2, \u90a3\u4e2ag\u7684q\u6b21\u65b9\u5c31\u53ef\u4ee5\u8868\u793a\u4e3a \\(g^q = f^{2q} = f^{p-1}\\ mod\\ p = 1\\) \u968f\u673a\u6570f\u7684\u7fa4order\u6b21\u65b9\u4e00\u5b9a\u662f1, \u4e5f\u5c31\u627e\u5230\u4e86\u90a3\u4e2ag                         - \u800ch\u4f5c\u4e3a\u8fd9\u4e2a\u7d20\u9636\u7fa4\u7684\u4e00\u4e2a\u5143\u7d20, \u80af\u5b9a\u4e5f\u662f\u4e00\u4e2a\u751f\u6210\u5143, \u4e5f\u610f\u5473\u7740\u5b83\u80af\u5b9a\u662f\u4e0a\u9762\u627e\u5230\u7684g\u7684\u67d0\u6b21\u65b9                 - [[\u539f\u6839]]                     - \u5982\u679cg\u662f\u6a21n\u4e0b\u7684\u539f\u6839, \u5219g\u8fd9\u4e2a\u5143\u7d20\u7684\u9636(\u5373\u51e0\u6b21g\u64cd\u4f5c\u53ef\u4ee5\u5f97\u5230e) \u7b49\u4e8en\u7684\u6b27\u62c9\u51fd\u6570(\u5373Zn*\u7684order, size, \u56e0\u4e3a\u662f\u4e58\u6cd5\u7fa4, \u6240\u4ee5\u5143\u7d20\u4e0en\u4e92\u7d20), \u6b64\u65f6g\u662f\u6a21n\u4e0b\u7684\u539f\u6839, \u4e5f\u662fZn*\u7684\u751f\u6210\u5143                     - \u5373g\u80fd\u591f\u5728\u6a21n\u65f6\u751f\u6210\u7684\u4e0d\u540c\u6570\u7684\u4e2a\u6570, \u7b49\u4e8en\u4ee5\u5185\u4e0en\u4e92\u7d20\u7684\u4e2a\u6570, \u5373g\u4e3a\u539f\u6839                     - \u4f46\u662f\u539f\u6839\u5e76\u4e0d\u4e00\u5b9a\u5b58\u5728, \u4e5f\u5c31\u662f\u8bf4\\(\\phi(n)\\)\u9636\u7684\u5143\u7d20\u5e76\u4e0d\u4e00\u5b9a\u5b58\u5728, \u4e5f\u5c31\u662f\u8bf4\u4e58\u6cd5\u7fa4\u4e0d\u4e00\u5b9a\u4f1a\u6709\u751f\u6210\u5143, \u5e76\u4e0d\u4e00\u5b9a\u662f\u5faa\u73af\u7fa4. \u5b58\u5728\u7684\u6761\u4ef6\u662f #card                         - \u8bbep\u662f\u5947\u7d20\u6570\uff0ce\u662f\u6b63\u6574\u6570                         - \\(n=1,2,4,p^e,2p^{e}\\)\u65f6\uff0c\u6a21n\u4e0b\u5b58\u5728\u539f\u6839. \u4e0b\u8868\u7684\u5468\u671f, \u5373\u4e3aZn\u548c\u539f\u6839\u7684order, \u4e5f\u662f\u6b27\u62c9\u51fd\u6570(n)                             - | n | \u6a21n\u7684\u539f\u6839(\u6709\u53f7\u7684\u6570\u6ca1\u6709\u539f\u6839\uff0c\u6b64\u65f6\u662f\u6709\u6700\u5927\u6a21n\u5468\u671f\u7684\u6570) | \u5468\u671f  |                               | 1 | 0 | 1 |                               | 2 | 1 | 1 |                               | 3 | 2 | 2 |                               | 4 | 3 | 2 |                               | 5 | 2, 3 | 4 |                               | 6 | 5 | 2 |                               | 7 | 3, 5 | 6 |                               | 8 | 3, 5, 7 | 2 |                               | 9 | 2, 5 | 6 |                               | 10 | 3, 7 | 4 |                               | 11 | 2, 6, 7, 8 | 10 |                               | 12 | 5, 7, 11 | 2 |                               | 13 | 2, 6, 7, 11 | 12 |                               | 14 | 3, 5 | 6 |                               | 15 | 2, 7, 8, 13 | 4 |                               | 16 | 3, 5, 11, 13 | 4 |                               | 17 | 3, 5, 6, 7, 10, 11, 12, 14 | 16 |                               | 18 | 5, 11 | 6 |                               | 19 | 2, 3, 10, 13, 14, 15 | 18 |                               | 20 | 3, 7, 13, 17 | 4 |                               | 21 | 2, 5, 10, 11, 17, 19 | 6 |                               | 22 | 7, 13, 17, 19 | 10 |                               | 23 | 5, 7, 10, 11, 14, 15, 17, 19, 20, 21 | 22 |                               | 24 | 5, 7, 11, 13, 17, 19, 23 | 2 |                               | 25 | 2, 3, 8, 12, 13, 17, 22, 23 | 20 |                               | 26 | 7, 11, 15, 19 | 12 |                               | 27 | 2, 5, 11, 14, 20, 23 | 18 |                               | 28 | 3, 5, 11, 17, 19, 23 | 6 |                               | 29 | 2, 3, 8, 10, 11, 14, 15, 18, 19, 21, 26, 27 | 28 |                               | 30 | 7, 13, 17, 23 | 4 |                               | 31 | 3, 11, 12, 13, 17, 21, 22, 24 | 30 |                               | 32 | 3, 5, 11, 13, 19, 21, 27, 29 | 8 |                               | 33 | 2, 5, 7, 8, 13, 14, 17, 19, 20, 26, 28, 29 | 10 |                               | 34 | 3, 5, 7, 11, 23, 27, 29, 31 | 16 |                               | 35 | 2, 3, 12, 17, 18, 23, 32, 33 | 12 |                               | 36 | 5, 7, 11, 23, 29, 31 | 6 |                         -                      - \u9898\u5916\u8bdd, RSA\u4e2d\u7528\u5230\u7684n=pq, \u5c31\u4e0d\u662f\u4e00\u4e2a\u5faa\u73af\u7fa4, \u53ea\u662f\u4e00\u4e2a\u666e\u901a\u7684\u963f\u8d1d\u5c14\u7fa4, \u5e76\u6ca1\u6709\u751f\u6210\u5143                     - \u6a21n\u4e0b\u6784\u6210\u7684\u4e58\u6cd5\u7fa4\u7684\u9636\u662fphi(n), \u56e0\u4e3a\u5176\u5143\u7d20\u5fc5\u987b\u8981\u6709\u9006\u5143. \u56e0\u4e3an\u9636\u6709\u9650\u5faa\u73af\u7fa4\u7684\u751f\u6210\u5143\u6709phi(n)\u4e2a, \u6240\u4ee5\u6a21n\u7684\u751f\u6210\u5143, \u5373\u539f\u6839\u4e00\u5171\u6709\\(\\phi(\\phi(n))\\)\u4e2a (\u6a21n\u5b58\u5728\u539f\u6839\u7684\u60c5\u51b5\u4e0b)                     - \u6027\u8d28\u90fd\u548c\u4e0a\u9762\u5143\u7d20\u7684\u9636\u7684\u6027\u8d28\u4e00\u81f4, \u53ea\u9700\u8981\u627e\u5230\u6a21n\u7684\u9636\u5373\u53ef                     - \u6709\u54ea\u4e9b\u5faa\u73af\u7fa4? #card                         - \\(Z^*_p\\), where p is a prime                         - Order \u4e3aprime\u7684\u90fd\u662f\u5faa\u73af\u7fa4                 - [[\u79bb\u6563\u5bf9\u6570]]                     -                      - \u51e0\u4e2a\u8981\u70b9:                         - g\u5fc5\u987b\u662f\u539f\u6839, \u624d\u53eb\u79bb\u6563\u5bf9\u6570                         - x\u8303\u56f4\u4e3a\\([0,\\phi(n))\\)                         - \u7531\u4e8ex\u6709\u56fa\u5b9a\u8303\u56f4, \u5728\u5e42\u5904\u53ef\u4ee5\u8fdb\u884cmod phi n\u7684\u64cd\u4f5c, \u4f46\u662f\u6700\u540e\u7684\u503c\u8fd8\u662fmod n\u7684                 - [[\u73af]] (ring)                     - (R, +, \u00b7) \u662f\u4e00\u4e2a\u73af (+\u548c\u4e58\u53ea\u662f\u62bd\u8c61\u6982\u5ff5, \u610f\u601d\u662f\u9700\u8981\u6709\u4e24\u4e2a\u4e8c\u5143\u8fd0\u7b97), \u4e14\u6ee1\u8db3                         - \u52a0\u6cd5\u963f\u8d1d\u5c14\u7fa4: (R, +)\u662f\u963f\u8d1d\u5c14\u7fa4                         - (R,\u00b7) \u6ee1\u8db3\u5c01\u95ed\u6027\u3001\u7ed3\u5408\u7387 (\u4e58\u6cd5\u534a\u7fa4)                         - \u5206\u914d\u5f8b                     - \u56e0\u4e3a\u591a\u9879\u5f0f\u8fd9\u4e2a\u5c31\u9700\u8981\u52a0\u6cd5\u548c\u4e58\u6cd5\u540c\u65f6\u5b58\u5728, \u6240\u4ee5\u6709\u4e86\u73af                     - \u4f8b\u5b50                         -                          -                          -                      - [[\u96f6\u5143]] theta (zero element)                         - \u5b9a\u4e49\uff1a*\u662f\u5b9a\u4e49\u5728\u975e\u7a7a\u96c6\u5408A (\u81f3\u5c11\u6709\u4e24\u4e2a\u5143\u7d20)\u4e0a\u7684\u4e8c\u5143\u8fd0\u7b97\uff0ctheta E A, \u5982\u679c\u5bf9\u4e8eA\u4e2d\u6240\u6709\u5143\u7d20\uff0c\u90fd\u6709\\(\\theta * a = a * \\theta = \\theta\\) \u4e5f\u5c31\u662f\u548c\u96f6\u5143\u64cd\u4f5c\u4f1a\u53d8\u6210\u96f6\u5143\u81ea\u5df1                         - \u7fa4\u4e2d\u4e0d\u53ef\u80fd\u6709\u96f6\u5143, \u56e0\u4e3a\u96f6\u5143\u6ca1\u6709\u9006\u5143, \u7fa4\u5185\u552f\u4e00\u7684\u5143\u7d20\u53ef\u4ee5\u662f\u5355\u4f4d\u5143                         - \u52a0\u6cd5\u5355\u4f4d\u5143\u5c31\u662f\u6240\u8c13\u7684\u96f6\u5143                         - \u73af\u91cc\u9664\u4e86\u52a0\u6cd5\u5355\u4f4d\u5143\u4ee5\u5916\u7684\u6240\u6709\u5143\u7d20, \u90fd\u7edf\u79f0\u4e3a\u975e\u96f6\u5143\u7d20                         - \u56e0\u4e3a\u73af\u4e2d\u7684\u52a0\u6cd5\u5fc5\u987b\u662f\u963f\u8d1d\u5c14\u7fa4, \u6240\u4ee5\u96f6\u5143\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684                         - \u5e73\u51e1\u73af(trivial ring) R\u53ea\u542b\u6709\u96f6\u5143, \\(e = \\theta\\)                         - \u975e\u5e73\u51e1\u73af(non-trivial ring): R \u4e2d\u4e0d\u6b62\u6709\u96f6\u5143 \\(e \\neq \\theta\\)                     - [[\u96f6\u56e0\u5b50]] zero devisor                         -                          - \u73af\u91ccn\u4e2ae\u53ef\u4ee5\u7b49\u4e8etheta, \u5982\u679c\u8fd9\u4e2an\u6709\u8d28\u56e0\u6570\u5206\u89e3, \u90a3\u5c31\u6709\u96f6\u56e0\u5b50\u4e86                         -                          -                      - [[\u6574\u73af]]: Z, Q, R\u662f\u4e2a\u6574\u73af, \u56e0\u4e3aZ\u662f\u4e2a\u542b\u5e7a\u4ea4\u6362\u73af, \u4e14\u6ca1\u6709\u96f6\u56e0\u5b50, \u56e0\u4e3a\u4ed6\u662f\u65e0\u9650\u7684, \u6ca1\u529e\u6cd5\u505a\u5230\u975e\u96f6\u4e24\u675f\u60f3\u4e58\u7b49\u4e8e0.                         - n\u662f\u7d20\u6570\u65f6, Zn\u662f\u6574\u73af, \u540c\u65f6\u4e5f\u662f\u57df, \u56e0\u4e3an\u4ee5\u4e0b\u7684\u6570\u90fd\u4e0en\u4e92\u7d20, \u4efb\u4f55\u4e24\u4e2a\u6570\u76f8\u4e58\u6a21n\u90fd\u4e0d\u4f1a\u7b49\u4e8e0                 - [[\u57df]] (field)                     - \u57df\u662f\u52a0\u6cd5\u548c\u4e58\u6cd5\u90fd\u80fd\u6784\u6210\u963f\u8d1d\u5c14\u7fa4\u7684\u73af                         -                      -                      - \u672c\u8d28\u4e0a\u57df\u4e5f\u662f\u4e00\u79cd\u6574\u73af, \u57df\u91cc\u4e5f\u6ca1\u6709\u96f6\u56e0\u5b50, \u5373\u96f6\u5143\u4e0d\u80fd\u5206\u89e3\u6210\u4e24\u4e2a\u975e\u96f6\u5143\u7d20\u7684\u4e58\u79ef                     - Q, R\u6709\u7406\u6570\u548c\u5b9e\u6570, \u90fd\u662f\u57df, \u662f\u65e0\u9650\u57df; \u4f46\u662f\u65e0\u9650\u6574\u73af\u4e0d\u4e00\u5b9a\u90fd\u662f\u57df, \u6bd4\u5982\u8bf4Z\u4e0d\u662f\u57df, \u56e0\u4e3a\u5c3d\u7ba1\u53bb\u6389\u4e860, \u4f46\u662fZ\u4e2d\u9664\u4e86+-1\u4ee5\u5916\u7684\u5143\u7d20\u90fd\u6ca1\u6709\u4e58\u6cd5\u9006\u5143, \u6240\u4ee5\u4e0d\u662f\u963f\u8d1d\u5c14\u7fa4, \u6240\u4ee5\u4e0d\u662f\u57df                     - \u4f46\u662f\u6709\u9650\u6574\u73af\u90fd\u662f\u6709\u9650\u57df                     - \u6027\u8d28                         - \u975e\u5e73\u51e1: \u5305\u6db5\u81f3\u5c11\u4e24\u4e2a\u5143\u7d20, theta\u548ce, \u4e24\u4e2a\u4e0d\u76f8\u7b49                           id:: 6409295e-d77e-485e-8525-27dcafcc0c4c                         - \u6ca1\u6709\u96f6\u56e0\u5b50                 - [[\u7279\u5f81]] Characteristic                     - \u5b9a\u4e49\uff1aR\u662f\u73af\uff0c\u5982\u679c\u5b58\u5728\u6700\u5c0f\u6b63\u6574\u6570m\uff0c\u5bf9\u4e8e\\(\\forall a \\in R\\), \u4f7f\u5f97\\(m a=\\theta\\)\uff0c\u5219\u79f0m \u662f\u73afR\u7684\u7279\u5f81\u3002\u5982\u679c\u8fd9\u6837\u7684 m\u4e0d\u5b58\u5728\uff0c\u5219\u79f0R\u7684\u7279\u5f81\u662f0\u3002\u8bb0\u4e3a Char R.                         - \u8fd9\u4e2a\u6709\u70b9\u7c7b\u4f3c\u4e8e\u4e58\u6cd5\u7fa4\u91cc\u9762\u5143\u7d20\u7684\u9636, \u5c31\u662f\u591a\u5c11\u4e2aa\u4e0e\u81ea\u5df1\u64cd\u4f5c\u80fd\u5230e, \u8fd9\u91cc\u5c31\u53d8\u6210\u4e86\u52a0\u6cd5\u9636, \u5373\u591a\u5c11\u4e2aa\u4e0e\u81ea\u5df1\u76f8\u52a0\u80fd\u5230theta. \u4f46\u662f\u7279\u5f81\u662f\u5bf9\u6240\u6709a\u800c\u8a00, \u4f7f\u5f97\u73af\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u90fd\u53ef\u4ee5\u5230\u8fbetheta, \u624d\u53eb\u505a\u7279\u5f81                         - \u96f6\u5143\u662f0, \u5355\u4f4d\u5143\u662f1                     - \u5b9a\u74061\uff1a\u5982\u679c\u73af\u7684\u7279\u5f81\u4e0d\u7b49\u4e8e0\uff0c\u73af\u91cc\u5143\u7d20\u7684\u52a0\u6cd5\u9636\u5c31\u90fd\u662f\u6709\u9650\u7684\uff0c\u800c\u4e14\u90fd\u662f\u7279\u5f81\u7684\u56e0\u5b50\u3002                     - \u4f8b\u5b50:                         - Z\u7684char\u662f0, \u56e0\u4e3a\u5176\u65e0\u9650                         -                          - \u56e0\u4e3a\u6a21n\u4e0bZ_n\u91cc\u7684\u6240\u6709\u5143\u7d20\u81ea\u5df1\u76f8\u52a0n\u904d\u90fd\u80fd\u5230\u8fbe0, \u4f8b\u5982Z6\u73af\u7684\u7279\u5f81\u662f6, \u91cc\u9762\u7684\u5143\u7d20\u7684\u9636\u90fd\u662f6\u7684\u56e0\u5b50, \u53ea\u80fd\u662f1,2,3,6                     - \u5b9a\u74062: \u542b\u5e7a\u4ea4\u6362\u73af\u7684\u7279\u5f81\u662f0 \u6216\u8005\u5176\u5355\u4f4d\u5143\u7684\u52a0\u6cd5\u9636                         -                      - \u5b9a\u74063: \u6574\u73af\u7684\u7279\u5f81\u7b49\u4e8e0\u6216\u662f\u7d20\u6570                         - \u6574\u73af\u662f\u975e\u5e73\u51e1\u73af, \u610f\u5473\u7740\u6574\u73af\u91cc\u9762\u81f3\u5c11\u5305\u6db5\u4e24\u4e2a\u5143\u7d20, theta \u548ce, \u4e14\u4e24\u4e2a\u4e0d\u76f8\u7b49                         -                          - \u8bbem\u4e3a\u5408\u6570, \u53ef\u4ee5\u5199\u4f5cs\u548ct\u7684\u4e58\u79ef, st\u90fd\u5728(1,m)\u4e4b\u95f4, \u56e0\u4e3a\u5408\u6570\u4e14\u4e58\u79ef\u624d\u80fd\u8ba9e=theta                         - \u56e0\u4e3ame=theta, \u6839\u636e\u7ed3\u5408\u7387, me=ste=sete (e\u662f\u5355\u4f4d\u5143\u53ef\u4ee5\u8fd9\u4e48\u5199), se\u548cte\u56e0\u4e3as\u548ct\u90fd\u5728m\u4ee5\u5185, \u662f\u6ca1\u529e\u6cd5\u5f97\u5230theta\u7684, \u4e5f\u5c31\u662f\u8bf4\u4e24\u8005\u90fd\u4e0d\u7b49\u4e8etheta, \u53c8\u56e0\u4e3a\u6574\u73af\u6ca1\u6709\u96f6\u56e0\u5b50(\u5373\u6ca1\u6709\u4e24\u4e2a\u975e\u96f6\u5143\u7684\u6570\u7684\u4e58\u79ef\u7b49\u4e8etheta), \u6240\u4ee5\u4e0a\u9762\u8fd9\u4e2a\u5f0f\u5b50\u4e0d\u7b49\u4e8etheta, \u4e5f\u5c31\u662fme\u4e0d\u7b49\u4e8etheta. \u8fd9\u8bc1\u660e\u4e86m\u4e0d\u80fd\u591f\u5199\u6210\u4e24\u4e2a\u975e\u96f6\u6570\u7684\u4e58\u79ef, \u4e5f\u5c31\u662f\u8bf4m\u5fc5\u987b\u662f\u4e00\u4e2ae\u4e58\u4e0a\u4e00\u4e2a\u7d20\u6570, \u4e5f\u5c31\u610f\u5473\u7740m\u672c\u8eab\u5fc5\u987b\u662f\u7d20\u6570. \u8fd9\u6837\u624d\u80fd\u5b9e\u73b0\u81f3\u5c11\u5305\u542b\u4e24\u4e2a\u5143\u7d20.                         - \u8fd9\u4e5f\u5370\u8bc1\u4e86\u4e3a\u4ec0\u4e48\u57df\u7684\u7279\u5f81\u5fc5\u987b\u8981\u662f\u7d20\u6570\u6216\u80050                     -          - 2122           collapsed:: true             - Perfect security of crypto systems                 - simple cipher system and computations                 - perfectly secure                 - encryption modes                 - chosen-plaintext attacks                 - quantum random generator                 - XOR             - Shamir secret sharing             - Commitment schemes                 - design                 - Security analysis         - 2021           collapsed:: true             - Perfect security of crypto systems                 - one-time pad                 - pseudo-random number             - Shamir Secret Sharing             - commitment scheme                 - Security analysis                 - Elliptic curves         - 1829           collapsed:: true             - Symmetric Encryption                 - perfectly secure             - Key Management             - Public Key Cryptography                 - RSA                 - Elliptic Curve Public Key             - Shamir Secret Sharing             - Commitment Schemes             - Zero-Knowledge Proofs         - \u603b\u7ed3\u4e00\u4e0b           collapsed:: true             - \u56fa\u5b9a\u7684\u57fa\u7840\u52a0\u5bc6\u7b97\u6cd5\u6982\u7387\u8ba1\u7b97\u9898, \u5206\u6790\u7b80\u5355\u52a0\u5bc6\u7cfb\u7edf\u7684\u4e0d\u5b89\u5168\u6027,             - \u5bf9\u9519\u9898\u4f1acover\u5f88\u591a\u5185\u5bb9             - Shamir secret sharing \u56fa\u5b9a\u8981\u8003, \u4f1a\u8ba1\u7b97, \u8bc1\u660e\u5176\u5b8c\u5907\u6027             - Commitment schemes \u56fa\u5b9a\u8981\u8003             - \u8981\u4f1a\u8bbe\u8ba1commitment protocol             - \u8981\u4f1a\u5206\u6790\u7b97\u6cd5\u7684security, \u4f8b\u5982commitment\u7684binding \u548cconcealing             - \u516c\u94a5\u5bc6\u7801\u7cfb\u7edf\u4e5f\u8981\u8003, \u8981\u719f\u6089RSA\u548cEC             - ZKP\u8981\u8003\u7684         - \u95ee\u7b54\u9898\u6a21\u7248             - \u5e76\u975e\u6240\u6709commitment schemes \u9700\u8981random input, deterministic\u7684g^x\u5c31\u4e0d\u9700\u8981             - MAC\u53ef\u4ee5accountability, commitment\u4e0d\u53ef\u4ee5             -             - w2                 - A crypto sys with |K|=|C|=|P|is perfectly secure, iff for all k in K, p(K=k)=1/|k| and for all m and c there is a unique k with e_k(m)=c. \u56e0\u4e3ap(C=c)&gt;0\u5c31\u4e00\u5b9a\u6709k\u4ecem\u6620\u5c04\u5230c; m\u52a0\u5bc6\u5f97\u5230\u7684\u4e00\u5b9a\u5728C\u4e2d, c\u4e00\u5b9a\u5b58\u5728\u610f\u5473\u7740\u4e00\u5b9a\u6709m\u5230c, \u6240\u4ee5c\u7684\u89e3\u5bc6\u4e5f\u4e00\u5b9a\u5728P\u4e2d, \u6240\u4ee5\u662f\u53cc\u5411\u5305\u542b\u7684injective\u5173\u7cfb. \u6240\u4ee5\u4e00\u5b9a\u662f\u552f\u4e00\u7684key\u5728map. 1/K\u7684\u6982\u7387\u5219\u7531\u8d1d\u53f6\u65af\u5206\u89e3p(P=m|C=c)\u548ckey\u7684\u552f\u4e00\u6027\u53ef\u4ee5\u63a8\u51fap(K=ki)=p(C=c), \u5404\u4e2akey\u6982\u7387\u76f8\u540c             - w4             - H=f(H||m_i) i=1...n\u7684\u65b9\u5f0f\u5f62\u6210MAC\u7684\u7f3a\u9677, \u4f7f\u7528k\u4f5c\u4e3a\u521d\u59cbH                 - h(k||m1) = c1 -&gt; h(k||m1||m2) = f(c1||m2)                 - h(k||m1) = c1 -&gt; h(k||m1||b||m2) = f(f(c1||b)||m2)                 - MAC\u7684\u8981\u6c42\u662f \u7b7e\u540d\u548c\u9a8c\u8bc1\u53ea\u80fd\u901a\u8fc7MAC_k(m)\u8fdb\u884c! k\u540e\u7f6e\u53ef\u4ee5\u7528collision\u6765\u627e\u5230\u4e24\u4e2a\u76f8\u540cm\u5f97\u5230\u4e00\u6837\u7684MAC             - w6             - Why &lt;t parties cannot recover secret                 - Parties 1 and 3 can determine a degree-1 polynomial g(x) by pooling their shares, but f(x) is degree-2. For any value z in Z31, there exists a degree-2 polynomial h(x) with h(0)=z, h(1)=g(1), and h(3)=g(3). Thus, parties 1 and 3 cannot learn anything about the secret s by combining their shares. - ## Info     - 8:2     - \u661f\u671f\u4e00 9:00 - 11:00     - \u8fd9\u95e8\u8bfe\u662f\u5173\u4e8e\u5f00\u53d1\u4fdd\u8bc1integrity \u548c confidentiality\u7684\u5bc6\u7801\u5de5\u5177, availability \u4e0d\u662f\u91cd\u70b9 - ## Syllabus     - Cryptographic primitives: pseudo-random number generators, block ciphers, pseudo-random functions, hash functions, message authentication codes, key derivation functions, public-key cryptography     - Symmetric key cryptography: perfect secrecy and the one-time pad, modes of operation for semantic security and authenticated encryption (e.g. encrypt-then-MAC, OCB, GCM), message integrity (e.g. CMAC, HMAC)     - Public key cryptography: trapdoor permutations (e.g. RSA), public key encryption (e.g. RSA, El Gamal), digital signatures, public-key infrastructures and certificates, hardness assumptions (e.g. integer factoring and Diffie-Hellmann), Elliptic Curve Cryptography     - Authenticated key exchange protocols (e.g. TLS)     - Quantum key exchange protocols     - Cryptographic protocols: challenge-response authentication, zero-knowledge protocols, commitment schemes, oblivious transfer, secret sharing and applications, anonymity (may pick different protocols from that list in different instances of that module)     - Security definitions and attacks on cryptographic primitives: goals (e.g. indistinguishability, unforgability, collison-resistance, cryptographic games, etc.) and attacker capabilities (e.g. chosen message attacks for signatures, birthday attacks, side channel attacks, fault injection attacks.     - Advanced topics such as Secure Multi-Party Computations: secret sharing schemes and other techniques needed for defining such computations; presentation of one full scheme for secure two-party computations.     - Cryptographic standards and references implementations - ## Links     - Scientia     - 2021-2022     - 2022-2023         - - ## PDF #PDF     -      -          -"},{"location":"crypto/Principles%20of%20Distributed%20Ledgers/","title":"Principles of Distributed Ledgers","text":"<p>tags:: IC, Security, Course, Block Chain, Uni-S10 alias:: PDL Ratio: 7:3 Time: \u661f\u671f\u4e94 11:00 - 13:00</p>"},{"location":"crypto/Principles%20of%20Distributed%20Ledgers/#notes","title":"Notes","text":"<ul> <li>\u672f\u8bed\u8868<ul> <li>ERC: Ethereum Request for Comment, \u7528\u6765\u589e\u52a0\u6216\u4fee\u6539\u4ee5\u592a\u574a\u7684\u529f\u80fd, \u6216\u662f\u6807\u51c6\u5316\u4ee5\u592a\u574afeatures</li> <li>EIP: Ethereal Improvement Proposals</li> <li>Application Binary Interface (ABI)</li> <li>Decentralised Finance ([[DeFi]])</li> <li>Oracle</li> </ul> </li> <li>Week2 Intro to blockchain   collapsed:: true<ul> <li>novelty of bitcoin</li> <li>smart contracts   collapsed:: true<ul> <li>a computerised transaction protocol that executes the terms of a contract.\u201d</li> <li>\u667a\u80fd\u5408\u7ea6\u7b49\u4ef7\u4e8e\u4e00\u6bb5\u4e8b\u5148\u5c31\u88ab\u89c4\u5b9a\u597d\u903b\u8f91\u548c\u6761\u6b3e\u7684\u8ba1\u7b97\u673a\u4ee3\u7801\u88ab\u6fc0\u6d3b\u8fd0\u884c\u7684\u72b6\u6001\uff0c\u540c\u65f6\uff0c\u667a\u80fd\u5408\u7ea6\u4e5f\u63d0\u4f9b\u4e86\u901a\u7528\u7684\u7528\u6237\u63a5\u53e3\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u63a5\u53e3\u4e0e\u7528\u6237\u4ea4\u4e92\u3002</li> <li>\u662f\u4e00\u4e2a\u7531\u6570\u5b57\u8868\u5355\u6307\u5b9a\u7684\u627f\u8bfa\uff0c\u8fd9\u4e2a\u627f\u8bfa\u5305\u542b\u5173\u7cfb\u5230\u591a\u65b9\u6267\u884c\u7684\u4e00\u7ec4\u534f\u8bae\u3002</li> <li>\u667a\u80fd\u5408\u7ea6\u662f\u4e00\u4e2a\u7531\u8ba1\u7b97\u673a\u5904\u7406\u3001\u53ef\u6267\u884c\u5408\u7ea6\u6761\u6b3e\u7684\u4ea4\u6613\u534f\u8bae\uff0c\u5176\u603b\u4f53\u76ee\u6807\u662f\b\u6ee1\u8db3\u534f\u8bae\u65e2\u5b9a\u7684\u6761\u4ef6\uff0c\u4f8b\u5982\u652f\u4ed8\u3001\u62b5\u62bc\u3001\u4fdd\u5bc6\u534f\u8bae\u3002\u8fd9\u53ef\u4ee5\u964d\u4f4e\u5408\u7ea6\u6b3a\u8bc8\u9020\u6210\u7684\u635f\u5931\uff0c\u964d\u4f4e\u4ef2\u88c1\u548c\u5f3a\u5236\u6267\u884c\u6240\u4ea7\u751f\u7684\u6210\u672c\u4ee5\u53ca\u5176\u4ed6\u7684\u4ea4\u6613\u6210\u672c\u3002</li> <li>\u8fd9\u91cc\u201c\u65e2\u5b9a\u7684\u4e1a\u52a1\u6d41\u7a0b\u3001\u673a\u5668\u4eba\u6a21\u6837\u7684\u4eba\u673a\u4ea4\u4e92\u754c\u9762\u3001\u53cc\u65b9\u540c\u610f\u627f\u8bfa\u201d\u7ec4\u6210\u4e86\u667a\u80fd\u5408\u7ea6\u7684\u6982\u5ff5\uff0c\u5b83\u751a\u81f3\u5177\u6709\u4e00\u5b9a\u7684\u6cd5\u5f8b\u6548\u529b\u3002\u5373\u4ea4\u6613\u811a\u672c, \u5b9a\u4e49\u4e86\u4ea4\u6613\u7684\u8fc7\u7a0b</li> </ul> </li> <li>\u201cTokens\u201d are smart contracts that contain the logic for storing and updating   balances of token holders</li> <li>Decentralized Finance (DeFi) is a peer-to-peer powered financial system.   collapsed:: true<ul> <li>\u2022 Non-custodial   \u2022 Permissionless   \u2022 Openly auditable \u2022 Composable</li> </ul> </li> <li>Blockchain   collapsed:: true<ul> <li>\u2022 A data structure that stores information, such as transaction data   \u2022 Peer-to-peer network   \u2022 Data is recorded in multiple identical data stores (ledgers) that are collectively maintained by a distributed network of computers (nodes)   \u2022 Consensus algorithm (all nodes see the same data)</li> </ul> </li> <li>Block   collapsed:: true<ul> <li>A data structure that stores information, such as transaction data.<ul> <li>block header: Identifies a particular block of transaction</li> <li>txn count: total number of transactions</li> <li>txns: every transaction in the block</li> </ul> </li> <li>Proof of Work<ul> <li>\u5927\u5bb6\u7528\u7b97\u529bsolve puzzles, \u7c7b\u4f3c\u4e8e\u66b4\u529b\u641c\u7d22, \u641c\u51fa\u6765\u4e86\u5c31\u53ef\u4ee5\u51b3\u5b9a\u65b0\u7684valid\u7684transaction\u53ef\u4ee5\u88ab\u52a0\u5165\u5230next block<ul> <li>\u6bd4\u5982\u8bf4\u901a\u8fc7\u89e3\u51b3\u4e00\u4e2apuzzle, \u2018sdhagisa\u2019\u540e\u9762\u52a0\u4ec0\u4e48\u6570\u5b57\u53ef\u4ee5\u8ba9hash\u7684\u524d\u9762\u51e0\u4f4d</li> </ul> </li> <li>\u6240\u6709\u4eba\u90fd\u53ef\u4ee5\u53d6\u9a8c\u8bc1</li> <li>target\u7684\u53ef\u80fd\u6027\u662f2\u7684256\u6b21\u5206\u4e4b\u4e00, \u5bf9\u4e8eSHA-256</li> <li>bitcoin\u7528\u7684\u662fmerckle tree \u7528\u6765\u8bb0\u5f55transactions \u6bcf\u4e00\u4e2a\u533a\u5757\u8fd8\u6709\u4e00\u4e2aMerkle\u54c8\u5e0c\u7528\u6765\u786e\u4fdd\u8be5\u533a\u5757\u7684\u6240\u6709\u4ea4\u6613\u8bb0\u5f55\u65e0\u6cd5\u88ab\u7be1\u6539\u3002</li> </ul> </li> <li>longest chain rule</li> </ul> </li> <li></li> <li>Forks:   collapsed:: true<ul> <li>soft fork: backward compatible protocol changes</li> <li>hard fork: incur permanent  split of the blockchain; \u4f8b\u5982\u65b0\u7684currency\u51fa\u73b0\u7684\u65f6\u5019, \u4f8b\u5982Bitcoin\u548cBitcoin cash, \u8fd9\u4fe9\u6ca1\u6709\u672c\u8d28\u533a\u522b, \u662fblock\u5927\u5c0f\u533a\u522b</li> </ul> </li> <li>ETH\u4ee5\u592a\u574a:   collapsed:: true<ul> <li>\u901a\u8fc7 proof of stake \u6765\u51b3\u5b9a\u8c01\u6709\u8bb0\u8d26\u6743, \u518d\u4e5f\u6ca1\u6709mining, \u53ea\u7528\u7b80\u5355\u8ba1\u7b97hash</li> <li>\u62e5\u6709\u7684\u4ee5\u592a\u574a\u8d8a\u591a, \u62e5\u6709\u7684\u65f6\u95f4\u8d8a\u957f, \u8d8a\u6709\u53ef\u80fd\u6709\u8fd9\u4e2a\u8bb0\u8d26\u6743, \u83b7\u5f97\u5956\u52b1</li> <li>\u4f46\u6709\u673a\u5236\u9632\u6b62\u8fc7\u5206, \u4f46\u662f\u8fd8\u662f\u9700\u8981\u4e00\u70b9\u8ba1\u7b97\u7684</li> <li>reward\u673a\u5236<ul> <li>\u2022 source vote: voted for the correct source checkpoint   \u2022 target vote: voted for the correct target checkpoint   \u2022 head vote: voted for the correct head block   \u2022 sync committee reward: participated in a sync committee \u2022 proposer reward: proposed a block in the correct slot   \u2022 block fees: fees associated with a particular block</li> </ul> </li> <li>Account<ul> <li>Externally owned accounts<ul> <li>\u516c\u94a5\u548c\u79c1\u94a5,</li> <li>\u516c\u94a5\u662f\u5730\u5740</li> <li>\u79c1\u94a5\u7528\u6765\u7b7e\u540d</li> </ul> </li> <li>contract accounts<ul> <li>\u90e8\u7f72\u4e3asmart contracts</li> <li>\u6ca1\u6709\u76f8\u5173\u8054\u7684\u79c1\u94a5</li> </ul> </li> </ul> </li> <li>\u4e0e\u6bd4\u7279\u5e01\u76f8\u6bd4\uff0c\u4ee5\u592a\u574a\u9996\u5148\u4e0d\u662f\u4e00\u4e2a\u5355\u7eaf\u7684\u6570\u5b57\u8d27\u5e01\u9879\u76ee\uff0c\u5b83\u53ef\u4ee5\u63d0\u4f9b\u5168\u4e16\u754c\u65e0\u5dee\u522b\u7684\u533a\u5757\u94fe\u667a\u80fd\u5408\u7ea6\u5e94\u7528\u5e73\u53f0\uff0c\u8fd9\u4e2a\u5e73\u53f0\u57fa\u4e8e\u6211\u4eec\u524d\u9762\u6587\u7ae0\u6240\u4ecb\u7ecd\u7684\u533a\u5757\u94fe\u56db\u5927\u6838\u5fc3\u6280\u672f\u8981\u7d20\uff0c\u5373P2P\u7f51\u7edc\u3001\u5171\u8bc6\u673a\u5236\u3001\u8d26\u6237\u6a21\u578b\u3001\u52a0\u5bc6\u6a21\u5757\u3002</li> <li>\u9664\u4e86\u4ee5\u4e0a\u7684\u56db\u4e2a\u6280\u672f\u8981\u7d20\uff0c\u4ee5\u592a\u574a\u8fd8\u63a8\u51fa\u4e86EVM\u2014\u2014\u4ee5\u592a\u574a\u667a\u80fd\u5408\u7ea6\u865a\u62df\u673a\uff0c\u5e76\u4e14\uff0c\u5b83\u8fd8\u63a8\u51fa\u4e86\u81ea\u5df1\u7684\u667a\u80fd\u5408\u7ea6\u8bed\u8a00Solidity\u3002</li> <li>\u4e8e\u662f\uff0c\u533a\u5757\u94fe\u7684\u5f00\u53d1\u8005\u56e0\u4e3a\u667a\u80fd\u5408\u7ea6\u7684\u51fa\u73b0\u5f00\u59cb\u5206\u4e3a\u4e24\u7c7b\u3002\u7b2c\u4e00\u7c7b\u662f\u516c\u94fe\u5e95\u5c42\u5f00\u53d1\u8005\uff0c\u4e3b\u8981\u662f\u4ee5C++\u548cGo\u8bed\u8a00\u4e3a\u4e3b\u7684\u5168\u8282\u70b9\u5f00\u53d1\u8005\uff0c\u4ed6\u4eec\u9700\u8981\u5bf9\u533a\u5757\u94fe\u5404\u4e2a\u6280\u672f\u6a21\u5757\u6709\u5f88\u6df1\u7684\u7406\u89e3\u3002</li> <li>\u7b2c\u4e8c\u7c7b\u662f\u667a\u80fd\u5408\u7ea6\u5f00\u53d1\u8005\uff0c\u4e5f\u5c31\u662f\u5e94\u7528\u5f00\u53d1\u8005\uff0c\u8fd9\u7c7b\u5f00\u53d1\u8005\u5bf9\u533a\u5757\u94fe\u7684\u8fd0\u884c\u539f\u7406\u4e0d\u9700\u8981\u7406\u89e3\u5f88\u6df1\uff0c\u53ea\u9700\u8981\u4f1a\u7f16\u5199Solidity\uff0c\u4e86\u89e3\u89c4\u8303\u5373\u53ef\u3002</li> </ul> </li> <li>Tutorial \u6a21\u62df\u4e00\u4e0b\u6bd4\u7279\u5e01\u7684\u5efa\u7acb<ul> <li>LATER \u8bb0\u5f55\u4e00\u4e0btut\u5b66\u5230\u7684\u6bd4\u7279\u5e01\u77e5\u8bc6</li> </ul> </li> </ul> </li> <li>Week3 Ethereum and Smart Contracts   collapsed:: true<ul> <li>Lecture Ethereum and Smart Contracts<ul> <li>Ethereum: a transaction-based state machine.<ul> <li>\u4ee5\u592a\u574a\u66f4\u50cf\u662f\u4e00\u53f0\u4e16\u754c\u8ba1\u7b97\u673a, world computer, truly global singleton, \u56e0\u4e3a\u667a\u80fd\u5408\u7ea6\u7684\u90e8\u7f72\u4f1a\u8ba9\u6240\u6709\u4eba\u90fd\u80fd\u63a5\u89e6\u5230\u8fd9\u53f0\u4e16\u754c\u8ba1\u7b97\u673a\u7684\u7b97\u529b, \u5e76\u4e14\u4fdd\u8bc1\u6240\u6709\u4eba\u63a5\u89e6\u5230\u7684\u7edf\u4e00</li> <li>\u4ee5\u592a\u574a\u7684\u7279\u70b9<ul> <li>\u2022 Allows you to run decentralized programs using the Ethereum Virtual Machine (EVM) \u8fd0\u884c\u53bb\u4e2d\u5fc3\u5316\u7684\u7a0b\u5e8f   \u2022 Uses a blockchain as a means of storing state, but is much more than just a blockchain \u533a\u5757\u94fe\u7528\u4e8e\u5b58\u50a8state   \u2022 Is maintained by a network of nodes, which store exact copies of this state \u7f51\u7edc\u8282\u70b9\u6765\u5b58\u50a8copies   \u2022 Is non-custodial and permissionless and completely transparent (like other   blockchains) \u5b8c\u5168\u900f\u660e, \u5565\u90fd\u53ef\u4ee5\u88ab\u6240\u6709\u4eba\u770b\u5230</li> </ul> </li> <li></li> <li>Blockchain<ul> <li>Stores a series of state transitions starting from a genesis state</li> <li>The transition to the next state is determined by all transactions included in a block. \u5230\u4e0b\u4e00\u4e2astate\u7684transition \u662f\u7531block\u4e2d\u7684\u6240\u6709transaction\u51b3\u5b9a\u7684</li> <li>transition is agreed upon by nodes</li> <li>Blocks and states   collapsed:: true<ul> <li></li> <li></li> <li>\u56e0\u6b64\u5c31\u6ca1\u6709\u6bd4\u7279\u5e01\u91cc\u9762\u7684UTXO, \u4e0d\u9700\u8981\u8ba1\u7b97\u672a\u82b1\u8d39\u4f59\u989d\u4e86</li> </ul> </li> <li>Accounts as world state<ul> <li>Accounts are a mapping between addresses   and account state with the following information: accounts \u662f\u5730\u5740\u5230\u5730\u5740state\u7684mapping<ul> <li></li> </ul> </li> <li>Externally owned accounts (EOA) \u5916\u90e8\u8d26\u6237   \u2022 Account is created by generating private/public key pair   \u2022 Address is derived from the public key   \u2022 Can initiate transactions</li> <li>Contract accounts (CA) \u5408\u7ea6\u8d26\u6237<ul> <li>\u2022 Deployed as smart contracts and controlled by their code \u2022 Do not have an associated private key   \u2022 Cannot initiate transactions</li> </ul> </li> </ul> </li> <li>Transactions<ul> <li>A transaction is a cryptographically signed instruction (message call) sent by an actor external to Ethereum, i.e., an EOA \u5916\u90e8\u8d26\u6237\u7b7e\u540d\u7684\u6307\u4ee4</li> <li>Basic components<ul> <li>\u2022 to: Address to receive the message call   \u2022 value: Amount of ETH to be transferred   \u2022 gasPrice and gasLimit: Maximum amount of ETH the sender is prepared to pay for the execution   \u2022 data: Byte array with the input data for the transaction   \u2022 signature: Signature of the sender to proof ownership</li> </ul> </li> <li>\u4ee5\u592a\u574a\u4e2d\u53ea\u6709transactions \u53ef\u4ee5\u4ea7\u751fstate change, transactions \u53ef\u4ee5\u505a\u5230\u7684\u6709<ul> <li>Transferring a balance to another account, e.g., a transfer of Ether \u4f59\u989d\u4ea4\u6613</li> <li>Triggering the execution of smart contract code, which can cause more complex state transitions \u667a\u80fd\u5408\u7ea6\u7684\u6267\u884c</li> </ul> </li> <li>Consensus + Chain Selection \u5982\u4f55\u8fbe\u6210\u5171\u8bc6   collapsed:: true<ul> <li>\u76ee\u6807 objective<ul> <li>\u2022 Agree on the current state of the system   \u2022 Find a way to transition to the next state   \u2022 Ensure that the state is valid (does not violate the rules of the system)</li> </ul> </li> <li>\u5411\u4e0b\u4e00\u4e2astate\u7684\u8f6c\u79fb transitioning to the next state<ul> <li>Select a proposer for the next state, \u5e76\u4e14\u786e\u4fdd\u6ca1\u6709\u592a\u591aproposers</li> <li>agree that the proposed state should indeed be the next state</li> </ul> </li> <li>Selection and validation of the next block: proof-of-stake<ul> <li>Validators \u201cstake\u201d some ETH to participate in the network \u6839\u636e\u6301\u6709\u4ee5\u592a\u5e01\u7684\u91cf\u548c\u65f6\u95f4\u6765\u4f5c\u4e3a\u80a1\u6743</li> <li>Each \u201cepoch\u201d, a validator is randomly selected to create a new block \u6839\u636e\u80a1\u6743\u968f\u673a\u9009\u62e9</li> <li>\u662f\u5426valid\u4f1a\u88ab\u5176\u4ed6validators check</li> </ul> </li> <li>Chain selection rule<ul> <li>Different selection rules are possible   \u2022 Bitcoin uses the \u201dlongest chain rule\u201d   \u2022 Ethereum uses the \u201dheaviest chain rule\u201d   \u2022 Allows nodes to reach agreement on which history is the right one   Heaviest Chain Rule: The version of the chain with the highest number of accumulated validator votes weighted by their staked balances</li> <li>\u4ee5\u592a\u574a\u7528\u7684\u662f\u6700\u80a5\u94fe\u6761\u6cd5\u5219<ul> <li>\u62e5\u6709\u6700\u591avalidator\u7684\u94fe\u6761\u4f1a\u88ab\u9009\u62e9</li> </ul> </li> </ul> </li> </ul> </li> <li>Ether<ul> <li>Ether: \u539f\u59cb\u57fa\u7840\u8d27\u5e01   \u2022 Ether is the native cryptocurrency of Ethereum   \u2022 Balances for each account are stored directly as part of the world state    \u2022 Ether is used to pay transaction fees (gas)</li> <li>Tokens: \u7531\u667a\u80fd\u5408\u7ea6\u751f\u6210\u7684\u4ee3\u5e01   \u2022 Tokens are implemented in smart contracts   \u2022 Balances are therefore stored in smart contracts rather than directly   \u2022 You will come across several standard interfaces for such tokens (ERC20 etc.)   \u2022 Anyone can create tokens on Ethereum</li> </ul> </li> <li>Ethereum Virtual Machine (EVM)<ul> <li>\u5982\u4f55\u5728p2p\u7f51\u7edc\u4e2d\u786e\u5b9a\u5730\u8dd1\u4ee3\u7801</li> <li>Objectives of the EVM   \u2022 Deterministic execution   \u2022 Verifiability of the execution and its outcome   \u2022 Atomicity of the execution (transactions are atomic)</li> <li></li> <li>EVM \u7b80\u4ecb   collapsed:: true<ul> <li>Stack-based virtual machine</li> <li>low-level, only jumps, no types</li> <li>ADD, SUB, PUSH, POP</li> <li>Has ephemeral and   permanent storage</li> <li>32bytes</li> <li>nodes \u4f1a reren the code executed by the proposer of the next block and verify that the outcome is correct   The code for execution is found in storage and its validity can be verified using the code hash of the contract account that is being called; code \u7528hash\u6765\u9a8c\u8bc1\u5bf9\u4e0d\u5bf9</li> <li>\u901a\u5e38\u7528Solidity\u7f16\u5199, \u88ab\u7f16\u8bd1\u6210\u5b57\u8282\u7801, \u7136\u540e\u7528transaction deploy, \u5e76\u4e14\u4fdd\u5b58\u5728\u5404\u4e2anodes\u4e2d (An EOA can issue a special transaction that contains the contract code to create a new contract address). CA\u4f1a\u5b58\u50a8\u8d77\u6765\u5b57\u8282\u7801\u548chash, \u4e00\u65e6\u8fd9\u4e2atransaction processed, contract is deployed</li> <li>Executing code<ul> <li>An Ethereum client (node) chooses to include a transaction that makes a call to a contract account \u5ba2\u6237\u7aefinclude\u4e00\u4e2atransaction</li> <li>After verifying the validity of the transaction, it executes the code locally using its EVM implementation \u5ba2\u6237\u7aef\u9a8c\u8bc1, \u7136\u540e\u672c\u5730\u6267\u884ccode</li> <li>This results in a state change that is broadcast to the network in the next block \u72b6\u6001\u8f6c\u79fb\u540e\u5e7f\u64ad\u5230\u7f51\u7edc\u91cc</li> </ul> </li> <li>Charge for compute resource (Gas)<ul> <li>why need gas?<ul> <li>\u2022 Prevent attacks on Ethereum (DoS)   \u2022 Prevent execution of infinite loops in smart contract code    \u2022 Reward validators/miners</li> </ul> </li> <li>Gas Mechanism and Block Reward<ul> <li>\u9700\u8981\u5956\u52b1\u8ba1\u7b97, \u63d0\u4ea4transaction base fee \u4f1a\u88ab\u82b1\u6389, \u8fd8\u9700\u8981\u7ed9\u522b\u4ebapriority fee, \u5f53\u7136\u4e5f\u53ef\u4ee5\u4e0d\u7ed9, \u4f46\u662f\u53ef\u80fd\u522b\u4eba\u5c31\u5f88\u96be\u4ecepool\u91cc\u628a\u4f60\u6316\u51fa\u6765\u4e86, \u56e0\u4e3a\u4f60\u4e0d\u7ed9\u4eba\u5bb6\u5956\u52b1</li> <li></li> </ul> </li> </ul> </li> <li>Submitting Transactions - Mempool<ul> <li>Node\u6536\u5230transaction\u540epending\u5728mem pool\u4e2d, \u7b49\u5f85\u5904\u7406</li> <li></li> </ul> </li> </ul> </li> <li>Smart Contracts<ul> <li>\u4e0dsmart, \u4e5f\u4e0d\u662fcontracts, \u77e5\u8bc6\u4e00\u5806\u90e8\u7f72\u5728\u533a\u5757\u94fe\u7684\u7a0b\u5e8f\u800c\u5df2</li> <li>Functions in smart contracts are called via transactions</li> <li></li> <li>They can:   \u2022 Perform almost any computation (Turing complete)    \u2022 Persist data   \u2022 Transfer money to other addresses or contracts   They cannot:   \u2022 Interact with anything outside of the blockchain: they are isolated (otherwise wouldn\u2019t be deterministic)   \u2022 Be scheduled to do something periodically</li> </ul> </li> <li>Solidity<ul> <li>Strongly typed, fairly simple type system   \u2022 Looks vaguely like Javascript except it is statically typed like Java, C, Rust   \u2022 Supports multiple inheritance   \u2022 Solidity: just one example of a high-level programming language, compiles down into EVM bytecode. Any high level language that can compile down into EVM bytecode would work</li> <li>LATER \u8865\u5145Solidity\u7684\u4e1c\u897f</li> </ul> </li> <li>Development flow for smart contracts<ul> <li> <ol> <li>Write high-level code</li> </ol> </li> <li>Test the code (using testing suite of choice, e.g. Hardhat, Brownie, Foundry) </li> <li>Optimise the code for gas e\u0000ciency</li> <li>Compile the contract into Bytecode</li> <li>Send a transaction to deploy the contract</li> <li>Interact with the contract by sending transactions to the generated address</li> </ul> </li> <li>Inter-contract Communication<ul> <li>contract \u95f4\u7684\u901a\u4fe1\u7528\u7684\u662fmessage calls</li> <li>Message calls are the mechanism for inter contract communication   \u2022 Smart contracts are a bit like classes. Deploying a smart contract is a bit like creating an instance of that class   \u2022 One contract can call another via a message call \u2022 Every transaction wrapped in a message call</li> </ul> </li> <li>\u5371\u9669<ul> <li>code is law - if the code allows it, it can be done</li> <li>Therefore, if there is a bug and the code allows unintended behaviour,   programming intent can separate from reality</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>Week4 Smart Contract development   collapsed:: true<ul> <li>Development tools<ul> <li>we use CLI commands with Foundry</li> </ul> </li> <li>ERC-20 tokens: standard for fungible tokens, \u6bd4\u5982ether, \u4e24\u4e2a\u53ef\u4ee5\u662finterchangeable, \u8054\u901a\u7684, \u53ef\u4ee5\u662f\u8868\u793a\u4e00\u6837\u7684</li> <li>ERC-721: standard for non-fungible tokens, \u8868\u793aunique\u7684things, not interchangeable, \u6bd4\u5982NFT, \u5c31\u662f\u4e00\u79cd\u8868\u793a\u6570\u5b57\u8d44\u4ea7\u7684\u552f\u4e00\u8bc1\u660e</li> </ul> </li> <li>Week5 Oracle, DeFi<ul> <li>centralised oracles are points of failure</li> <li>\u4ece\u5f88\u591aexchange \u6e90\u4e2d\u83b7\u5f97\u5e76aggregate, \u4fdd\u8bc1\u4e0d\u53d7\u5355\u4e00exchange dominant</li> <li>\u9884\u8a00\u673a, \u5c31\u662f\u7528\u6765\u7ed9\u81ea\u6210\u4f53\u7cfb\u7684\u533a\u5757\u94fe\u63d0\u4f9b\u5916\u90e8\u4fe1\u606f\u7684, \u4f8b\u5982\u5916\u90e8\u7684\u8d44\u91d1\u4fe1\u606f\u7b49\u7b49</li> <li>\u5728ETH\u6d4b\u8bd5\u7f51\u7edc\u4e0a\u9762\u5b9e\u9a8c\u4e86\u4eceChainlink VRF \u83b7\u53d6\u53ef\u9a8c\u8bc1\u7684\u968f\u673a\u6570</li> <li>Decentralised Finance ([[DeFi]])   collapsed:: true<ul> <li>Decentralized Finance (DeFi): a peer-to-peer powered financial system.<ul> <li></li> </ul> </li> <li>Inter-Contract Communication<ul> <li>smart contract support DeFi needs support inter-contract communication<ul> <li>\u2022 be expressive enough to encode financial protocol rules   \u2022 allow conditional execution and bounded iteration   \u2022 feature atomic transactions so that no execution can result in an invalid state</li> </ul> </li> </ul> </li> <li>Application Binary Interface (ABI)</li> <li>Composability: Snapping Protocols Together Like Lego<ul> <li>Composability I: New Contract Instances</li> <li>Composability II: Known Interface and Existing Instance</li> <li>Composability III: Existing Instances and Low Level Calls</li> </ul> </li> <li>Properties of DeFi:<ul> <li>Non-custodial:   participants have full control over their funds at any point in time   \u603b\u662f\u6709\u63a7\u5236\u6743</li> <li>Permissionless:   anyone can interface with financial services without being censored or blocked by a third party   \u4e0d\u4f1a\u88ab\u7b2c\u4e09\u65b9\u51bb\u7ed3</li> <li>Openly auditable:   anyone can audit the state of the system   \u6bcf\u4e2a\u4eba\u90fd\u53ef\u4ee5audit, check myself</li> <li>Composable:   the financial services can be arbitrarily composed such that new financial products and services can be created   \u53ef\u4ee5\u4efb\u610fcompose \u4ea7\u54c1</li> </ul> </li> <li>Point of DeFi<ul> <li>offering a new financial architecture that is non-custodial, permissionless, openly auditable, (pseudo)anonymous, and with potentially new capital e\u0000ciencies.</li> <li>generalizes the promise at the heart of the original Bitcoin whitepaper, extending the innovation of non-custodial transactions to complex financial operations.</li> </ul> </li> <li>DeFi Primitives<ul> <li>Keepers, external agents who can trigger state updates.   \u7528\u6765\u53d1\u52a8state \u66f4\u65b0\u7684\u4eba, \u56e0\u4e3a\u53ea\u6709\u975econtract\u8d26\u6237\u624d\u53ef\u4ee5\u53d1\u8d77transactions, \u624d\u53ef\u4ee5\u66f4\u65b0\u72b6\u6001</li> <li>Oracles, a mechanism for importing o\u21b5-chain data into the blockchain virtual machine   \u7528\u6765\u83b7\u53d6\u5916\u90e8\u4fe1\u606f\u52a0\u8f7d\u5230\u94fe\u5185\u7684\u9884\u8a00\u673a</li> <li>Governance, the process through which an on-chain system is able to change the terms of interaction   \u5982\u4f55make decisions, \u5982\u4f55\u7ba1\u7406contract, \u51b3\u5b9a\u5982\u4f55update, contract. \u56e0\u4e3a\u8fd9\u4e9bcontract\u672c\u8eab\u90fd\u662fdistributed \u6267\u884c, \u4f46\u662f\u62e5\u6709\u8005\u63a7\u5236\u8005\u662f\u90a3\u4e2aowner</li> </ul> </li> <li>Composability (\u8fd8\u6ca1\u5199\u5b8c)   SCHEDULED: &lt;2023-02-14 Tue&gt;<ul> <li>Protocols for Loanable Funds (PLFs)<ul> <li>\u7528\u6765\u6361\u94b1\u7684\u4e00\u79cd\u4e1c\u897f, \u53ef\u4ee5\u62b5\u62bc\u8d44\u4ea7, \u6839\u636e\u5b9e\u65f6\u7684\u6c47\u7387\u7b97\u5f97\u503a\u7684\u5065\u5eb7\u5ea6, \u5065\u5eb7\u5ea6\u4e0d\u884c\u5c31\u81ea\u52a8\u6267\u884c\u6e05\u7b97\u7a0b\u5e8f, \u53ef\u4ee5\u6709\u6e05\u7b97\u4eba\u6765\u83b7\u5f97\u62b5\u62bc\u8d44\u4ea7</li> </ul> </li> <li>Flash loans<ul> <li>\u4ec5\u53d1\u751f\u5728\u51e0\u5206\u949f\u51e0\u79d2\u949f\u7684\u501f\u6b3e, \u4ec5\u5728\u4e00\u7b14transaction\u4e2d\u53d1\u751f. \u6ca1\u6709\u62b5\u62bc, \u53ea\u6709\u7b97\u597d\u7684\u5229\u606f, \u7528\u5b8c\u5c31\u5f97\u8fd8</li> </ul> </li> <li>Yield Aggregators<ul> <li>\u6839\u636e\u4f9b\u9700\u5173\u7cfb, \u51b3\u5b9a\u6700\u4f73\u7684\u501f\u6b3e\u4eba\u548c\u653e\u8d37\u4eba\u7684\u5229\u606f</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>Week6</li> <li>Week7</li> <li>Week8</li> </ul>"},{"location":"crypto/Principles%20of%20Distributed%20Ledgers/#-include-valid-mine-minerreward","title":"- \u6700\u957f\u7684\u4f1a\u88ab\u8ba4\u4e3a\u662f\u6700\u9760\u8c31\u7684, \u624d\u4f1a\u88abinclude\u8fdb\u53bb, \u5982\u679c\u8fd9\u6b21\u4e0d\u591f\u957f, \u5c31\u4e0d\u4f1a\u88ab\u8ba4\u4e3avalid, \u4f1a\u5728\u6c60\u4e2d\u7b49\u5f85\u4e0b\u6b21, \u5982\u679cmine\u4e86\u4e00\u4e2a\u6ca1\u7528\u7684, miner\u6ca1\u529e\u6cd5\u5f97\u5230reward, \u5927\u591a\u6570\u90fd\u6ca1\u6709","text":""},{"location":"crypto/Principles%20of%20Distributed%20Ledgers/#tutorial","title":"Tutorial","text":"<ul> <li>Week 4 Smart Contract Development<ul> <li>[[Solidity]]</li> </ul> </li> </ul>"},{"location":"crypto/Principles%20of%20Distributed%20Ledgers/#info","title":"Info","text":"<ul> <li>8:2</li> <li>CW: Week 6</li> <li>Exam: 2H</li> </ul>"},{"location":"crypto/Principles%20of%20Distributed%20Ledgers/#syllabus","title":"Syllabus","text":"<ul> <li>Hash Functions</li> <li>Digital Signature</li> <li>Decentralisation and permissionless/ permissioned ledgers</li> <li>Wallets and transactions</li> <li>Authenticated datastructures</li> <li>Blocks and the blockchain</li> <li>Proof of work and mining</li> <li>Ethereum smart contracts</li> <li>Smart contract security</li> <li>Network layer propagation</li> <li>Blockchain security and privacy</li> <li>Building decentralised applications</li> <li>Security and privacy of distributed ledgers</li> <li>Scaling decentralised ledger and alternatives</li> <li>Network and Hardware aspects of decentralised ledgers</li> </ul>"},{"location":"crypto/Principles%20of%20Distributed%20Ledgers/#links","title":"Links","text":"<ul> <li>Scientia</li> <li>Solidity by Example \u2014 Solidity 0.8.17 documentation</li> <li>Solidity by Example</li> <li> </li> </ul>"},{"location":"crypto/Principles%20of%20Distributed%20Ledgers/#github-smartcontractkithardhat-starter-kit-a-repo-for-boilerplate-code-for-testing-deploying-and-shipping-chainlink-solidity-code","title":"GitHub - smartcontractkit/hardhat-starter-kit: A repo for boilerplate code for testing, deploying, and shipping chainlink solidity code.","text":""},{"location":"game/","title":"Game","text":""},{"location":"linux/","title":"Linux","text":""},{"location":"linux/AstroNvim/","title":"AstroNvim","text":"<p>tags:: Terminal, Tool, Vim, Config, neovim, nvim</p>"},{"location":"linux/AstroNvim/#_1","title":"\u5b89\u88c5","text":"<ul> <li>```bash   brew install neovim</li> </ul> <p>mv ~/.config/nvim ~/.config/nvim.bak   mv ~/.local/share/nvim ~/.local/share/nvim.bak</p> <p>git clone --depth 1 GitHub - AstroNvim/AstroNvim: AstroNvim is an aesthetic and feature-rich neovim config that is extensible and easy to use with a great set of plugins ~/.config/nvim   nvim   ``` - https://github.com/neovim/neovim - GitHub - AstroNvim/AstroNvim: AstroNvim is an aesthetic and feature-rich neovim config that is extensible and easy to use with a great set of plugins #GitHub #\u79d1\u6280\u8d44\u6e90 - GitHub - AstroNvim/astrocommunity: A community repository of common plugin specifications - https://github.com/AstroNvim/user_example - Vim Cheat Sheet #[[Cheat Sheet]] #\u79d1\u6280\u8d44\u6e90 #Vim - \u3010\u5168\u7a0b\u8bb2\u89e3\u3011Neovim\u4ece\u96f6\u914d\u7f6e\u6210\u5c5e\u4e8e\u4f60\u7684\u4e2a\u4eba\u7f16\u8f91\u5668_\u54d4\u54e9\u54d4\u54e9_bilibili - Custom Plugins | AstroNvim - https://github.com/AstroNvim/astrocommunity/tree/feat/nvim-ts-autotag - -</p>"},{"location":"linux/Starship/","title":"Starship","text":"<p>tags::  Terminal, Tool, Theme</p> <ul> <li>Installation #\u5b89\u88c5<ul> <li><code>brew tap homebrew/cask-fonts   brew install --cask font-caskaydia-cove-nerd-font   \u8bbe\u7f6e\u597d\u5b57\u4f53   brew install starship   \u5728.zshrc\u4e2d\u52a0\u5165init\u6307\u4ee4   eval \"$(starship init zsh)\"   mkdir -p ~/.config &amp;&amp; touch ~/.config/starship.toml   \u5c06\u4e0b\u9762\u7684\u6587\u4ef6\u653e\u5165\u8be5\u76ee\u5f55</code> -</li> </ul> </li> <li>References<ul> <li>Starship: Cross-Shell Prompt</li> <li>starship.toml     - -</li> </ul> </li> </ul>"},{"location":"linux/Tmux/","title":"Tmux","text":"<p>tags::  \u547d\u4ee4\u884c, unix, Mac, Tool</p> <ul> <li>\uff081\uff09\u5b83\u5141\u8bb8\u5728\u5355\u4e2a\u7a97\u53e3\u4e2d\uff0c\u540c\u65f6\u8bbf\u95ee\u591a\u4e2a\u4f1a\u8bdd\u3002\u8fd9\u5bf9\u4e8e\u540c\u65f6\u8fd0\u884c\u591a\u4e2a\u547d\u4ee4\u884c\u7a0b\u5e8f\u5f88\u6709\u7528\u3002   \uff082\uff09 \u5b83\u53ef\u4ee5\u8ba9\u65b0\u7a97\u53e3\"\u63a5\u5165\"\u5df2\u7ecf\u5b58\u5728\u7684\u4f1a\u8bdd\u3002   \uff083\uff09\u5b83\u5141\u8bb8\u6bcf\u4e2a\u4f1a\u8bdd\u6709\u591a\u4e2a\u8fde\u63a5\u7a97\u53e3\uff0c\u56e0\u6b64\u53ef\u4ee5\u591a\u4eba\u5b9e\u65f6\u5171\u4eab\u4f1a\u8bdd\u3002   \uff084\uff09\u5b83\u8fd8\u652f\u6301\u7a97\u53e3\u4efb\u610f\u7684\u5782\u76f4\u548c\u6c34\u5e73\u62c6\u5206\u3002</li> <li>Manual #manual<ul> <li>Tmux \u4f7f\u7528\u6559\u7a0b - \u962e\u4e00\u5cf0\u7684\u7f51\u7edc\u65e5\u5fd7</li> <li>\u9700\u8981\u8bbe\u7f6emouse on \u6765\u5f00\u542f\u6eda\u52a8<ul> <li>echo \"set -g mouse on\" &gt;&gt; ~/.tmux.conf</li> <li>set -g mouse on</li> <li>how do i scroll<ul> <li>https://superuser.com/questions/209437/how-do-i-scroll-in-tmux</li> </ul> </li> </ul> </li> <li>\u7981\u6b62\u81ea\u542f\u52a8<ul> <li>touch ~/.no_auto_tmux</li> </ul> </li> <li><code>tmux new -s &lt;session-name&gt;   tmux attach -t 0   tmux kill-session -t 0   tmux switch -t 0   tmux detach</code></li> </ul> </li> <li>References<ul> <li>GitHub - tmux/tmux: tmux source code</li> <li> </li> </ul> </li> </ul>"},{"location":"linux/Tmux/#tmux-cheat-sheet-quick-reference","title":"Tmux Cheat Sheet &amp; Quick Reference","text":""},{"location":"linux/Tmux/#-","title":"-","text":""},{"location":"linux/Vim/","title":"Vim","text":"<p>tags:: \u547d\u4ee4\u884c, Unix, Linux</p> <p>- - Vim\u57fa\u672c\u64cd\u4f5c #card #Doc #[[Cheat Sheet]]     - <code>nvim filename       :sav[eas] file - save file as       u - undo       U - restore last undo       Ctrl r - redo       . - repeat last command       i - insert       d - delete marked text       dd - delete the line       ~ - switch case       u - lower case       U - upper case       :w - save       :w !sudo tee % - sudo save       :wq - save and quit       :q - quit       :q! - quit and ignore unsaved changes       :wqa - save and quit on all tabs</code></p>"},{"location":"linux/git/","title":"Git","text":"<p>Reference: Learn Git Branching</p>"},{"location":"linux/git/#_1","title":"\u57fa\u672c\u64cd\u4f5c","text":"<pre><code># \u67e5\u770b\u4ed3\u5e93\ngit remote -v\n\n# \u66f4\u6539\u8fdc\u7a0burl\ngit remote set-url origin git@github.com:csl122/anything.git\n</code></pre>"},{"location":"linux/git/#add-and-commit","title":"add and commit","text":"<pre><code>git add &lt;file&gt;: \u5c06\u6307\u5b9a\u7684\u6587\u4ef6\u6dfb\u52a0\u5230\u6682\u5b58\u533a\u3002\u4f8b\u5982\uff0cgit add readme.txt\ngit add . : \u5c06\u5f53\u524d\u76ee\u5f55\u4e0b\u7684\u6240\u6709\u6587\u4ef6\u6dfb\u52a0\u5230\u6682\u5b58\u533a\uff0c\u5305\u62ec\u65b0\u5efa\u7684\u548c\u4fee\u6539\u8fc7\u7684\u6587\u4ef6\ngit add -u : \u5c06\u5df2\u4fee\u6539\u548c\u5df2\u5220\u9664\u7684\u6587\u4ef6\u6dfb\u52a0\u5230\u6682\u5b58\u533a\uff0c\u4e0d\u5305\u62ec\u65b0\u5efa\u7684\u6587\u4ef6\ngit add -A : \u5c06\u6240\u6709\u7684\u6539\u52a8\u6587\u4ef6\u6dfb\u52a0\u5230\u6682\u5b58\u533a\uff0c\u5305\u62ec\u65b0\u5efa\u7684\u3001\u4fee\u6539\u8fc7\u7684\u548c\u5df2\u5220\u9664\u7684\u6587\u4ef6\ngit add --interactive : \u7c7b\u4f3c\u4e8e--patch\u9009\u9879\uff0c\u4f46\u662f\u4ee5\u4ea4\u4e92\u5f0f\u65b9\u5f0f\u663e\u793a\u6587\u4ef6\u7684\u6539\u52a8\uff0c\u53ef\u4ee5\u9009\u62e9\u6027\u5730\u6dfb\u52a0\u5230\u6682\u5b58\u533a\n\ngit commit -m \"commit message\": \u63d0\u4ea4\u6682\u5b58\u533a\u7684\u6539\u52a8\u5e76\u6dfb\u52a0\u63d0\u4ea4\u63cf\u8ff0\u4fe1\u606f\ngit commit -a: \u5c06\u6240\u6709\u6539\u52a8\u8fc7\u7684\u6587\u4ef6\u6dfb\u52a0\u5230\u6682\u5b58\u533a\uff0c\u5e76\u63d0\u4ea4\u5230\u672c\u5730\u4ed3\u5e93\u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4f7f\u7528\u8be5\u547d\u4ee4\u65f6\uff0c\u65b0\u6dfb\u52a0\u7684\u6587\u4ef6\u4e0d\u4f1a\u88ab\u63d0\u4ea4\u3002\ngit commit --amend: \u4fee\u6539\u6700\u8fd1\u4e00\u6b21\u7684\u63d0\u4ea4\u3002\u5982\u679c\u4f60\u53d1\u73b0\u521a\u521a\u7684\u63d0\u4ea4\u6709\u9519\u8bef\u6216\u9057\u6f0f\u4e86\u4e00\u4e9b\u6539\u52a8\uff0c\u53ef\u4ee5\u4f7f\u7528\u8be5\u547d\u4ee4\u8fdb\u884c\u4fee\u6539\u3002\n</code></pre>"},{"location":"linux/git/#branch","title":"branch","text":"<pre><code>git branch &lt;branch_name&gt;: \u521b\u5efa\u5206\u652f\uff0c\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a &lt;branch_name&gt; \u7684\u65b0\u5206\u652f\u3002\ngit checkout &lt;branch_name&gt;: \u5207\u6362\u5206\u652f\uff0c\u5207\u6362\u5230 &lt;branch_name&gt; \u5206\u652f\u3002\ngit switch &lt;branch_name&gt;: \u5207\u6362\u5206\u652f\uff0c\u5207\u6362\u5230 &lt;branch_name&gt; \u5206\u652f\u3002\ngit checkout -b &lt;branch_name&gt;: \u521b\u5efa\u5e76\u5207\u6362\u5230\u5206\u652f\uff0c\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a &lt;branch_name&gt; \u7684\u65b0\u5206\u652f\uff0c\u5e76\u5207\u6362\u5230\u8be5\u5206\u652f\u3002\ngit branch: \u67e5\u770b\u6240\u6709\u5206\u652f\uff0c\u663e\u793a\u5f53\u524d\u4ed3\u5e93\u4e2d\u6240\u6709\u7684\u5206\u652f\u3002\ngit branch -f &lt;branch_name&gt; &lt;commit_id&gt;: \u5f3a\u5236\u79fb\u52a8\u5206\u652f\u7684\u6307\u5411\u4f4d\u7f6e\uff0c\u5c06 &lt;branch_name&gt; \u5206\u652f\u79fb\u52a8\u5230\u6307\u5b9a\u7684 &lt;commit_id&gt; \u5904\u3002\ngit merge &lt;branch_name&gt;: \u5408\u5e76\u5206\u652f\uff0c\u5c06 &lt;branch_name&gt; \u5206\u652f\u5408\u5e76\u5230\u5f53\u524d\u5206\u652f\u3002\ngit branch -d &lt;branch_name&gt;: \u5220\u9664\u5206\u652f\uff0c\u5220\u9664\u540d\u4e3a &lt;branch_name&gt; \u7684\u5206\u652f\u3002\ngit branch -D &lt;branch_name&gt;: \u5f3a\u5236\u5220\u9664\u5206\u652f\uff0c\u5f3a\u5236\u5220\u9664\u540d\u4e3a &lt;branch_name&gt; \u7684\u5206\u652f\u3002\ngit branch -m &lt;new_branch_name&gt;: \u91cd\u547d\u540d\u5206\u652f\uff0c\u5c06\u5f53\u524d\u5206\u652f\u91cd\u547d\u540d\u4e3a &lt;new_branch_name&gt;\u3002\ngit branch -r: \u67e5\u770b\u8fdc\u7a0b\u5206\u652f\uff0c\u663e\u793a\u4e0e\u5f53\u524d\u4ed3\u5e93\u5173\u8054\u7684\u8fdc\u7a0b\u5206\u652f\u3002\ngit branch -a: \u67e5\u770b\u6240\u6709\u5206\u652f\uff0c\u663e\u793a\u5f53\u524d\u4ed3\u5e93\u4e2d\u7684\u6240\u6709\u5206\u652f\uff0c\u5305\u62ec\u672c\u5730\u548c\u8fdc\u7a0b\u5206\u652f\u3002\ngit push origin &lt;branch_name&gt;: \u5c06\u672c\u5730\u5206\u652f\u63a8\u9001\u5230\u8fdc\u7a0b\uff0c\u5c06\u5f53\u524d\u5206\u652f\u63a8\u9001\u5230\u540d\u4e3a origin \u7684\u8fdc\u7a0b\u4ed3\u5e93\u3002\ngit checkout -b &lt;local_branch_name&gt; origin/&lt;remote_branch_name&gt;: \u83b7\u53d6\u8fdc\u7a0b\u5206\u652f\u5230\u672c\u5730\uff0c\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a &lt;local_branch_name&gt; \u7684\u65b0\u672c\u5730\u5206\u652f\uff0c\u5e76\u5c06\u540d\u4e3a &lt;remote_branch_name&gt; \u7684\u8fdc\u7a0b\u5206\u652f\u4e0e\u4e4b\u5173\u8054\u3002\n</code></pre>"},{"location":"linux/git/#merge","title":"merge","text":"<pre><code># currently in main branch\ngit merge bugFix : \u5728main\u7684\u6700\u65b0\u8282\u70b9\u4e0b\u9762\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u8282\u70b9\u6709\u4e24\u4e2abranch\u7684\u6700\u65b0\u5185\u5bb9\n</code></pre>"},{"location":"linux/git/#rebase","title":"rebase","text":"<pre><code>git rebase &lt;branch&gt;: \u5c06\u5f53\u524d\u5206\u652f\u4ee5 &lt;branch&gt; \u4e3a\u57fa\u7840\u8fdb\u884c\u53d8\u57fa\u3002\u8fd9\u5c06\u628a\u5f53\u524d\u5206\u652f\u4e0a\u7684\u63d0\u4ea4\u79fb\u52a8\u5230 &lt;branch&gt; \u7684\u6700\u65b0\u4f4d\u7f6e\uff0c\u5e76\u66f4\u65b0\u5f53\u524d\u5206\u652f\u7684\u63d0\u4ea4\u5386\u53f2\u3002\ngit rebase -i &lt;commit&gt;: \u4ea4\u4e92\u5f0f\u53d8\u57fa\uff0c\u5141\u8bb8\u7528\u6237\u7f16\u8f91\u63d0\u4ea4\u5386\u53f2\u3002\u4f7f\u7528\u6b64\u547d\u4ee4\u65f6\uff0cgit\u4f1a\u6253\u5f00\u4e00\u4e2a\u6587\u672c\u7f16\u8f91\u5668\uff0c\u663e\u793a\u5f53\u524d\u5206\u652f\u7684\u63d0\u4ea4\u5386\u53f2\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u91cd\u65b0\u6392\u5e8f\u3001\u5220\u9664\u6216\u5408\u5e76\u63d0\u4ea4\n</code></pre>"},{"location":"linux/git/#head","title":"HEAD","text":"<p>\u5982\u679c\u60f3\u770b HEAD \u6307\u5411\uff0c\u53ef\u4ee5\u901a\u8fc7 cat .git/HEAD \u67e5\u770b \u5982\u679c HEAD \u6307\u5411\u7684\u662f\u4e00\u4e2a\u5f15\u7528\uff0c\u8fd8\u53ef\u4ee5\u7528 git symbolic-ref HEAD \u67e5\u770b\u5b83\u7684\u6307\u5411\u3002 \u5f53\u6211\u4eeccheckout\u4e00\u4e2a\u5206\u652f\u540d\u7684\u65f6\u5019\u5176\u5b9e\u662f:  HEAD -&gt; main -&gt; C1 HEAD \u6307\u5411 main\uff0c main \u6307\u5411 C1 \u800c\u5f53\u6211\u4eeccheckout\u4e00\u4e2a\u8282\u70b9\u7684\u65f6\u5019: git checkout C1 HEAD -&gt; C1     HEAD \u6307\u5411 C1, main\u4e5f\u6307\u5411 C1</p> <pre><code>git checkout &lt;commit_hash&gt;: \u5c06HEAD\u6307\u5411\u5bf9\u5e94\u7684commit hash\u8282\u70b9\n</code></pre>"},{"location":"linux/git/#_2","title":"\u76f8\u5bf9\u5f15\u7528 ~ \u548c ^","text":"<pre><code>1. ^ \u7b26\u53f7\uff1a\u5411\u4e0a\u79fb\u52a8\u4e00\u4e2a\u63d0\u4ea4\n    - commit^\uff1a\u8868\u793a\u7ed9\u5b9a\u63d0\u4ea4\u7684\u7236\u63d0\u4ea4\u3002\u4f8b\u5982\uff0cHEAD^\u8868\u793a\u5f53\u524d\u63d0\u4ea4\u7684\u7236\u63d0\u4ea4\u3002\n    - commit^n\uff1a\u8868\u793a\u7ed9\u5b9a\u63d0\u4ea4\u7684\u7b2cn\u4e2a\u7236\u63d0\u4ea4\u3002\u4f8b\u5982\uff0cHEAD^2\u8868\u793a\u5f53\u524d\u63d0\u4ea4\u7684\u7b2c\u4e8c\u4e2a\u7236\u63d0\u4ea4\uff08\u5bf9\u4e8e\u5408\u5e76\u63d0\u4ea4\uff09\u3002\n\n2. ~ \u7b26\u53f7\uff1a\u5411\u4e0a\u79fb\u52a8\u591a\u4e2a\u63d0\u4ea4\n    - commitn\uff1a\u8868\u793a\u7ed9\u5b9a\u63d0\u4ea4\u7684n\u4ee3\u7236\u63d0\u4ea4\u3002\u4f8b\u5982\uff0cHEAD~3\u8868\u793a\u5f53\u524d\u63d0\u4ea4\u7684\u7b2c\u4e09\u4ee3\u7236\u63d0\u4ea4\u3002\n\n\u8fd9\u4e9b\u7b26\u53f7\u53ef\u4ee5\u4e0e\u5404\u79cdgit\u547d\u4ee4\u548c\u5f15\u7528\u4e00\u8d77\u4f7f\u7528\uff0c\u4f8b\u5982\u5206\u652f\u540d\u3001\u6807\u7b7e\u540d\u3001commit\u54c8\u5e0c\u6216\u5f15\u7528\u64cd\u4f5c\u7b26\uff08\u4f8b\u5982HEAD\uff09\u3002\n\n\u4ee5\u4e0b\u662f\u4e00\u4e9b\u793a\u4f8b\uff1a\n\n3. HEAD^: \u8868\u793a\u5f53\u524d\u63d0\u4ea4\u7684\u7236\u63d0\u4ea4\u3002\n4. HEAD^^\uff1a\u8868\u793a\u5f53\u524d\u63d0\u4ea4\u7684\u7236\u63d0\u4ea4\u7684\u7236\u63d0\u4ea4\u3002\n5. HEAD~3\uff1a\u8868\u793a\u5f53\u524d\u63d0\u4ea4\u7684\u7b2c\u4e09\u4ee3\u7236\u63d0\u4ea4\u3002\n6. branch_name^\uff1a\u8868\u793a\u5206\u652f\u540d\u79f0\u5bf9\u5e94\u63d0\u4ea4\u7684\u7236\u63d0\u4ea4\u3002\n7. tag_name~2\uff1a\u8868\u793a\u6807\u7b7e\u540d\u79f0\u5bf9\u5e94\u63d0\u4ea4\u7684\u7b2c\u4e8c\u4ee3\u7236\u63d0\u4ea4\u3002\n\u5982\u679c\u662fmerge\u540e\u7684\u7ed3\u679c\u6709\u4e24\u4e2aparent, \u53ef\u4ee5\u901a\u8fc7^\u6765\u7ed9\u5b9a\u54ea\u4e00\u4e2aparent\n8. commit_hash^2\uff1a\u8868\u793a\u7ed9\u5b9a\u63d0\u4ea4\u54c8\u5e0c\u7684\u7b2c\u4e8c\u4e2a\u7236\u63d0\u4ea4\u3002\n</code></pre>"},{"location":"linux/git/#_3","title":"\u64a4\u9500\u53d8\u66f4","text":"<pre><code>git reset \u4e00\u822c\u7528\u6765\u56de\u9000\u672c\u5730\u7684\u4fee\u6539, \u5c06\u5f53\u524d\u5206\u652f\u6307\u9488\u6307\u5411\u4e4b\u524d, \u4e4b\u540e\u7684\u66f4\u6539\u4f1a\u4ee5\u672a\u52a0\u5165\u6682\u5b58\u533a\u7684\u5f62\u5f0f\u663e\u793a\ngit revert \u5e38\u7528\u6765\u56de\u9000origin\u4e0a\u7684\u4fee\u6539, \u65b0\u5efa\u4e00\u4e2acommit, \u8be5commit\u4e2d\u662f\u4e4b\u524d\u505a\u8fc7\u7684\u4e8b\u60c5\u9006\u5411\u53d8\u5316\n\ngit reset --soft [commit]\uff1a\u4fdd\u7559\u4e4b\u524d\u7684\u63d0\u4ea4\uff0c\u5e76\u5c06\u4e4b\u540e\u7684\u63d0\u4ea4\u653e\u5165\u6682\u5b58\u533a\uff0c\u6211\u4eec\u53ef\u4ee5\u91cd\u65b0\u4fee\u6539\u5e76\u63d0\u4ea4\u3002\ngit reset --mixed [commit]\uff1a\u4fdd\u7559\u4e4b\u524d\u7684\u63d0\u4ea4\uff0c\u5e76\u5c06\u4e4b\u540e\u7684\u63d0\u4ea4\u653e\u5165\u5de5\u4f5c\u533a\uff0c\u6211\u4eec\u53ef\u4ee5\u91cd\u65b0\u4fee\u6539\uff0c\u5e76\u5c06\u4fee\u6539\u540e\u7684\u5185\u5bb9\u91cd\u65b0\u63d0\u4ea4\u3002\ngit reset --hard [commit]\uff1a\u5c06\u4e4b\u524d\u7684\u63d0\u4ea4\u548c\u4e4b\u540e\u7684\u63d0\u4ea4\u5168\u90e8\u4e22\u5f03\uff0c\u56de\u5230\u6307\u5b9a\u7684\u63d0\u4ea4\u72b6\u6001\uff0c\u6211\u4eec\u4f1a\u4e22\u5931\u4e4b\u524d\u7684\u4fee\u6539\u3002\n\ngit revert HEAD: HEAD\u539f\u672c\u6240\u6307\u5411\u7684C2\u540e\u51fa\u73b0\u4e86\u65b0\u63d0\u4ea4\u8bb0\u5f55 C2\u2019 \u5f15\u5165\u4e86\u66f4\u6539, \u8fd9\u4e9b\u66f4\u6539\u521a\u597d\u662f\u7528\u6765\u64a4\u9500 C2 \u8fd9\u4e2a\u63d0\u4ea4\u7684\u3002\u4e5f\u5c31\u662f\u8bf4 C2\u2019 \u7684\u72b6\u6001\u4e0e C1 \u662f\u76f8\u540c\u7684\u3002\n</code></pre>"},{"location":"linux/git/#cherry-pick","title":"cherry-pick","text":"<pre><code>git cherry-pick \u5141\u8bb8\u4f60\u4ece\u4e00\u4e2a\u5206\u652f\uff08\u6216\u591a\u4e2a\u5206\u652f\uff09\u4e2d\u9009\u62e9\u4e00\u4e2a\u6216\u591a\u4e2a\u63d0\u4ea4\uff0c\u5e76\u5c06\u5b83\u4eec\u5e94\u7528\u4e8e\u5f53\u524d\u5206\u652f\u3002\ngit cherry-pick &lt;commit&gt;\ngit cherry-pick &lt;commit1&gt; &lt;commit3&gt; &lt;commit2&gt; \u6309\u7167132\u7684\u987a\u5e8f\u63a5\u5230\u5f53\u524dbranch\u540e\u9762\ngit cherry-pick &lt;start-commit&gt;..&lt;end-commit&gt; \u5408\u5e76\u4e00\u4e2a\u8303\u56f4\u7684\u63d0\u4ea4\n\u6ce8\u610f, \u5373\u4fbf\u7ed9\u7684\u662fbranch name, \u62fc\u63a5\u5230\u540e\u9762\u7684\u4e5f\u662f\u6307\u5411\u7684\u90a3\u4e00\u4e2a\u8282\u70b9\u7684commit\u800c\u5df2\n</code></pre>"},{"location":"linux/git/#rebase_1","title":"\u4ea4\u4e92\u5f0frebase","text":"<pre><code>git rebase -i &lt;commit&gt;: \u4ea4\u4e92\u5f0f\u53d8\u57fa\uff0c\u5141\u8bb8\u7528\u6237\u7f16\u8f91\u63d0\u4ea4\u5386\u53f2\u3002\u4f7f\u7528\u6b64\u547d\u4ee4\u65f6\uff0cgit\u4f1a\u6253\u5f00\u4e00\u4e2a\u6587\u672c\u7f16\u8f91\u5668\uff0c\u663e\u793a\u5f53\u524d\u5206\u652f\u7684\u63d0\u4ea4\u5386\u53f2\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u91cd\u65b0\u6392\u5e8f\u3001\u5220\u9664\u6216\u5408\u5e76\u63d0\u4ea4\ngit rebase -i &lt;base&gt; &lt;branch&gt;\ngit rebase -i HEAD~4\n\u53ef\u4ee5\u81ea\u5df1\u8c03\u6574HEAD\u4e4b\u524d\u7684\u8fd9\u4e9b\u4e1c\u897f, omit\u8c03, \u6216\u8005\u8c03\u6574\u987a\u5e8f, \u90fd\u53ef\u4ee5\n</code></pre>"},{"location":"linux/git/#_4","title":"\u63d0\u4ea4\u6280\u5de7","text":"<pre><code>\u5982\u679c\u6211\u4eec\u5728fix bug\u4e4b\u524d\u6709\u5f88\u591aprint\u7684\u4e1c\u897f, \u4f46\u662f\u6211\u4eec\u6700\u540e\u4e0d\u60f3\u8981\u8fd9\u4e9bprint\u7684commit, \u6211\u4eec\u53ef\u4ee5\u53ea\u83b7\u53d6bugFix \u8fd9\u4e2a\u8282\u70b9\u7684commit\u6765\u653e\u5230main\u91cc\u9762, \u53ef\u4ee5\u7528\u4e24\u79cd\u65b9\u5f0f\ngit rebase -i main bugFix : \u5c06\u4e2d\u95f4\u65e0\u7528\u8282\u70b9\u5220\u53bb, based on main, \u4e0ebugFix\u4e00\u8d77\u751f\u6210\u4e00\u6761\u65b0\u8def\n\ngit checkout main\ngit cherry-pick bugFix : \u4ec5\u5c06bugFix\u653e\u5230main\u540e\u9762\n</code></pre>"},{"location":"linux/git/#tag-and-describe","title":"tag and describe","text":"<pre><code>git tag -a v1.0 -m \"Version 1.0\"\ngit push origin v1.0 : \u63a8\u9001\u6807\u7b7e\u81f3\u8fdc\u7a0b\u4ed3\u5e93\ngit tag -d v1.0 : \u8fd9\u4f1a\u5220\u9664\u540d\u4e3av1.0\u7684\u6807\u7b7e\n\ngit describe &lt;ref&gt;\n&lt;ref&gt; \u53ef\u4ee5\u662f\u4efb\u4f55\u80fd\u88ab Git \u8bc6\u522b\u6210\u63d0\u4ea4\u8bb0\u5f55\u7684\u5f15\u7528\uff0c\u5982\u679c\u4f60\u6ca1\u6709\u6307\u5b9a\u7684\u8bdd\uff0cGit \u4f1a\u4f7f\u7528\u4f60\u76ee\u524d\u6240\u5728\u7684\u4f4d\u7f6e\uff08HEAD\uff09\u3002\n&lt;tag&gt;_&lt;numCommits&gt;_g&lt;hash&gt;\n</code></pre>"},{"location":"linux/git/#fetch","title":"fetch","text":"<pre><code>fetch\u4f1a\u5c06\u8fdc\u7a0b\u6ca1\u6709\u4e0b\u8f7d\u5230\u672c\u5730\u7684commit\u4e0b\u8f7d\u5230\u672c\u5730, \u5e76\u5c06\u8fdc\u7a0b\u7684oigin/main\u6307\u9488\u6307\u5411\u8fdc\u7a0bmain\u7684\u4f4d\u7f6e\n1. \u4ece\u8fdc\u7a0b\u4ed3\u5e93\u4e0b\u8f7d\u672c\u5730\u4ed3\u5e93\u4e2d\u7f3a\u5931\u7684\u63d0\u4ea4\u8bb0\u5f55\n2. \u66f4\u65b0\u8fdc\u7a0b\u5206\u652f\u6307\u9488(\u5982 o/main)\ngit fetch origin foo : \u4ec5\u4e0b\u8f7d\u8fdc\u7a0b\u4ed3\u5e93\u4e2dfoo\u5206\u652f\u4e2d\u7684\u6700\u65b0\u63d0\u4ea4\u8bb0\u5f55\uff0c\u5e76\u66f4\u65b0\u4e86 o/foo\ngit fetch origin source:dest \u5c06\u8fdc\u7a0b\u7684source branch \u62c9\u5230\u672c\u5730\u7684dest branch\u4e0a, \u5982\u679csource\u4e3a\u7a7a, \u672c\u5730\u7684dest\u4f1a\u88ab\u5220\u9664\ngit fetch origin :bugFix : \u5728\u672c\u5730\u65b0\u5efa\u4e00\u4e2abugFix\u5206\u652f\n</code></pre>"},{"location":"linux/git/#pull","title":"pull","text":"<pre><code>git pull\n== git fetch + git merge\ngit pull --rebase\n== git fetch + git rebase origin/main\n</code></pre>"},{"location":"linux/git/#push","title":"push","text":"<pre><code>git push &lt;remote&gt; &lt;place&gt;\ngit push origin main : \u5207\u5230\u672c\u5730\u4ed3\u5e93\u4e2d\u7684\u201cmain\u201d\u5206\u652f\uff0c\u83b7\u53d6\u6240\u6709\u7684\u63d0\u4ea4\uff0c\u518d\u5230\u8fdc\u7a0b\u4ed3\u5e93\u201corigin\u201d\u4e2d\u627e\u5230\u201cmain\u201d\u5206\u652f\uff0c\u5c06\u8fdc\u7a0b\u4ed3\u5e93\u4e2d\u6ca1\u6709\u7684\u63d0\u4ea4\u8bb0\u5f55\u90fd\u6dfb\u52a0\u4e0a\u53bb\uff0c\u641e\u5b9a\u4e4b\u540e\u544a\u8bc9\u6211\n\ngit push origin foo~:main : \u53ef\u4ee5\u5c06 foo^ \u89e3\u6790\u4e3a\u4e00\u4e2a\u4f4d\u7f6e\uff0c\u4e0a\u4f20\u6240\u6709\u672a\u88ab\u5305\u542b\u5230\u8fdc\u7a0b\u4ed3\u5e93\u91cc main \u5206\u652f\u4e2d\u7684\u63d0\u4ea4\u8bb0\u5f55\u3002\ngit push origin main:newBranch : \u53ef\u4ee5\u63a8\u9001\u4e00\u4e2a\u65b0\u7684branch\ngit push origin :side : \u5220\u9664\u8fdc\u7a0b\u4e2d\u7684side\u5206\u652f\n</code></pre>"},{"location":"linux/git/#pull-request","title":"pull request","text":"<ul> <li>\u5982\u679c\u8fdc\u7a0b\u4e0d\u5141\u8bb8\u76f4\u63a5push, \u800c\u9700\u8981pull request\u6765\u66f4\u65b0main\u5206\u652f\u7684\u8bdd</li> <li>\u65b0\u5efa\u4e00\u4e2afeature\u5206\u652f, \u5c06\u672c\u5730\u7684\u4fee\u6539\u653e\u5230\u8fd9\u4e2abranch\u4e2d, \u7136\u540e\u63a8\u9001\u5230\u8fdc\u7a0b</li> <li>\u7136\u540ereset\u672c\u5730\u7684main\u548c\u8fdc\u7a0b\u7684\u4e00\u81f4, \u4ee5\u9632\u6b62\u4e4b\u540epull\u4ec0\u4e48\u7684\u51fa\u73b0\u9ebb\u70e6</li> </ul>"},{"location":"linux/git/#remote-tracking","title":"remote tracking","text":"<ul> <li>\u76f4\u63a5\u4e86\u5f53\u5730\u8bb2\uff0c<code>main</code>\u00a0\u548c\u00a0<code>o/main</code>\u00a0\u7684\u5173\u8054\u5173\u7cfb\u5c31\u662f\u7531\u5206\u652f\u7684\u201cremote tracking\u201d\u5c5e\u6027\u51b3\u5b9a\u7684\u3002<code>main</code>\u00a0\u88ab\u8bbe\u5b9a\u4e3a\u8ddf\u8e2a\u00a0<code>o/main</code>\u00a0\u2014\u2014 \u8fd9\u610f\u5473\u7740\u4e3a\u00a0<code>main</code>\u00a0\u5206\u652f\u6307\u5b9a\u4e86\u63a8\u9001\u7684\u76ee\u7684\u5730\u4ee5\u53ca\u62c9\u53d6\u540e\u5408\u5e76\u7684\u76ee\u6807\u3002</li> <li>\u5f53\u4f60\u514b\u9686\u4ed3\u5e93\u7684\u65f6\u5019, Git \u5c31\u81ea\u52a8\u5e2e\u4f60\u628a\u8fd9\u4e2a\u5c5e\u6027\u8bbe\u7f6e\u597d\u4e86\u3002</li> <li>\u5f53\u4f60\u514b\u9686\u65f6, Git \u4f1a\u4e3a\u8fdc\u7a0b\u4ed3\u5e93\u4e2d\u7684\u6bcf\u4e2a\u5206\u652f\u5728\u672c\u5730\u4ed3\u5e93\u4e2d\u521b\u5efa\u4e00\u4e2a\u8fdc\u7a0b\u5206\u652f\uff08\u6bd4\u5982\u00a0<code>o/main</code>\uff09\u3002\u7136\u540e\u518d\u521b\u5efa\u4e00\u4e2a\u8ddf\u8e2a\u8fdc\u7a0b\u4ed3\u5e93\u4e2d\u6d3b\u52a8\u5206\u652f\u7684\u672c\u5730\u5206\u652f\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u8fd9\u4e2a\u672c\u5730\u5206\u652f\u4f1a\u88ab\u547d\u540d\u4e3a\u00a0<code>main</code>\u3002</li> </ul> <pre><code>git checkout -b totallyNotMain o/main : \u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a totallyNotMain \u7684\u5206\u652f\uff0c\u5b83\u8ddf\u8e2a\u8fdc\u7a0b\u5206\u652f o/main\u3002\ngit branch -u o/main foo : \u8fd9\u6837 foo \u5c31\u4f1a\u8ddf\u8e2a o/main \u4e86\ngit branch -u o/main: \u5982\u679c\u5f53\u524d\u5c31\u5728 foo \u5206\u652f\u4e0a, \u8fd8\u53ef\u4ee5\u7701\u7565 foo\n</code></pre>"},{"location":"mac/","title":"Mac","text":""},{"location":"mac/Mac/","title":"Mac","text":"<p>- - ## locale     - \u8bed\u8a00\u73af\u5883\u7684\u9009\u9879, \u544a\u8bc9\u7a0b\u5e8f\u6211\u7684\u8bed\u8a00\u73af\u5883. #locale #linux #Terminal     - LC_ALL \u4f1a\u8986\u76d6\u6240\u6709\u5b50\u9009\u9879, C\u662fascii, \u6392\u5e8f\u4f1a\u6309\u7167ascii\u6765     - <code>bash       locale       LANG=\"en_GB.UTF-8\"       LC_COLLATE=\"en_GB.UTF-8\"       LC_CTYPE=\"en_GB.UTF-8\"       LC_MESSAGES=\"en_GB.UTF-8\"       LC_MONETARY=\"en_GB.UTF-8\"       LC_NUMERIC=\"en_GB.UTF-8\"       LC_TIME=\"en_GB.UTF-8\"       LC_ALL=\"en_GB.UTF-8\"</code>     - ```bash       .zshrc\u4e2dexport, \u907f\u514d\u9ed8\u8ba4\u503c\u9020\u6210\u5f71\u54cd       export LANG=en_GB.UTF-8       export LC_ALL=en_GB.UTF-8</p> <pre><code>  \u7279\u5b9a\u60c5\u51b5\u4e0b, \u91cd\u65b0\u81ea\u5b9a\u4e49\u73af\u5883\u53d8\u91cf, \u8ba9\u7279\u5b9a\u7a0b\u5e8f\u4f7f\u7528\u7279\u5b9a\u8bed\u8a00\n  alias gic='LC_ALL=zh_CN.UTF-8 git'\n  echo \"alias git='LANG=zh_CN git'\" &gt;&gt; ~/.zshrc\n\n  /etc/ssh/ssh_config \u6587\u4ef6\u6700\u540e\n    Host *\n        SendEnv LANG LC_*\n  \u4f1a\u5c06\u672c\u673a\u7684locale \u4fe1\u606f\u53d1\u9001\u5230\u670d\u52a1\u5668\u540c\u6b65, \u51fa\u95ee\u9898\u8981\u6ce8\u91ca\u6389\n  ```\n- ![3201687274244_.pic.jpg](../assets/3201687274244_.pic_1687293977119_0.jpg)\n</code></pre>"},{"location":"misc/","title":"Miscellaneous","text":""},{"location":"misc/23W26/","title":"23W26","text":""},{"location":"misc/23W26/#chatgpt","title":"\u5feb\u6377\u6307\u4ee4 + ChatGPT + \u65e5\u8bb0 = \u65e0\u75db\u65e5\u8bb0","text":"<ul> <li>\u665a\u4e0a\u771f\u5b9e\u4f53\u9a8c\u4e86\u4f7f\u7528\u8bed\u97f3\u6765\u8bb0\u5f55\u7b14\u8bb0\u65e5\u8bb0, \u77ed\u6682\u4f53\u9a8c\u4e0b\u6765, \u6211\u4f1a\u89c9\u5f97\u662f\u76f8\u5f53\u597d\u7684, \u4fbf\u6377\u6027, \u4e14\u4e0d\u6253\u6270\u4f60\u7684\u5fc3\u6d41, \u6253\u5b57\u771f\u7684\u5f88\u7834\u574f\u611f\u89c9, \u8bed\u97f3\u5c31\u50cf\u662f\u81ea\u5df1\u5728\u8ddf\u81ea\u5df1\u5bf9\u8bdd, \u800c\u4e14\u8fd8\u4e0d\u7528\u62c5\u5fc3\u4f60\u8bf4\u7684\u591a\u5570\u55e6, \u56e0\u4e3a\u6709\u4eba\u4f1a\u5e2e\u4f60\u6574\u7406<ul> <li>\u4e3a\u4ec0\u4e48\u4f60\u5e94\u8be5\u5f00\u59cb\u7528ChatGPT\u5199\u65e5\u8bb0|\u505a\u7b14\u8bb0(Prompt\u548c\u81ea\u52a8\u5316) - YouTube</li> <li>\u89c6\u9891\u4e2d\u7684Prompt\u548ciPhone\u5feb\u6377\u65b9\u5f0f:<ol> <li>\u521b\u5efa\u65e5\u8bb0-\u82f1\u6587\u7248 \u5feb\u6377\u6307\u4ee4</li> <li>\u521b\u5efa\u65e5\u8bb0-\u4e2d\u6587\u7248 \u5feb\u6377\u6307\u4ee4</li> <li>\u5c06GPT-list\u52a0\u5165\u5f85\u529e\u4e8b\u9879 \u5feb\u6377\u6307\u4ee4</li> <li>\u521b\u5efaChatGPT\u8bed\u97f3\u7b14\u8bb0-\u82f1\u6587\u7248 \u5feb\u6377\u6307\u4ee4</li> <li>\u521b\u5efaChatGPT\u8bed\u97f3\u7b14\u8bb0-\u4e2d\u6587\u7248 \u5feb\u6377\u6307\u4ee4</li> </ol> </li> </ul> </li> </ul>"},{"location":"misc/mkdocs/","title":"References for MkDocs","text":""},{"location":"misc/mkdocs/#buttons","title":"Buttons","text":""},{"location":"misc/mkdocs/#default-button","title":"Default Button","text":"<p>Subscribe to our newsletter</p>"},{"location":"misc/mkdocs/#primary-button","title":"Primary Button","text":"<p>Subscribe to our newsletter</p>"},{"location":"misc/mkdocs/#icon-button","title":"Icon Button","text":"<p>Send </p>"},{"location":"misc/mkdocs/#content-tabs","title":"Content Tabs","text":"Unordered listOrdered list <ul> <li>Sed sagittis eleifend rutrum</li> <li>Donec vitae suscipit est</li> <li>Nulla tempor lobortis orci</li> </ul> <ol> <li>Sed sagittis eleifend rutrum</li> <li>Donec vitae suscipit est</li> <li>Nulla tempor lobortis orci</li> </ol>"},{"location":"misc/mkdocs/#admonitions","title":"Admonitions","text":""},{"location":"misc/mkdocs/#note","title":"Note","text":"<p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"misc/mkdocs/#collapsible-blocks","title":"Collapsible blocks","text":"Note <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> Note <p>Adding a + after the ??? token renders the block expanded.</p>"},{"location":"misc/mkdocs/#supported-admonitions","title":"Supported admonitions","text":"<ul> <li><code>note</code></li> <li><code>warning</code></li> <li><code>tip</code></li> <li><code>abstract</code></li> <li><code>info</code></li> <li><code>success</code></li> <li><code>question</code></li> <li><code>failure</code></li> <li><code>danger</code></li> <li><code>bug</code></li> <li><code>example</code></li> <li><code>quote</code></li> </ul>"},{"location":"misc/mkdocs/#code-blocks","title":"Code Blocks","text":""},{"location":"misc/mkdocs/#python-block","title":"Python Block","text":"<pre><code>import torch \n</code></pre>"},{"location":"misc/mkdocs/#python-block-with-title","title":"Python Block with Title","text":"test.py<pre><code>import numpy as np\n</code></pre>"},{"location":"misc/mkdocs/#python-block-with-line-numbers","title":"Python Block with Line Numbers","text":"<pre><code>import torch \nimport numpy as np\ndef to_be_tensor(np_arr):\nreturn torch.from_numpy(np_arr)\n</code></pre>"},{"location":"misc/mkdocs/#python-block-with-highlighted-lines","title":"Python Block with Highlighted Lines","text":"<pre><code>import torch \nimport numpy as np\ndef to_be_tensor(np_arr):\nreturn torch.from_numpy(np_arr)\n</code></pre>"},{"location":"misc/mkdocs/#adding-annotations-stripping-line-noise","title":"Adding Annotations &amp; Stripping Line Noise","text":"<pre><code>theme:\nfeatures:\n- content.code.annotate # (1)\nFollowing line strips line noise from the code block\n# (2)!\n</code></pre> <ol> <li> I'm a code annotation! I can contain <code>code</code>, formatted     text, images, ... basically anything that can be written in Markdown.</li> <li>Look ma, less line noise!</li> </ol>"},{"location":"misc/mkdocs/#embedding-external-code","title":"Embedding External Code","text":"Code block with external content<pre><code>site_name: \u30b3\u30ea\u30f3\u306e\u30b9\u30da\u30fc\u30b9\nsite_description: Colin's Space\nsite_url: 'https://csl122.com/'\nsite_author: Shiliang Chen\n# repo_url: 'https://github.com/csl122/mkdocs'\n# edit_uri: edit/main/docs/\ntheme:\n  name: material\n  favicon: images/avt.png\n  logo: images/avt.png\n  features:\n    - navigation.tabs\n    - navigation.top\n    - navigation.sections\n    - navigation.tracking\n    - navigation.indexes\n    - navigation.expand\n    # - toc.integrate\n    - toc.follow\n    - search.suggest\n    - search.highlight\n    - content.tabs.link\n    - content.code.annotation\n    - content.code.copy\n    - content.code.annotate\n  language: en\n  palette:\n    - scheme: default\n      toggle:\n        icon: material/brightness-7\n        name: Switch to dark mode\n      primary: white\n      accent: cyan\n    - scheme: slate \n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode    \n      primary: black\n      accent: cyan\n  icon:\n    # logo: material/apple-keyboard-command\n    repo: fontawesome/brands/github\n    admonition:\n      note: octicons/tag-16\n      abstract: octicons/checklist-16\n      info: octicons/info-16\n      tip: octicons/squirrel-16\n      success: octicons/check-16\n      question: octicons/question-16\n      warning: octicons/alert-16\n      failure: octicons/x-circle-16\n      danger: octicons/zap-16\n      bug: octicons/bug-16\n      example: octicons/beaker-16\n      quote: octicons/quote-16\n\nnav:\n  - Home:\n      - index.md\n      - Bio: home/bio.md\n      # - \u672f\u8bed: GettingStarted/term.md\n      # - \u58f0\u660e: GettingStarted/state.md\n      # - \u8d21\u732e\u6307\u5357: GettingStarted/contributing.md\n  - AIGC:\n      - aigc/index.md\n      - Stable Diffusion: aigc/Stable Diffusion.md\n  - NLP:\n      - nlp/index.md\n      - Natural Language Processing: nlp/Natural Language Processing.md\n  - ML/DL:\n      - ml/index.md\n      - Deep Learning: ml/Deep Learning.md\n      - ML for Imaging: ml/Machine Learning for Imaging.md\n      - NumPy: ml/numpy.md\n      - PyTorch: ml/PyTorch.md\n  - Project:\n      - project/index.md\n  - Mac: \n      - mac/index.md\n      - Mac: mac/Mac.md\n  - Linux: \n      - linux/index.md\n      - Git: linux/git.md\n      - Vim: linux/Vim.md\n      - AstroNvim: linux/AstroNvim.md\n      - Starship: linux/Starship.md\n      - Tmux: linux/Tmux.md\n  - Networking: \n      - networking/index.md\n      - Nginx: networking/Nginx.md\n      - Nginx Proxy Manager: networking/Nginx Proxy Manager.md\n  - Game: \n      - game/index.md\n  - Miscellaneous:\n      - misc/index.md\n      - References: misc/mkdocs.md\n      - 23W26: misc/23W26.md\n\n# plugins:\n#   - social\n\nextra:\n  homepage: https://csl122.com\n  social:\n    - icon: fontawesome/brands/github-alt\n      link: https://github.com/csl122\n    # - icon: fontawesome/brands/twitter\n    #   link: https://twitter.com/csl122\n    - icon: fontawesome/brands/linkedin\n      link: https://www.linkedin.com/in/csl122/\n    - icon: fontawesome/solid/paper-plane\n      link: mailto:csl122@hotmail.com\n\nmarkdown_extensions:\n  - pymdownx.highlight:\n      anchor_linenums: true\n  - pymdownx.inlinehilite\n  - pymdownx.snippets\n  - admonition\n  - pymdownx.arithmatex:\n      generic: true\n  - footnotes\n  - pymdownx.details\n  - pymdownx.superfences\n  - pymdownx.mark\n  - attr_list\n  - pymdownx.emoji:\n      emoji_index: !!python/name:materialx.emoji.twemoji\n      emoji_generator: !!python/name:materialx.emoji.to_svg\n  - pymdownx.tabbed:\n      alternate_style: true\ncopyright: |\n  &amp;copy; 2023 &lt;a href=\"https://github.com/csl122\"  target=\"_blank\" rel=\"noopener\"&gt;Shiliang Chen&lt;/a&gt;\n</code></pre>"},{"location":"misc/mkdocs/#footnote","title":"Footnote","text":"<p>Lorem ipsum1 dolor sit amet, consectetur adipiscing elit.2</p> <ol> <li> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit.\u00a0\u21a9</p> </li> <li> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit.\u00a0\u21a9</p> </li> </ol>"},{"location":"ml/","title":"ML/DL","text":""},{"location":"ml/Deep%20Learning/","title":"Deep Learning","text":"<p>tags:: IC, Course, ML, Uni-S10 alias:: DL Ratio: 5:5 Time: \u661f\u671f\u4e94 9:00 - 11:00</p>"},{"location":"ml/Deep%20Learning/#notes","title":"Notes","text":"<ul> <li>\u672f\u8bed\u8868<ul> <li>LVM: Latent Variable Model \u9690\u53d8\u91cf\u6a21\u578b</li> <li>GMM: Gaussian Mixture Model \u9ad8\u65af\u6df7\u5408\u6a21\u578b</li> <li>VAE: Variational Auto-encoder \u53d8\u5206\u81ea\u7f16\u7801\u5668</li> <li>GAN: Generative Adversarial Network \u751f\u6210\u5f0f\u5bf9\u6297\u7f51\u7edc</li> <li>PCA: Principal Component Analysis \u4e3b\u6210\u5206\u5206\u6790</li> <li>KL Divergence: Kullback-Leibler divergence KL\u6563\u5ea6</li> <li>ELBO: evidence lower bound</li> </ul> </li> <li>Week2 Curse of dimensionality &amp; Convolution &amp; Padding &amp; Strides   collapsed:: true<ul> <li>Slides   collapsed:: true<ul> <li>Curse of dimensionality<ul> <li>As the number of features or dimensions grows,   the amount of data we need to generalise accurately grows exponentially!</li> <li>[[Lipschitz]] continuous function<ul> <li>\u5728\u6570\u5b66\u4e2d\uff0c\u7279\u522b\u662f\u5b9e\u5206\u6790\uff0c\u5229\u666e\u5e0c\u8328\u8fde\u7eed\uff08Lipschitz continuity\uff09\u4ee5\u5fb7\u56fd\u6570\u5b66\u5bb6\u9c81\u9053\u592b\u00b7\u5229\u666e\u5e0c\u8328\u547d\u540d\uff0c\u662f\u4e00\u4e2a\u6bd4\u4e00\u81f4\u8fde\u7eed\u66f4\u5f3a\u7684\u5149\u6ed1\u6027\u6761\u4ef6\u3002\u76f4\u89c9\u4e0a\uff0c\u5229\u666e\u5e0c\u8328\u8fde\u7eed\u51fd\u6570\u9650\u5236\u4e86\u51fd\u6570\u6539\u53d8\u7684\u901f\u5ea6\uff0c\u7b26\u5408\u5229\u666e\u5e0c\u8328\u6761\u4ef6\u7684\u51fd\u6570\u7684\u659c\u7387\uff0c\u5fc5\u5c0f\u4e8e\u4e00\u4e2a\u79f0\u4e3a\u5229\u666e\u5e0c\u8328\u5e38\u6570\u7684\u5b9e\u6570\uff08\u8be5\u5e38\u6570\u4f9d\u51fd\u6570\u800c\u5b9a\uff09</li> </ul> </li> <li></li> <li></li> <li>\u4f8b\u5982\u8bf4\u6211\u4eec\u9700\u898120%\u7684\u6570\u636e\u6765\u5e2e\u52a9KNN\u6765\u51b3\u5b9a\u54ea\u4e2alabel, 1d\u7684\u60c5\u51b5\u53ea\u9700\u89810.2\u4e2a\u5355\u4f4d, \u4f46\u662f\u5982\u679c\u662f2d\u5c31\u9700\u89810.45^2\u7ea6\u7b49\u4e8e0.2; \u4e5f\u5c31\u662f\u8bf4, \u4e24\u4e2a\u7ef4\u5ea6\u90fd\u9700\u89810.45\u4e2a\u5355\u4f4d\u5927\u5c0f\u6765\u83b7\u5f970.2\u7684\u6570\u636e\u91cf</li> </ul> </li> </ul> </li> <li>Notes   collapsed:: true<ul> <li>Leo Breiman 100 observations cover well the one-dimensional space of real numbers between 0 and 1.</li> <li>To achieve a similar coverage as in one-dimensional space, 100^10 = 10^20 samples must be taken for 10-dimensional space, which results in a much higher effort.</li> <li>Formulation of curse: different kinds of random distributions of the data sets, the difference between the smallest and the largest distance between data sets becomes arbitrarily small compared to the smallest distance if the dimensionality d increases (in other words, the smallest and largest distance differ only relatively little). \u7eac\u5ea6\u8d8a\u5927, \u6700\u5c0f\u8ddd\u79bb\u548c\u6700\u5927\u8ddd\u79bb\u7684\u5dee\u8ddd\u4f1a\u8d8a\u5c0f, \u56e0\u4e3a\u7ef4\u5ea6\u5927\u4e0a\u53bb\u4ee5\u540e, \u5927\u5bb6\u90fd\u5f97\u8de8\u8d8a\u5343\u5c71\u4e07\u6c34, \u56e0\u6b64\u5728\u8fd9\u79cd\u60c5\u51b5, \u666e\u901a\u7684\u8ddd\u79bb\u51fd\u6570\u7684\u4f5c\u7528\u5c31\u4e0d\u5927\u4e86</li> <li>\u8fd8\u6709\u4e00\u79cdcurse\u7684\u63cf\u8ff0\u662f, \u9ad8\u7eacsphere\u548c\u9ad8\u7eaccube\u4e4b\u95f4\u7684\u6bd4\u503c, \u4f1a\u968f\u7740\u7ef4\u5ea6\u7684\u589e\u52a0\u6025\u5267\u7f29\u5c0f, \u70b9\u548c\u4e2d\u5fc3\u7684\u8ddd\u79bb\u4f1a\u7279\u522b\u5927<ul> <li>\"the volume of a high dimensional orange is mostly in its skin, not the pulp!\"</li> <li>the volume of the hypersphere becomes very small (\u201dinsignificant\u201d) compared to the volume of the hypercube with increasing dimension</li> <li>\u4e3a\u4ec0\u4e48\u9ad8\u7eac\u5ea6\u5bf9clustering method\u4e0d\u53cb\u597d: the number of dimensions increase, points move further away from each other, which causes sparsity (large unknown subspaces) and common distance measures to break down. \u56e0\u6b64\u662f\u4e0d\u662f\u53ef\u4ee5\u8ba4\u4e3adense space could still be used for clustering analysis with distance measures? #Question</li> <li></li> </ul> </li> <li></li> <li>Weight sharing<ul> <li>The weights of this network are sparse and spatially shared.</li> <li>These shared weights can be interpreted as weights of a filter function. The input tensor is convoluted with this filter function (c.p. sliding window filter) before the activation of a convolutional layer.</li> </ul> </li> <li>Why convolution flip?<ul> <li>In NN, no flip for convenience.</li> <li>When performing the convolution, you want the kernel to be flipped with respect to the axis along which you're performing the convolution because if you don't, you end up computing a correlation of a signal with itself.</li> </ul> </li> <li>[[Padding]] and [[Stride]] #convolution #[[transposed convolution]]<ul> <li>[[Convolution]]<ul> <li>Assuming padding of size p, no stride<ul> <li>\\(o_j=i_j-k_j+2p+1\\)</li> <li>\u4f8b\u5982\u539f\u56fe\u662f5, kernal\u662f4, padding\u662f2, output\u5c31\u662f5-4+4+1 = 6</li> </ul> </li> <li>Assuming padding of size p, stride \\(s_j\\)<ul> <li>\\(o_j=[(i_j-k_j+2p)/s_j]+1\\)</li> <li>\u4f8b\u5982\u539f\u56fe\u662f5, kernal\u662f3, padding\u662f1, stride\u662f2, output\u5c31\u662f (5 \u2212 3 + 2)/2 + 1 = 3</li> </ul> </li> </ul> </li> <li>[[Transposed Convolution]]<ul> <li>\u8f6c\u7f6e\u5377\u79ef\u521a\u521a\u8bf4\u4e86\uff0c\u4e3b\u8981\u4f5c\u7528\u5c31\u662f\u8d77\u5230\u4e0a\u91c7\u6837\u7684\u4f5c\u7528\u3002\u4f46\u8f6c\u7f6e\u5377\u79ef\u4e0d\u662f\u5377\u79ef\u7684\u9006\u8fd0\u7b97\uff08\u4e00\u822c\u5377\u79ef\u64cd\u4f5c\u662f\u4e0d\u53ef\u9006\u7684\uff09\uff0c\u5b83\u53ea\u80fd\u6062\u590d\u5230\u539f\u6765\u7684\u5927\u5c0f\uff08shape\uff09\u6570\u503c\u4e0e\u539f\u6765\u4e0d\u540c\u3002\u8f6c\u7f6e\u5377\u79ef\u7684\u8fd0\u7b97\u6b65\u9aa4\u53ef\u4ee5\u5f52\u4e3a\u4ee5\u4e0b\u51e0\u6b65\uff1a<ul> <li>\u5728\u8f93\u5165\u7279\u5f81\u56fe\u5143\u7d20\u95f4\u586b\u5145s-1\u884c\u3001\u52170\uff08\u5176\u4e2ds\u8868\u793a\u8f6c\u7f6e\u5377\u79ef\u7684\u6b65\u8ddd\uff09</li> <li>\u5728\u8f93\u5165\u7279\u5f81\u56fe\u56db\u5468\u586b\u5145k-p-1\u884c\u3001\u52170\uff08\u5176\u4e2dk\u8868\u793a\u8f6c\u7f6e\u5377\u79ef\u7684kernel_size\u5927\u5c0f\uff0cp\u4e3a\u8f6c\u7f6e\u5377\u79ef\u7684padding\uff0c\u6ce8\u610f\u8fd9\u91cc\u7684padding\u548c\u5377\u79ef\u64cd\u4f5c\u4e2d\u6709\u4e9b\u4e0d\u540c\uff09</li> </ul> </li> <li>\u4f8b\u5982:<ul> <li>\u4e0b\u9762\u5047\u8bbe\u8f93\u5165\u7684\u7279\u5f81\u56fe\u5927\u5c0f\u4e3a2x2\uff08\u5047\u8bbe\u8f93\u5165\u8f93\u51fa\u90fd\u4e3a\u5355\u901a\u9053\uff09\uff0c\u901a\u8fc7\u8f6c\u7f6e\u5377\u79ef\u540e\u5f97\u52304x4\u5927\u5c0f\u7684\u7279\u5f81\u56fe\u3002\u8fd9\u91cc\u4f7f\u7528\u7684\u8f6c\u7f6e\u5377\u79ef\u6838\u5927\u5c0f\u4e3ak=3\uff0cstride=1\uff0cpadding=0\u7684\u60c5\u51b5\uff08\u5ffd\u7565\u504f\u6267bias\uff09\u3002<ul> <li>\u9996\u5148\u5728\u5143\u7d20\u95f4\u586b\u5145s-1=0\u884c\u3001\u52170\uff08\u7b49\u4e8e0\u4e0d\u7528\u586b\u5145\uff09</li> <li>\u7136\u540e\u5728\u7279\u5f81\u56fe\u56db\u5468\u586b\u5145k-p-1=2\u884c\u3001\u52170</li> </ul> </li> <li></li> </ul> </li> <li>GIF<ul> <li></li> <li></li> <li></li> </ul> </li> <li>\u8ba1\u7b97:</li> <li>\\(o = (i-1) \\times s - 2p + k\\)</li> <li>\\(H_{out\u200b}=(H_{in\u200b}\u22121)\u00d7s[0]\u22122\u00d7p[0]+k[0]\\)</li> <li> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>Week3 Invariance &amp; equivariance &amp; LeNet, AlexNet, VGG   collapsed:: true<ul> <li>Slides<ul> <li>Invariance &amp; equivariance<ul> <li>\u4e0d\u53d8\u6027\u548c\u7b49\u53d8\u6027</li> <li>\u6307\u7684\u662f\u8f93\u51fa\u968f\u7740\u8f93\u5165\u7684\u53d8\u5316\u53d8\u4e0d\u53d8\u7684\u6027\u8d28</li> <li>CNN \u62e5\u6709approximate shift equivariance, \u662f\u901a\u8fc7convolution layers\u5b9e\u73b0\u7684, \u56e0\u4e3a\u539f\u56fe\u7684\u79fb\u52a8, conv\u5b8c\u540e\u7684feature map\u4e5f\u4f1a\u62e5\u6709\u540c\u6837\u7684\u79fb\u52a8</li> <li>CNN \u62e5\u6709approximate shift invariance, \u662f\u901a\u8fc7pooling \u548cstriding\u5b9e\u73b0\u7684, \u56e0\u4e3alocally max pooling\u7684\u7ed3\u679c\u4f1a\u662f\u4e00\u6837\u7684, \u90a3\u4e48\u8f93\u51fa\u5c31\u4e0d\u4f1a\u53d8\u5316, \u5982\u679c\u5c3a\u5ea6\u5927\u7684\u8bdd, \u6709\u89e3\u51b3\u65b9\u6cd5\u4f8b\u5982blurring<ul> <li>But striding ignores the [[Nyquist]] sampling theorem and aliases \u91c7\u6837\u7387\u4e0d\u591f, \u5c31\u4f1a\u51fa\u73b0\u95ee\u9898</li> </ul> </li> <li>CNN\u4e5f\u5bf9deformation invariance, \u4f8b\u5982\u6570\u5b573\u7684\u4e00\u4e9b\u5c0f\u7ec6\u8282\u4e0a\u9762\u7684\u5c0f\u53d8\u52a8, \u662f\u4e0d\u5f71\u54cdcnn\u7684\u5206\u7c7b\u8f93\u51fa\u7684</li> <li>\u4f20\u7edfCNN \u65e0\u6cd5\u505a\u5230rotation equivariance, \u56e0\u4e3a\u65cb\u8f6c\u4ee5\u540e\u7684\u5206\u5e03\u53d8\u5316\u4f1a\u5bfc\u81f4feature map\u4ea7\u751f\u5927\u7684\u4e0d\u540c</li> </ul> </li> <li>LeNet<ul> <li></li> <li>AlexNet<ul> <li>Key modifications:   \u2022 Add a dropout layer after two hidden dense layers   (better robustness / regularization)   SVM   Softmax regression   \u2022 Change activation function from sigmoid to ReLu   (no more vanishing gradient)   \u2022 MaxPooling   \u2022 Heavy data augmentation   \u2022 Model ensembling</li> </ul> </li> </ul> </li> <li>VGG<ul> <li>Deep and narrow = better</li> <li>a larger number of compositions of simple functions turns out to be more expressive and more able to fit meaningful models than a small number of shallower and more complicated functions.</li> <li></li> </ul> </li> </ul> </li> <li>Notes<ul> <li>The [[Nyquist]] rate<ul> <li>Samples must be taken at a rate that is twice the frequency of the highest frequency component to be reconstructed.</li> <li>Under-sampling: sampling at a rate that is too coarse, i.e., is below the Nyquist rate.</li> <li>Aliasing: artefacts (jaggies) that result from under-sampling.</li> </ul> </li> <li>\u56fe\u7247\u7684frequency\u5176\u5b9e\u5c31\u662f gradient, edge\u5c31\u662fgradient\u5927\u7684\u5730\u65b9, \u5c31\u662f\u9ad8\u9891\u90e8\u5206, \u5e73\u6ed1\u533a\u57df\u5c31\u662f\u4f4e\u9891\u90e8\u5206, \u53ef\u4ee5\u901a\u8fc7\u4e0d\u540c\u9891\u7387\u7684\u4e0d\u65ad\u91c7\u6837reconstruct</li> </ul> </li> </ul> </li> <li> <p>Week4 NiN, ResNet, Inception, BatchNorm, Activations, Loss, Augmentation, U-net   collapsed:: true</p> <ul> <li>Slides<ul> <li>Parameters of Convolution layers<ul> <li>\\(c_i \\times c_0 \\times k^2\\)</li> <li>computation\u5c31\u662f\u524d\u9762\u4e58\u4e0afeature map\u7684\u5927\u5c0f\u50cf\u7d20\u4e2a\u6570 \\(m_h \\times m_w\\)</li> </ul> </li> <li>Parameters of fully connected layer connected with conv and output<ul> <li>\\(c \\times m_w \\times m_h \\times n\\)</li> </ul> </li> <li>Networks in Networks<ul> <li>NiN\u8bbe\u8ba1\u5728conv\u5c42\u4e2d\u95f4\u52a0\u5165MLP\u6765\u83b7\u53d6\u66f4\u591aconv\u4fe1\u606f</li> <li>\u5b9e\u73b0\u4e2d, MLP\u4f7f\u7528\u4e86conv 1x1\u6765\u4f5c\u4e3amlp</li> <li></li> <li>global average pooling<ul> <li>\u5168\u5c40\u5e73\u5747\u6c60\u5316, \u5bf9\u4e8e\u6bcf\u4e00\u4e2achannel\u4e2d\u7684\u6240\u6709\u6570\u636e\u53d6\u5e73\u5747\u5f97\u5230\u552f\u4e00\u7684\u4e00\u4e2a\u503c, \u4e5f\u5c31\u662f\u51e0\u4e2a\u9891\u9053\u5c31\u6709\u51e0\u4e2a\u503c, \u800c\u4e00\u822c\u8fd9\u4e2a\u9891\u9053\u6570\u4f1a\u4e8b\u5148\u8bbe\u5b9a\u4e3a\u7c7b\u522b\u6570, \u76f4\u63a5\u5efa\u7acb\u8d77\u4e86conv\u7ed3\u679c\u4e0e\u7c7b\u522b\u7684\u5173\u7cfb. \u53d6\u5e73\u5747\u662f\u4e3a\u4e86\u4fdd\u7559\u8be5\u901a\u9053\u5168\u5c40\u4fe1\u606f, \u800c\u4e0d\u662f\u6700\u663e\u8457\u4fe1\u606f</li> </ul> </li> </ul> </li> <li>Inception (GooLeNet)<ul> <li>inception net\u7684\u601d\u60f3\u5c31\u662f\u628a\u8be5\u7528\u4ec0\u4e48\u6837\u7684\u5377\u79ef\u6838\u5927\u5c0f\u4e2a\u6570padding stride\u76f4\u63a5\u4ea4\u7ed9\u7f51\u7edc\u53bb\u4f18\u5316, \u6bcf\u4e00\u6761path\u53ea\u8981\u4fdd\u8bc1\u53bb\u5176\u4ed6\u7684\u5f62\u72b6\u4e00\u81f4\u5373\u53ef, \u8f93\u51fa\u901a\u9053\u6570\u662f\u4e0d\u4e00\u6837\u7684, \u6700\u540e\u5ef6\u901a\u9053\u62fc\u63a5\u8d77\u6765. \u671f\u5f85\u6bcf\u4e2achannel\u90fd\u80fdwork for \u7279\u5b9a\u7684\u4efb\u52a1 have an architecture with different channels doing different things and the hope is one of those channels is gonna work for the cats and one is gonna work for I some birds and so on.</li> <li></li> <li></li> <li>\u7531\u4e8e\u662f\u4e0d\u540c\u7c7b\u578b\u7684\u5377\u79ef\u6838concat, \u80fd\u591f\u5927\u5927\u51cf\u5c11\u7531\u5355\u4e00\u5377\u79ef\u6838\u751f\u6210\u5bfc\u81f4\u7684\u5927\u91cf\u53c2\u6570\u95ee\u9898</li> <li></li> <li>\u795e\u7ecf\u7f51\u7edc\u7b2c\u4e00\u5c42\u901a\u5e38\u4f1a\u91c7\u7528\u4f8b\u598277\u7684conv\u548cmaxpool, \u5176\u4f5c\u7528\u5728\u4e8ehave some basic   amount of translation invariance and that I am able to reduce the dimensionality reasonably quickly \u5e73\u79fb\u4e0d\u53d8\u6027, \u51cf\u5c11\u7279\u5f81\u7a7a\u95f4\u5927\u5c0f;</li> <li>\u7b2c\u4e8c\u4e2a\u9636\u6bb5\u4e00\u822c\u4f1a\u505a\u7b80\u5355\u768433\u5377\u79ef\u548cmaxpool\u6765\u83b7\u5f97\u603b\u4f53\u7684\u7a7a\u95f4\u4e0a\u7684\u5173\u8054get some overall spatial correlation and then some pooling operation in the end.</li> <li>\u5269\u4f59\u7684\u6b65\u9aa4\u53ef\u4ee5\u7406\u89e3\u4e3a\u51cf\u5c11pixels, \u4f46\u662f\u8ba9\u4ed6\u4eec\u62e5\u6709\u66f4\u591a\u9ad8\u5c42\u6b21\u7684\u6709\u7528\u7684\u4fe1\u606fshrinking resolution but I'm also increasing the number of channels because now while I have fewer pixels they have more valuable more higher- order information that I'm going to use later on.</li> <li>\u7528\u4e24\u4e2a33conv\u4ee3\u66ff55\u53ef\u4ee5\u51cf\u5c11\u53c2\u6570\u91cf, \u4f46\u662f\u5177\u6709\u76f8\u540c\u7684\u611f\u53d7\u91ce</li> <li>V3\u7248\u672c\u4e2d\u8fd8\u7528\u4e8617 71 conv, you might only get for example vertically contiguous features in some way</li> <li>\u5b66\u5230\u4e86\u4ec0\u4e48?<ul> <li>Dense layers are computationally and memory intensive</li> <li>1x1 convolutions act like a multi-layer perceptron per pixel.</li> <li>If not sure, just take all options and let the optimization decide or even learn this through trial and error (genetic algorithm, AmoebaNet)</li> </ul> </li> </ul> </li> <li> <p>BatchNorm</p> <ul> <li>the trouble is as I'm adapting from the top down, the features are going back up, are going to change so now that last layer that was actually fairly well adapted to begin with, has to readapt now to the new inputs that it's getting. long time to converge and adopt all layers.</li> <li></li> <li>BN\u5728\u8bad\u7ec3\u4e2d, \u6bcf\u4e2abatch\u90fd\u4f1a\u4ee5channel\u7b97\u51fa\u4e00\u4e2a\u6574\u4e2abatch\u7684mean \u548cvariance, \u6bd4\u59823\u4e2a\u901a\u9053, \u5c31\u4f1a\u6709\u4e09\u4e2amean \u548cvariance, \u7136\u540e\u6240\u6709\u7684\u6570\u636e\u6839\u636e\u8fd9\u4e2a\u503c\u6765\u8fdb\u884cnormalise   \u6d4b\u8bd5\u4e2d, \u5219\u4f7f\u7528\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e0d\u65ad\u66f4\u65b0\u7684running mean \u548cvariance\u8fdb\u884cnormalise, \u4e5f\u5c31\u662f\u6574\u4e2a\u6570\u636e\u96c6\u7684, \u662f\u5728\u4e00\u4e2a\u4e2abatch iter\u4e2d\u7c7b\u4f3c\u4e8etemporal difference \u66f4\u65b0\u7684, \u4ee3\u7801\u5982\u4e0b #BatchNorm #ML<ul> <li> <p>```python   class BatchNorm2d(nn.Module):       def init(self, num_features, eps=1e-05, momentum=0.1):           super(BatchNorm2d, self).init()           \"\"\"           An implementation of a Batch Normalization over a mini-batch of 2D inputs.</p> <pre><code>  The mean and standard-deviation are calculated per-dimension over the\n  mini-batches and gamma and beta are learnable parameter vectors of\n  size num_features.\n\n  Parameters:\n  - num_features: C from an expected input of size (N, C, H, W).\n  - eps: a value added to the denominator for numerical stability. Default: 1e-5\n  - momentum: the value used for the running_mean and running_var\n  computation. Default: 0.1 . (i.e. 1-momentum for running mean)\n  - gamma: the learnable weights of shape (num_features).\n  - beta: the learnable bias of the module of shape (num_features).\n  \"\"\"\n  # TODO: Define the parameters used in the forward pass                 #\n  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n  self.num_features = num_features\n  self.eps = eps\n  self.momentum = momentum\n\n  # self.register_parameter is not used as it was mentioned on piazza\n  # that this will be overridden\n  self.gamma = torch.ones((1, num_features, 1, 1))\n  self.beta = torch.zeros((1, num_features, 1, 1))\n  self.running_mean = torch.zeros((1, num_features, 1, 1))\n  self.running_var = torch.ones((1, num_features, 1, 1))\n</code></pre> <p># *END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***</p> <p>def forward(self, x):       \"\"\"       During training this layer keeps running estimates of its computed mean and       variance, which are then used for normalization during evaluation.       Input:       - x: Input data of shape (N, C, H, W)       Output:       - out: Output data of shape (N, C, H, W) (same shape as input)       \"\"\"       # TODO: Implement the forward pass                                     #       #       (be aware of the difference for training and testing)          #       # *START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***       if self.training:           mean = x.mean(dim=(0, 2, 3), keepdim=True)           var = x.var(dim=(0, 2, 3), keepdim=True)</p> <pre><code>      x_hat = (x-mean)/torch.sqrt(var+self.eps)\n      self.running_mean = self.momentum * mean + (1-self.momentum) * self.running_mean\n      self.running_var = self.momentum * var + (1-self.momentum) * self.running_var\n  else:\n      x_hat = (x-self.running_mean)/torch.sqrt(self.running_var+self.eps)\n\n  x = self.gamma * x_hat + self.beta\n  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n  return x\n                  ```\n            - **capacity control**: it does not really reduce covariate shift. they find out that it actually makes covariate shift worse. it actually turns out that basically this is doing regularization by noise injection. a mini batch of maybe 64 observations and so what you\u2019re effectively doing is you\u2019re subtracting some empirical mean and that's obviously noisy. And you\u2019re dividing by some empirical standard deviation. That's obviously also noisy.\n            - Batch Norm\u5bf9batch size \u654f\u611f:\n                - batch that\u2019s too large then you're not injecting enough noise and you're not regularizing enough.\n                - one that\u2019s too small then basically the noise becomes too high and then you\u2019re not converging very well.\n                - \u4f46\u662f\u5df2\u7ecf\u901a\u8fc7gamma\u548cbeta\u6765\u4fee\u6b63\u89e3\u51b3\u4e86a learned scale and offset.\n        - ResNet\n            - ![image.png](../assets/image_1675271910997_0.png)\n            - ![image.png](../assets/image_1675271930329_0.png)\n            - \u2018Taylor expansion\u2019 style parametrization\n            - it also means that as I add another layer, the identity function still goes through and it will still leave the outputs of the previous layers unchanged.\n            - ResNet\u901a\u5e38\u56db\u4e2alayer, \u6bcf\u4e2alayer\u6700\u5f00\u59cb\u7684\u90a3\u4e2ablock\u56e0\u4e3a\u9700\u8981\u5bf9\u9f50\u6b8b\u5dee\u7ed3\u6784\u7684\u8f93\u5165\u548c\u8f93\u51fa, \u9700\u8981\u4e0a\u9762\u56fe\u4e2d\u768411conv\u6765\u8fdb\u884c\u9891\u9053\u5bf9\u9f50\u4ee5\u53cadownsample\n        - ResNext\n            - ![image.png](../assets/image_1675272224692_0.png)\n            - So what we are doing is basically taking the network on the left, and slicing it up into 16 or more networks that just have four channels, or some larger number of channels each. And then in the end, we stack everything together. \u51cf\u5c11\u4e86\u53c2\u6570\u4e2a\u6570, \u901a\u8fc7\u5206\u7ec4\u7684\u65b9\u5f0f\n            - ![image.png](../assets/image_1675272362556_0.png)\n        - DenseNet\n            - ![image.png](../assets/image_1675272406263_0.png)\n            - ![image.png](../assets/image_1675272469553_0.png)\n        - Squeeze-Excite Net\n            - ![image.png](../assets/image_1675272880491_0.png)\n            - So attention is essentially a mechanism where, rather than taking averages over a bunch of vectors, we're using a separate function to gate how that average should be computed. attention \u5b9a\u4e49\u4e86\u8fd9\u4e2aaverage\u7684\u64cd\u4f5c\u600e\u4e48\u505a\n            - SE-Net\u505a\u7684\u4e8b\u60c5\u5c31\u662f, \u4f8b\u5982\u6709\u5f88\u591a\u9891\u9053, \u8fd9\u4e2a\u9891\u9053\u662f\u4e13\u653bcat\u7684, \u53e6\u4e00\u4e2a\u662f\u6050\u9f99, \u5f53\u4f60\u9884\u6d4b\u732b\u54aa\u7684\u65f6\u5019\u80af\u5b9a\u8981weight \u732b\u54aa\u591a\u4e00\u70b9, \u6240\u4ee5\u8fd9\u4e2a\u7f51\u7edc\u5c31\u662f\u5728learn global weighting function per channel. \u4f46\u662f\u6211\u4eec\u5982\u679c\u770b\u5230\u6709\u4e00\u7897\u725b\u5976\u7684\u65f6\u5019, \u90a3\u6709\u732b\u54aa\u7684\u53ef\u80fd\u6027\u5c31\u4f1a\u9ad8\u5f88\u591a, \u8fd9\u5c31\u662f\u4e00\u4e2a\u601d\u8def, \u628achannel\u4e4b\u95f4\u7684\u4fe1\u606f\u94fe\u63a5\u8d77\u6765\n            - What you could actually do is you could take very simply a product of the entire image in a per channel basis, with some other vector. \u76f8\u5f53\u4e8e\u77e5\u9053\u4e86\u5173\u8054\u5ea6. Finally you use those numbers and the softmax over them to rewrite your channels.\n</code></pre> </li> </ul> </li> </ul> <p>So therefore, if this very cheap procedure tells me, well, there's a good chance that there's a cat somewhere, I can now up weigh the cat channel.     -             - ShuffleNet -  - we do something a little bit more structured, or, more sparse, structured with our networks. - if we have three channels, well, we go and basically pick, one from the red greens and blues and turn that into a new block.   And then I essentially intertwine things in a meaningful way. - Efficient on mobile devices             - Activation functions - Linear activation     -      - if all are linear in nature, the final activation function of last layer is nothing but just a linear function of the input of first layer!     - That means these N linearly activated layers can be replaced by a single layer.       No matter how we stack, the whole network is still equivalent to a single layer with linear activation - Sigmoid function        - looks smooth and \u201cstep function like\u201d     - has a tendency to bring the Y values to either end of the curve.     - Making clear distinctions on prediction.     - the Y values tend to respond very less to changes in X. What does that mean? The gradient at that region is going to be small. It gives rise to a problem of \u201cvanishing gradients\u201d.       This meant the network may refuse to learn further or is drastically slow. - tanh function     -      - One advantage of tanh is that we can expect the output to be close to have a zero-mean.     - because they see positive and negative values and therefore tend to converge faster.     - Deciding between the sigmoid or tanh will depend on your requirement of gradient strength.     - \u4e0d\u80fdstack\u592a\u591a - ReLU     -      - It is not bound and the range of ReLu is [0, inf). This means it can blow up the activation.       the sparsity of the activation can also be a problem in networks.     - We would ideally want a few neurons in the network to not activate and thereby making the activations sparse and efficient.       RELU allows this.     - Because of the horizontal line in ReLu( for negative X ), the gradient can go towards 0. For activations in that region of ReLu, gradient will be 0 because of which the weights will not get adjusted during descent. That means, those neurons which go into that state will stop responding to variations in error/ input. This is called dying ReLu problem. This problem can cause several neurons to just die and not respond making a substantial part of the network passive.     - ReLu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations. - Leaky ReLU     -      - for example y = 0.01x for x&lt;0 will make it a slightly inclined line rather than horizontal line. This is leaky ReLu. There are other variations too. The main idea is to let the gradient be non zero and recover during training eventually. - PReLU     -      - Here a is a learnable parameter.     - What\u2019s interesting about the ReLus is that they are scale invariant.       You can multiply the signal by a value and the output will not be changed, except the scale. So these are equivariant to scale. There is only one linearity. - SoftPlus     -      - Functions like these here are affected by the amplitude of the input signal.     - Softplus is a smooth approximation of the ReLU function. A differentiable version of ReLU.     - It has a scale parameter beta. The higher beta, the more this function will look like a ReLU.       It can be used to constrain the output of a unit to always be positive. - ELU     -      - Another soft version of RELU. You use Relu as a basis and add a small constant that makes it smooth.     - One difference is that this one here can become negative, unlike the RELu.       That may have an advantage but very much depends on the application it is used for. Allows the network to make the average of the output zero, which can help convergence. - CELU     -      -  - SELU     -  - GELU     -  - ReLU6     -  - LogSigmoid     -  - Softmin     -  - Softmax     -  - LogSoftmax     -  - Which function to use depends on the nature of the targeted problem. Most often you will be fine with ReLUs for classification problems. If the network does not converge, use leakyReLUs or PReLUs, etc. Tanh is quite ok for regression and continuous reconstruction problems.             - Loss -  -  -  -  -  -  -  -  -  -      -             - U-net -              - Augmentation -  -          - Notes             - mini batch\u7684\u7528\u6cd5\u7531\u6765 - \u6ca1\u6709\u529e\u6cd5\u628a\u6574\u4e2adatasset\u653e\u8fdb\u6765, \u53ea\u80fdrandom sample \u4e00\u4e9b, \u6765\u4f5c\u4e3a\u4f30\u8ba1 - \u4f46\u662f\u540e\u6765\u53d1\u73b0\u8fd9\u4e2a\u8fd8\u6709regularisation\u4f5c\u7528, \u6bcf\u4e2abatch\u6709\u81ea\u5df1\u7684\u5206\u5e03, \u5bfc\u81f4weights\u4e0d\u51c6\u4e86     - Week5 Generative models, VAE, GAN         - Unsupervised learning: Given data without supervision signal, goal is to infer a function that describes the hidden structure of unlabelled data             - \u2022 Probability distribution/density estimation               \u2022 Dimensionality Reduction                \u2022 Clustering         - Probability distribution/density estimation             -              - \u6570\u636e\u662f\u4ece\u67d0\u4e2a\u5206\u5e03\u4e2d\u624d\u517b\u51fa\u6765\u7684, \u6211\u4eec\u7684\u76ee\u7684\u662f\u5b66\u4e60\u4e00\u4e2a\u5206\u5e03\u53c2\u6570, \u80fd\u8ba9\u8fd9\u4e2a\u5206\u5e03\u6a21\u578b\u751f\u6210\u51fa\u6765\u7684\u6570\u636e\u5206\u5e03\u548c\u539f\u6570\u636e\u5206\u5e03\u5f88\u50cf         - Generative Latent Variable Models             -              - z\u662f\u9690\u53d8\u91cf, \u89c2\u6d4b\u4e0d\u5230\u7684\u4e00\u4e9b\u6f5c\u5728\u5206\u5e03\u5c5e\u6027, \u53ef\u4ee5\u7528\u6765\u4ee3\u8868\u4e00\u7c7b\u7684samples, \u4f8b\u5982\u53ef\u4ee5\u662f\u5199\u7684\u5b57\u7684label, \u4e66\u5199\u98ce\u683c, \u76f8\u673a\u7684\u89d2\u5ea6, \u5149\u7167\u6761\u4ef6, x\u662f\u901a\u8fc7\u8fd9\u4e9b\u4e1c\u897f\u51b3\u5b9a\u751f\u6210\u7684\u5b9e\u9645\u5206\u5e03\u7ed3\u679c\u7684             - x\u662f\u53ef\u89c2\u6d4b\u53d8\u91cf, \u5373\u4e3a\u89c2\u6d4b\u5230\u7684\u7ed3\u679c, \u4f8b\u5982\u751f\u6210\u7684\u6570\u5b57, \u5b9e\u9645\u7684\u7167\u7247             - p(z)\u662f\u5148\u9a8c, \u9700\u8981\u5148\u77e5\u9053\u624d\u80fd\u6709\u540e\u7eed\u751f\u6210, \u6211\u4eec\u5047\u5b9a\u4ed6\u670d\u4ece\u4e00\u5b9a\u7684\u6982\u7387\u5206\u5e03, \u6765\u8868\u793a\u67d0\u4e00\u7c7b\u7684sample\u7684\u5148\u9a8c\u6f5c\u5728\u53d8\u91cf             - p(x|z)\u662f\u4f3c\u7136, \u662f\u7ed9\u5b9az\u540ex\u51fa\u73b0\u7684\u6982\u7387             - p(x)\u662fevidence, \u5728\u8fd9\u4e2a\u95ee\u9898\u80cc\u666f\u4e0b, \u4e5f\u662f\u6211\u4eec\u8981\u62df\u5408\u7684\u5bf9\u8c61, \u901a\u8fc7sample z, \u52a0\u8d77\u6765expectation\u5c31\u662fp(x), \u4e5f\u5c31\u662f\u4e0a\u56fe\u7684\u63a8\u5bfc\u5f0f\u5b50             - p(z|x)\u662f\u540e\u9a8c, \u662f\u5f97\u5230x\u4ee5\u540e, \u67d0\u4e2az\u51fa\u73b0\u7684\u6982\u7387, \u6211\u4eec\u5e0c\u671b\u80fd\u7528\u8fd9\u4e2a\u6a21\u578bsample\u51fa\u4e0ex\u9ad8\u5ea6\u76f8\u5173\u7684z\u6765\u5e2e\u52a9\u751f\u6210\u6709\u7528\u7684x         - Dimensionality reduction (Prob PCA)             -              - \u7528\u6982\u7387\u5b66\u6765\u8fdb\u884c\u4e3b\u6210\u5206\u5206\u6790, \u5bf9\u539f\u672c\u6570\u636e\u7684d\u7ef4\u5ea6\u964d\u7ef4\u5230k, z\u5c31\u662fk\u7ef4\u7684\u53ef\u4ee5\u8868\u5f81\u4e00\u5927\u7c7bx\u7684\u9690\u53d8\u91cf \u5b58\u6709\u7edd\u5927\u591a\u6570\u6709\u7528\u7684\u4fe1\u606f, \u56e0\u6b64\u53ef\u4ee5\u7528\u6765\u590d\u539fx.             - \u8fd9\u91cc\u7b2c\u4e8c\u884c\u5c31\u662f\u60f3\u8981\u5b66\u4e60\u4e00\u4e2a\u6620\u5c04\u5173\u7cfb, \u4ee5\u671f\u628ak\u7ef4\u5ea6\u7684z\u6062\u590d\u6210d\u7ef4\u7684x, \u4e5f\u5c31\u662fx conditioned on z             - z\u4e5f\u662f\u9700\u8981\u5efa\u6a21\u7684, \u56e0\u4e3a\u6211\u4eec\u4e0d\u77e5\u9053\u54ea\u4e9bz\u662f\u6709\u7528\u7684z, \u662f\u5305\u542b\u4e86\u4e3b\u8981\u6210\u5206\u7684z, \u800c\u7b26\u5408\u67d0\u4e00\u4e2a\u5206\u5e03\u7684z\u5c31\u53ef\u80fd\u6ee1\u8db3\u54cd\u5e94\u7684\u8981\u6c42, \u80fd\u591f\u88ab\u7b2c\u4e8c\u4e2a\u5f0f\u5b50\u4e2d\u7684W\u7ed9\u6709\u6548\u6062\u590d\u6210\u53ef\u884c\u7684x             - \u751f\u6210Z\u7684\u8fc7\u7a0b\u5c31\u662f\u964d\u7ef4, encoder\u7684\u8fc7\u7a0b             - \u4ecez\u751f\u6210x \u7684\u8fc7\u7a0b\u5c31\u662f\u5347\u7ef4, decode\u7684\u8fc7\u7a0b         - Jensen's inequality:             -              - \u6ce8\u610f: \u5982\u679c\u662f\u4e0b\u51f9\u51fd\u6570\u7684\u8bdd, \u8fd9\u4e2a\u4e0d\u7b49\u5f0f\u7684\u5927\u4e8e\u7b49\u4e8e\u5c31\u662f\u5c0f\u4e8e\u7b49\u4e8e\u4e86, \u4f8b\u5982log         - Divergence minimisation (KL divergence)             -              - \\(KL[p(x)||q(x)]\\ =\\ E_{p(x)}[log\\frac{p(x)}{q(x)}]\\)             -          - Maximum Likelihood Estimation             -              - \u8fd9\u4e2a\u7b80\u5355\u7406\u89e3\u5c31\u662f, KL divergence\u5176\u5b9e\u5c31\u662f\u8ba9theta\u4e0b\u7684prob\u6700\u63a5\u8fd1\u771f\u5b9e\u60c5\u51b5, \u6240\u4ee5\u5c31\u662fMLE, \u4ecedata\u4e2d\u91c7\u6837\u7684\u70b9, \u5728\u6211\u4eec\u7684\u6982\u7387\u6a21\u578b\u4e2d\u51fa\u73b0\u7684\u6982\u7387\u6700\u5927\u5316, \u90a3\u5c31\u662f\u5f88\u63a5\u8fd1\u4e86         - ((63e78193-b63d-4865-b333-a0759ae6f935))             - ((63e78203-d202-469c-9dd8-1185c4cbb85d))             - \u6211\u4eec\u7684p(x)\u7684\u5efa\u7acb, \u9700\u8981p(z)\u8fd9\u4e2a\u5148\u9a8c, \u4f46\u662f\u6211\u4eec\u6ca1\u529e\u6cd5\u505a\u5230sample\u6240\u6709\u53ef\u80fd\u7684z, \u800c\u4e14\u4e0ex\u65e0\u5173\u7684z\u53ea\u4f1a\u5e26\u6765\u65e0\u7528\u7684\u566a\u97f3, \u56e0\u6b64\u6211\u4eec\u9700\u8981\u7528\u5230\u540e\u9a8cp(z|x)\u6765\u627e\u5230\u9002\u5408\u7684z         - [[VAE]]             - References: \u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u2014\u4f18\u96c5\u7684\u6a21\u578b\uff08\u4e00\uff09\uff1a\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09 - \u77e5\u4e4e             - ((63e782cc-b656-4ffa-8c6e-63a2ca501ad0))             - ((63e782f0-897d-43d3-bfb4-5753ec863a3c))             - ((63e782fd-6145-4156-b8b2-7d112b470d1c))             - ((63e7830d-d431-463f-8f8c-2a92414de066))             - ((63e7831c-09a9-43e3-8b78-819d4dd176c4)) - \u539f\u672c\u662f\u91c7\u6837z\u6765\u8fdb\u884c\u8bad\u7ec3, \u6ca1\u6709\u529e\u6cd5\u8fdb\u884cBP, \u4f46\u662f\u6211\u4eec\u901a\u8fc7epsilon\u8fd9\u4e2a\u4ece\u9ad8\u65af\u5206\u5e03\u4e2d\u91c7\u6837\u7684\u4e1c\u897f\u6765\u6539\u53d8z\u7684\u65b9\u5dee, \u6765\u83b7\u5f97\u4e0d\u540c\u7684z\u6837\u672c, \u56e0\u4e3aepsilon\u662f\u4e00\u4e2a\u5e38\u91cf, \u5c31\u4e0d\u7528\u62c5\u5fc3BP\u7684\u95ee\u9898\u4e86 - ((63e78419-fa60-4c52-91d5-cb831af8bb52))             - \u4e0b\u9762\u8bb2\u4e24\u4e2a\u63a8\u51faLoss\u7684\u65b9\u5f0f             - ((63e78343-7822-49c1-8424-b2a339b4d13d))             - ((63e78350-a414-4295-ab0f-e8d999023d5b))             - ((63e7836c-9a5e-438a-8dc0-f782dbdb53dd))             - VAE objective - ((63e78381-3db8-4b4e-9bce-7e3201dda44a))             - \u6211\u4eec\u9700\u8981\u7ed9encoder \u5b9a\u4e49\u4e00\u4e2a\u9700\u8981\u5b66\u4e60\u7684\u5206\u5e03\u7684\u6807\u51c6 q distribution design - ((63e783a9-82ca-405d-8757-d0d2985b54a1)) - \u590d\u6742\u7684, \u4e0d\u540c\u7684\u5206\u5e03\u5047\u8bbe, \u4f1a\u5e26\u6765\u4e0d\u4e00\u6837\u7684\u7ed3\u679c             -  - \u6700\u7ec8\u7684Loss\u8003\u8651\u5230\u4e86reconstruction loss, \u5373\u6781\u5927\u4f3c\u7136\u90e8\u5206, \u751f\u6210\u6570\u636e\u548c\u6570\u636e\u96c6\u7684\u76f8\u4f3c\u7a0b\u5ea6, \u4ee5\u53caKL regulariser, \u7528\u6765\u8ba9\u6211\u4eec\u8bbe\u8ba1\u7684encoder\u4e2d\u7684q\u63a5\u8fd1\u6211\u4eec\u671f\u671b\u7684\u771f\u5b9e\u9690\u53d8\u91cf\u5206\u5e03p(z)             - Practical implementation - ((63e78506-afdc-486f-82f4-914c0d7608e0))         - [[GAN]]             -              - Two-player game objective: - ((63e78686-63fa-4ba2-ab32-206fdbbb14fa)) - \u5bf9Discriminator \u800c\u8a00, \u8fd9\u4e2aLoss\u8981\u8d8a\u5927\u8d8a\u597d, \u8868\u793a\u4ed6\u8bc6\u522b\u51fa\u6765\u4e86 - \u5bf9Generator\u800c\u8a00, \u8fd9\u4e2aLoss\u8981\u8d8a\u5c0f\u8d8a\u597d, \u8868\u793a\u4e0d\u80fd\u88ab\u8ba4\u51fa\u6765\u6211\u751f\u6210\u7684\u56fe\u7247 -  - \u8bad\u7ec3\u7684\u65f6\u5019, \u56fa\u5b9a\u67d0\u4e00\u4e2a\u53c2\u6570, \u8bad\u7ec3\u53e6\u4e00\u4e2a             - Solving the two-player game objective - ((63e786ff-afce-47d9-92c9-626bf5e5eef6))             - ((63e78726-8149-4648-8cb9-534c26e34832)) - ((63e78730-bee1-494f-8775-cc1a44382fe0))             - Practical implementation for solving - ((63e78748-a626-4d85-8f35-3d9bb651073c))             - Practical strategy for training the generator G - At the beginning, generated image quality is bad, Generator \u53ef\u4ee5\u5f88\u8f7b\u6613\u7684\u8bc6\u522b\u51fa\u6765, \u96be\u4ee5\u7ee7\u7eed\u8bad\u7ec3\u4e86 - ((63e7879d-c35e-45b4-abca-bf9d841e2daa))             - ((63e787a9-d297-450a-b944-35b2356bfc16)) - ((63e787b7-222e-4a99-9cbe-9c18400d627f)) - ((63e787d3-c2fe-41bb-99e0-91f287757609)) - ((63e787e6-8e9a-4d1e-9b6c-8ff984a05cf9))     - Week6 RNN, LSTM, Attention, Transformers         -         -     - Week7     - Week8 - ## CW   collapsed:: true     - GAN         * Enlarge the model with more layers and parameters         * learning rate scheduler         * In GAN, if the discriminator depends on a small set of features to detect real images, the generator may just produce these features only to exploit the discriminator. The optimization may turn too greedy and produces no long term benefit. In GAN, overconfidence hurts badly. To avoid the problem, we penalize the discriminator when the prediction for any real images go beyond 0.9 (D(real image)&gt;0.9). This is done by setting our target label value to be 0.9 instead of 1.0.         * larger learning rate for the discriminator         * Two Time-Scale Update Rule         *  Add noise to the real and generated images before feeding them into the discriminator.         *  \u5f53GAN\u751f\u6210\u7684\u56fe\u50cf\u4e0d\u591f\u51c6\u786e\u3001\u6e05\u6670\u65f6\uff0c\u53ef\u5c1d\u8bd5\u589e\u52a0\u5377\u79ef\u5c42\u4e2d\u7684\u5377\u79ef\u6838\u7684\u5927\u5c0f\u548c\u6570\u91cf\uff0c\u7279\u522b\u662f\u521d\u59cb\u7684\u5377\u79ef\u5c42\u3002\u5377\u79ef\u6838\u7684\u589e\u5927\u53ef\u4ee5\u589e\u52a0\u5377\u79ef\u7684\u89c6\u91ce\u57df\uff0c\u5e73\u6ed1\u5377\u79ef\u5c42\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4f7f\u5f97\u8bad\u7ec3\u4e0d\u8fc7\u5206\u5feb\u901f\u5730\u6536\u655b\u3002\u589e\u52a0\u5377\u79ef\u6838\u6570\uff08\u7279\u522b\u662f\u751f\u6210\u5668\uff09\uff0c\u53ef\u4ee5\u589e\u52a0\u7f51\u7edc\u7684\u53c2\u6570\u6570\u91cf\u548c\u590d\u6742\u5ea6\uff0c\u589e\u52a0\u7f51\u7edc\u7684\u5b66\u4e60\u80fd\u529b\u3002         *  Real=False, fake=True         *  leaky for generator         *  transfer from VAE         *  \u5728dis\u7684\u5377\u524d\u52a0\u5165\u9ad8\u65af\u566a\u58f0         *  new loss function     - * What didn't work, what worked and what mattered most         * Add noise to the real and generated images before feeding them into the discriminator.         * \u5728\u8f93\u5165\u56fe\u7247\u4e2d\u52a0\u5165\u9ad8\u65af\u566a\u58f0         * dropout</p> </li> </ul> </li> </ul> </li> <li> <p>Are there any tricks you came across in the literature etc. which you suspect would be helpful here</p> <ul> <li>Wasserstein GANs</li> </ul> </li> <li> </li> <li></li> <li></li> <li></li> <li></li> <li></li> <li> </li> <li> </li> <li>5:5</li> <li>\u661f\u671f\u4e94 9:00 - 11:00</li> <li>CW: 2 Tasks, both assessed, Task 1: 40%, Task 2: 60%<ul> <li>Cloud computing, evolved | Paperspace</li> <li>Google Colab</li> <li>\u7b54\u6848\u5728\u4e24\u5468\u540e\u516c\u5e03</li> </ul> </li> <li>Tutorial: \u6bcf\u5468\u90fd\u6709, \u7b54\u6848\u5468\u4e00\u516c\u5e03</li> <li> </li> <li>Supervised vs unsupervised learning, generalisation, overfitting</li> <li>Perceptrons, including deep vs shallow models</li> <li>Stochastic gradient descent and backpropagation</li> <li>Convolutional neural networks (CNN) and underlying mathematical principles</li> <li>CNN architectures and applications in image analysis</li> <li>Recurrent neural networks (RNN), long-short term memory (LSTM), gated recurrent units (GRU)</li> <li>Applications on RNNs in speech analysis and machine translation</li> <li>Mathematical principles of generative networks; variational autoencoders (VAE); generative adversarial networks (GAN)</li> <li>Applications of generative networks in image generation</li> <li>Graph neural networks (GNN): spectral and spatial domain methods, message passing</li> <li>Applications of GNNs in computational social sciences, high-energy physics, and medicine</li> <li>\u2022 Feature extraction, convolutions and CNNs   \u2022 Common Network architectures   \u2022 Automatic parameter optimisation   \u2022 RNNs, LSTMs, GRUs   \u2022 VAEs and GANs   \u2022 GNNs   \u2022 Deep learning programming frameworks \u2022 Applications of deep learning</li> </ul>"},{"location":"ml/Deep%20Learning/#w_outw_in1s12p1k1","title":"\\(W_{out}\u200b=(W_{in\u200b}\u22121)\u00d7s[1]\u22122\u00d7p[1]+k[1]\\)","text":""},{"location":"ml/Deep%20Learning/#pdf-pdf","title":"PDF #PDF","text":"collapsed:: true"},{"location":"ml/Deep%20Learning/#_1","title":"Deep Learning","text":""},{"location":"ml/Deep%20Learning/#info","title":"Info","text":"collapsed:: true"},{"location":"ml/Deep%20Learning/#syllabus","title":"Syllabus","text":"collapsed:: true"},{"location":"ml/Deep%20Learning/#links","title":"Links","text":"<p>collapsed:: true - Scientia - 70010 Deep Learning | Bernhard Kainz - Panopto - \u300a\u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60\u300b \u2014 \u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60 2.0.0 documentation #\u4e66 #\u79d1\u6280\u8d44\u6e90 #ML #DL #tutorial - Rent GPUs | Vast.ai - Cloud computing, evolved | Paperspace #\u79d1\u6280\u8d44\u6e90 #GPU #\u670d\u52a1\u5668 - Browse the State-of-the-Art in Machine Learning | Papers With Code - #\u79d1\u6280\u8d44\u6e90 #GitHub #ML #DL #Academic     - \u2022 GitHub - alievk/avatarify-python: Avatars for Zoom, Skype and other video-conferencing apps.       \u2022 GitHub - CompVis/stable-diffusion: A latent text-to-image diffusion model       \u2022 GitHub - deepfakes/faceswap: Deepfakes Software For All       \u2022 GitHub - Avik-Jain/100-Days-Of-ML-Code: 100 Days of ML Coding       \u2022 GitHub - facebookresearch/detectron2: Detectron2 is a platform for object detection, segmentation and other visual recognition tasks.       \u2022 GitHub - fastai/fastai: The fastai deep learning library       \u2022 GitHub - CMU-Perceptual-Computing-Lab/openpose: OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation       \u2022 GitHub - matterport/Mask_RCNN: Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow     - GitHub - soumith/ganhacks: starter from \"How to Train a GAN?\" at NIPS2016 #GAN     - - -</p>"},{"location":"ml/Machine%20Learning%20for%20Imaging/","title":"Machine Learning for Imaging","text":"<p>tags:: IC, Course, ML, Uni-S10 alias:: MLI Ratio: 7:3 Time: \u661f\u671f\u4e94 14:00 - 16:00</p>"},{"location":"ml/Machine%20Learning%20for%20Imaging/#notes","title":"Notes","text":"<ul> <li>Week2 intro   collapsed:: true<ul> <li>Lecture</li> <li> <p>Tutorial   collapsed:: true</p> <ul> <li>\u57fa\u672c\u7684\u673a\u5668\u5b66\u4e60\u7c7b, \u4ee5regresion\u4e3a\u4f8b</li> <li>```python</li> </ul> <p>class LogisticRegression:       def init(self, lr=0.05, num_iter=1000, add_bias=True, verbose=True):           self.lr = lr           self.verbose = verbose           self.num_iter = num_iter # \u591a\u5c11\u4e2aepoch           self.add_bias = add_bias # \u7528\u4e8e\u52a0\u5165bias           self.weight = np.random.normal(0, 0.01, 50) # \u5982\u679c\u77e5\u9053\u591a\u5c11\u53c2\u6570\u7684\u8bdd, \u53ef\u4ee5\u76f4\u63a5\u521d\u59cb\u5316,            # \u4e5f\u53ef\u4ee5fit\u91cc\u9762\u6839\u636e\u5b9e\u9645feature\u6570\u91cf\u51b3\u5b9a</p> <pre><code>  def __add_bias(self, X):\n      bias = np.ones((X.shape[0], 1)) # (10000, 1) \u591a\u52a0\u4e86\u4e00\u4e2abias \u653e\u5728\u539f\u672c\u7684feature\u4e4b\u540e, \u7528\u4e8e\u6c42\u548c\u7684\u65f6\u5019\u591a\u4e00\u4e2abias\n      return np.concatenate((bias, X), axis=1) # \u591a\u52a0\u4e00\u4e2a\u5217\u7ef4\u5ea6\n\n# \u635f\u5931\u51fd\u6570\n  def __loss(self, h, y):\n      ''' computes loss values '''\n      y = np.array(y,dtype=float)\n      ############################################################################\n      # Q: compute the loss \n      ############################################################################\n      return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean() # \u4e8c\u5206\u7c7b\u95ee\u9898\u7684cross-entropy loss, \u4e24\u4e2a\u5206\u7c7b\u5206\u522b\u7684cross entropy\u76f8\u52a0, \u56e0\u4e3a\u662f\u4e8c\u5206\u7c7b, \u6240\u4ee5\u662fy, 1-y\n    # \u603b\u548c\u4e3a1\u7684\u7c7b\u6982\u7387, \u7528\u4e0across entropy, \u5bf9\u6743\u91cd\u6c42\u5bfc\u7684\u7ed3\u679c\u5c31\u662f1/N (y^ - y)\n\n  def fit(self, X, y):\n      ''' \n      Optimise our model using gradient descent\n      Arguments:\n          X input features\n          y labels from training data\n\n      '''\n      if self.add_bias:\n          X = self.__add_bias(X)\n\n      ############################################################################\n      # Q: initialise weights randomly with normal distribution N(0,0.01)\n      ############################################################################\n      self.theta = np.random.normal(0.0,0.01,X.shape[1])\n\n      for i in range(self.num_iter):\n          ############################################################################\n          # Q: forward propagation\n          ############################################################################\n          z = X.dot(self.theta) # \u4e58\u4e0a\u4e86\u6743\u91cd\u540e\u7684\u7ed3\u679c\uff0c \u6bcf\u4e2afeature \u90fd\u6709\u4e00\u4e2a\u5bf9\u5e94\u7684\u6743\u91cd\uff0c\u4e58\u8d77\u6765\u76f8\u52a0\u5c31\u662f\u67d0\u4e2asample\u7684\u503c\n          h = 1.0 / (1.0 + np.exp(-z)) # activation, \u5f97\u5230\u9884\u6d4b\u6982\u7387\n          ############################################################################\n          # Q: backward propagation\n          ############################################################################\n          # (h - y) / y.size \u662f Loss\u5bf9z\u7684\u6c42\u5bfc; X\u5219\u662fz\u5bf9weight\u7684\u6c42\u5bfc, \u56e0\u4e3az\n          gradient = np.dot(X.T, (h - y)) / y.size # (785,) 11774\u4e2asample\uff0c \u6bcf\u4e2a\u7ef4\u5ea6\u90fd\u662f\u6240\u6709sample\u68af\u5ea6\u7684\u548c\uff0c \u6700\u540e\u9664\u4ee5\u4e86sample \u91cf\n          # update parameters \u68af\u5ea6\u4e0b\u964d\n          self.theta -= self.lr * gradient\n          ############################################################################\n          # Q: print loss\n          ############################################################################\n          if(self.verbose == True and i % 50 == 0): \n              h = 1.0 / (1.0 + np.exp(-X.dot(self.theta)))\n              print('loss: {} \\t'.format(self.__loss(h, y)))\n\n  def predict_probs(self,X):\n      ''' returns output probabilities\n      '''\n      ############################################################################\n      # Q: forward propagation\n      ############################################################################\n      if self.add_bias:\n          X = self.__add_bias(X)\n      z = X.dot(self.theta)\n      return 1.0 / (1.0 + np.exp(-z))\n\n  def predict(self, X, threshold=0.5):\n      ''' returns output classes\n      '''\n      return self.predict_probs(X) &gt;= threshold\n</code></pre> <p>model = LogisticRegression(lr=1e-2, num_iter=1000)   # shuffle data   shuffle_index = np.random.permutation(len(selected_labels))   selected_data, selected_labels = selected_data[shuffle_index], selected_labels[shuffle_index]   model.fit(selected_train_data, selected_train_labels)   train_preds = model.predict(selected_train_data)   logistic_train_acc = 100.0 * (train_preds == selected_train_labels).mean()   <code>- Week3 Classification       id:: 63dacd04-711a-4465-a450-60bf0076ed43       collapsed:: true         - Lecture - feature extraction   collapsed:: true     - pipeline       collapsed:: true         - images-&gt;features-&gt;prediction     - feature extraction and descriptors       collapsed:: true         - \u2013 Intensities           collapsed:: true             - ![image.png](../assets/image_1674664156563_0.png)         - \u2013 Gradient           collapsed:: true             - Gradients: Invariant to absolute illumination         - \u2013 Histogram             - patch-level\u548cimage-level\u90fd\u53ef\u4ee5\u7528hist         - \u2013 SIFT           collapsed:: true             - ![image.png](../assets/image_1674664387996_0.png)         - \u2013 HoG Histogram of gradients           collapsed:: true             - ![image.png](../assets/image_1674664443665_0.png)             - ![image.png](../assets/image_1674664461115_0.png)             - ![image.png](../assets/image_1674664760075_0.png)         - \u2013 SURF           collapsed:: true             - ![image.png](../assets/image_1674664774363_0.png)             - ![image.png](../assets/image_1674664807093_0.png)         - \u2013 BRIEF           collapsed:: true             - ![image.png](../assets/image_1674664845583_0.png)             -         - \u2013 LBP           collapsed:: true             - ![image.png](../assets/image_1674664927209_0.png)         - \u2013 Haar           collapsed:: true             - ![image.png](../assets/image_1674664941210_0.png)             - ![image.png](../assets/image_1674664975489_0.png) - image classification: Ensemble classifiers   collapsed:: true     - classification models       collapsed:: true         - \u2013 Logistic regression D           \u2013 Na\u00efve Bayes G           \u2013 K-nearest neighbors (KNN) G           \u2013 Support vector machines (SVM) D           \u2013 Boosting            \u2013 Decision/Random forests D           \u2013 Neural networks D     - bias and variance       collapsed:: true         - Bias error           \u2013 how much, on average, **predicted values are different from the actual value**.           \u2013 high bias error means we have an under-performing model, which misses important trends.           Accurate\u548cinaccurate         - Variance error           \u2013 how much **predictions made from different samples vary from one other**.           \u2013 high variance error means the model will over-fit and perform badly on any observation beyond training.         - variable \u548cconsistent         - ![image.png](../assets/image_1674665312313_0.png)         -     - Ensemble learning       collapsed:: true         - Aggregate the predictions of a group of predictors (either classifiers or regressors) \u805a\u5408\u4e00\u5806\u9884\u6d4b\u7684\u7ed3\u679c, \u4e00\u5806\u9884\u6d4b\u5668\u5c31\u662f\u4e00\u4e2aensemble         - A learning algorithm which uses multiple models, such as classifiers or experts, is called Ensemble Learning (so called meta-algorithms) \u4e00\u4e2a\u4f7f\u7528\u4e86\u591a\u4e2a\u5206\u7c7b\u5668\u7684\u7b97\u6cd5\u5c31\u53eb\u505aensemble learning \u6216\u8005meta \u7b97\u6cd5         - Types of ensemble learning           collapsed:: true             - \u2022 Homogenous:               \u2013 Combine predictions made from models built from the same ML class               \u540c\u8d28\u7684, \u7528\u4e86\u540c\u4e00\u79cd\u6a21\u578b               \u6bd4\u5982\u5f88\u591aweak learner, \u7528\u4e86\u4e0d\u540csubset of data             - \u2022 Heterogenous:               \u2013 Combine predictions made from models built from different ML classes               \u4e0d\u540c\u7684\u79cd\u7c7b\u7684\u6a21\u578b             - \u2022 Sequential \u2013 base (ML) learners are added one at a time; mislabelled               examples are upweighted each time               \u2013 Exploits the dependence between base learners \u2013 thus learning a complementary set of predictors that reduce the bias and increase the accuracy.               \u5e8f\u5217\u5316\u7684, \u51cf\u5c0fbias, \u56e0\u4e3a\u4e00\u5c42\u4e00\u5c42\u7b5b\u9009, \u4f1a\u975e\u5e38\u7cbe\u786e             - \u2022 Parallel \u2013 many independent base learners are trained               simultaneously and then combined               \u2013 Combines prediction learnt from multiple models run independently averaging away impact of isolated errors - thus reduces variance of the prediction.               \u5e73\u884c\u7684, \u51cf\u5c0f\u65b9\u5dee, \u51cf\u5c0f\u5b64\u7acb\u7684\u9519\u8bef\u7684\u5f71\u54cd         - Decision Stump           collapsed:: true             - ![image.png](../assets/image_1674666376082_0.png)         - Ensemble methods             - \u2022 Voting             - \u2022 Bagging (Bootstrap Aggregation)               collapsed:: true                 - \u5e73\u884c\u7684\u4e00\u79cdensemble learning, \u901a\u8fc7\u4e00\u5806weak learner\u5e73\u884c\u9884\u6d4b, \u6765\u51cf\u5c0fvariance, \u56e0\u4e3a$var(x^-) = var(x)/n$                 - Bootstrapping                   collapsed:: true                     - 1. Take the original dataset E with N training samples                       2. Create T copies  by sampling with replacement                           \u2013 Each copy will be different since some examples maybe repeated while others will not be sampled at all                       3. Train a separate weak learner on each Bootstrap sample                 - Aggregating results                   collapsed:: true                     - ![image.png](../assets/image_1674666835882_0.png)                 - Out-of-Bag (OOB) error                     - \u5982\u4f55\u8bc4\u4ef7error\u5462, \u6211\u4eec\u7528left-out-set\u6765\u4f5c\u4e3avalidation set             - \u2022 Boosting               collapsed:: true                 - \u5e8f\u5217\u5316\u7684, \u4e00\u4e2alearner\u63a5\u7740\u4e00\u4e2alearner, \u5927\u5927\u964d\u4f4ebias                 - Rather than building independent weak learners in parallel and aggregating at end:                   \u2013 build weak learners in serial                   \u2013 but adaptively reweight training data prior to training each new weak learner in order to give a higher weight to previously misclassified examples                 - \u4e0d\u65ad\u91cd\u65b0weight training data \u6765\u7ed9\u65b0\u7684weak learner, \u7ed9misclassified \u66f4\u9ad8\u6743\u91cd                 - ![image.png](../assets/image_1674668364470_0.png)                 - Adaboost - \u4e00\u79cd\u81ea\u9002\u5e94\u7684boosting, adaptive                     - ![image.png](../assets/image_1674668416628_0.png)             - \u2022 Random Forests               collapsed:: true                 - Single Decision Trees are prone to overfitting, but robustness can be significantly increased by combining trees in ensembles                 - Use decision trees for homogenous ensemble learning                 - Random forests form an ensemble of uncorrelated classifiers by                   exploiting random subsampling of the                    \u2013 training data used to build each tree                   \u2013 set of features that are used for selection of the splits                    \u2013 set of feature values that are used for splitting                 - ![image.png](../assets/image_1674686267545_0.png)                 - - NN   collapsed:: true     - Single-layer perceptron       collapsed:: true         - ![image.png](../assets/image_1674686334440_0.png)     - Neural networks (multilayer perceptron)       collapsed:: true         - ![image.png](../assets/image_1674686588881_0.png)     - Neural networks with K classes       collapsed:: true         - one-hot encoding             - ![image.png](../assets/image_1674686671846_0.png)         - softmax             - ![image.png](../assets/image_1674686779670_0.png)         - cross entropy: Distance between probability distributions             - ![image.png](../assets/image_1674686805554_0.png)             - softmax\u548c\u4ea4\u53c9\u71b5\u7684\u68af\u5ea6\u5f88\u7b80\u5355, \u5c31\u662f1/N * (y^ - y)         - back propogation             - ![image.png](../assets/image_1674687354916_0.png)             - \u5f80\u56de\u8d70\u9047\u5230\u5c94\u8def, \u628a\u5f53\u524d\u7684\u68af\u5ea6\u5206\u914d\u51fa\u53bb             - \u5f80\u56de\u8d70\u9047\u5230\u6c47\u5408\u70b9, \u6c47\u5408\u70b9\u7684\u68af\u5ea6\u662f\u5206\u53c9\u51fa\u53bb\u7684\u68af\u5ea6\u548c             - ![image.png](../assets/image_1674687623328_0.png) - Activation and optimisation   collapsed:: true     - ![image.png](../assets/image_1674687714289_0.png)     - Optimizing neural networks: Stochastic gradient descent         - ![image.png](../assets/image_1674687748304_0.png)         - Large batches provide a more accurate estimate of the gradient but with less than linear returns.         - \u8003\u8651memory, \u9650\u5236\u4e86batch\u6700\u5927size         - Small batch sizes can have the effect of regularization (more about regularization later) as it adds noise to the learning process. (The generalisation ability is often best with a batch size of 1, but small batch sizes require small learning rates and can lead to slow convergence).         - \u5c0fbatch\u4f1a\u7ed9\u6700\u4f73\u7684\u6b63\u5219\u5316, \u66f4\u597d\u7684\u666e\u9002\u6027, \u4f46\u662f\u9700\u8981\u66f4\u5c0f\u7684\u5b66\u4e60\u7387, \u5b66\u4e60\u4f1a\u5f88\u6162     - gradient and learning rate         - ![image.png](../assets/image_1674687940771_0.png) - tips and tricks   collapsed:: true     - data augmentation         - ![image.png](../assets/image_1674687996227_0.png)     - Under- and overfitting         - underfitting             - adding more neurons, adding more layers         - overfitting             - adding more regularization     - Regularisation         - Early stopping: Interrupt training when its performance on the validation set starts dropping         - ![image.png](../assets/image_1674688096633_0.png)         - Max-norm             - ![image.png](../assets/image_1674688127437_0.png)         - Dropout             - At every training step, every neuron (input or hidden) has a probability p of being temporarily \u201cdropped out\u201d:     - Weight initialisation         - ![image.png](../assets/image_1674688253094_0.png)     - Normalisation         - Standardization \u6807\u51c6\u5316             - \u51cf\u53bb\u5747\u503c, \u9664\u4ee5\u6807\u51c6\u5dee \u5f97\u5230\u4e00\u4e2a\u5747\u503c\u4e3a\u96f6, \u6807\u51c6\u5dee\u4e3a1\u7684\u5206\u5e03         - \u5f52\u4e00\u5316             - (X-min)/(max-min), \u5f97\u5230\u4e00\u4e2a\u6700\u5927\u503c\u548c\u6700\u5c0f\u503c\u4e3a1\u548c-1\u7684\u7f29\u653e\u5206\u5e03         - Batch normalisation           collapsed:: true             - ![09AF0F45-7CAD-465D-A6AC-65F3F0BEBBDB.jpeg](../assets/09AF0F45-7CAD-465D-A6AC-65F3F0BEBBDB_1674658218903_0.jpeg)             - \u6bcf\u4e00\u4e2afeatuer\u7ef4\u5ea6, \u5bf9\u4e8e\u4e00\u4e2abatch\u91cc\u7684\u6240\u6709sample\u8fdb\u884c\u6807\u51c6\u5316             - \u5728NLP\u4e2d\u4f1a\u53d7\u5236\u4e8e\u6bcf\u4e2asample\u53e5\u5b50\u957f\u5ea6\u4e0d\u4e00\u6837, \u9ad8\u5ea6\u5c31\u4e0d\u4e00\u6837, \u4f1aunstable             - ![image.png](../assets/image_1674688305892_0.png)             - ![image.png](../assets/image_1674688422555_0.png)         - Layer normalisation             - \u5bf9\u6bcf\u4e00\u4e2asample\u7684\u6240\u6709feature\u505a\u6807\u51c6\u5316             - NLP\u4e2d\u4e00\u4e2abatch\u662f\u6709\u5f88\u591a\u4e0d\u540c\u957f\u5ea6\u7684\u53e5\u5b50\u7ec4\u6210\u7684, \u6bcf\u4e2asample\u53ef\u4ee5\u8ba4\u4e3a\u662f\u591a\u4e2a\u8bcd\u8bed\u7684\u7ec4\u5408, \u8bcd\u8bed\u7684\u6570\u91cf\u4e0d\u4e00\u5b9a             - NLP\u4e2d\u5c31\u662f\u5bf9\u67d0\u4e2a\u8bcd\u8bed\u7684\u6574\u4e2avector\u505a\u6807\u51c6\u5316 - CNN   collapsed:: true     - ![image.png](../assets/image_1674688501664_0.png)     - \u8ba1\u7b97         - Assuming padding of size p, no stride             - $o_j=i_j-k_j+2p+1$             - \u4f8b\u5982\u539f\u56fe\u662f5, kernal\u662f4, padding\u662f2, output\u5c31\u662f5-4+4+1 = 6         - Assuming padding of size p, stride $s_j$             - $o_j=[(i_j-k_j+2p)/s_j]+1$             - \u4f8b\u5982\u539f\u56fe\u662f5, kernal\u662f3, padding\u662f1, stride\u662f2, output\u5c31\u662f (5 \u2212 3 + 2)/2 + 1 = 3     - \u8f93\u5165\u56fe\u6709\u4e09\u4e2achannel, \u8fd9\u4e2a\u65f6\u5019\u6211\u4eec\u4e00\u4e2afilter\u4e5f\u5f97\u6709\u4e09\u4e2achannel\u548c\u4ed6\u4eec\u4e00\u4e00\u5bf9\u5e94, \u5982\u679c\u6211\u4eec\u6709\u56db\u4e2afilter, \u90a3\u4e48\u6700\u540e\u751f\u6210\u7684\u8f93\u51fa\u4e5f\u6709\u56db\u4e2a     - pooling:         - \u2022 Reduce size (memory) of deeper layers           \u2022 Invariance to small translations           \u2022 Contraction forces network to learn high-level features     - upsampling         - \u628a\u539f\u56fe\u5148\u586b\u4e0a0, \u518d\u7528\u5377\u79ef\u6269\u5927         - ![image.png](../assets/image_1674689023013_0.png)         - Tutorial           collapsed:: true - Haar features     - quickly computed by integral images     - Computational complexity of computing haar features from integral images is constant - End-to-end learning     - model \u80fd\u591f\u76f4\u63a5\u4ece\u56fe\u5230\u7ed3\u679c     - \u4e0d\u9002\u5408\u5c0f\u6570\u636e\u96c6, \u56e0\u4e3a\u9700\u8981\u5927\u6570\u636e\u6765generalise model     - \u4e0d\u9700\u8981hand crafted features     - not efficient in terms of model parameters, \u56e0\u4e3a\u6211\u4eec\u9700\u8981\u8db3\u591f\u590d\u6742\u7684\u6a21\u578b     - models are not human readable - Regularisation     - L1, L2         -     - dropout     - small batch size + SGD     - data augmentation     - Week4 Image Segmentation       collapsed:: true         - Lecture - Semantic Segmentation     - In semantic segmentation, a segmented region is also assigned a       semantic meaning - Application   collapsed:: true     - conducting quantitative analyses, e.g. measuring the volume of the       ventricular cavity.     - determining the precise location and extent of an organ or a certain type of tissue, e.g. a tumour, for treatment such as radiation therapy.     - creating 3D models used for simulation, e.g. generating a model of an abdominal aortic aneurysm for simulating stress/strain distributions. - challenges   collapsed:: true     - noise     - partial volume effects       collapsed:: true         - coarse sampling, \u91c7\u6837\u5bfc\u81f4\u7684\u4e0d\u8db3         - ![image.png](../assets/image_1675439370714_0.png)     - intensity inhomogeneities, anisotropic resolution       collapsed:: true         - \u4e0d\u540c\u8d28\u7684intensity\u548c\u4e0d\u540c\u65b9\u5411\u7684resolution\u7684\u4e0d\u4e00\u81f4         - ![image.png](../assets/image_1675439440151_0.png)         - ![image.png](../assets/image_1675439452285_0.png)     - imaging artifacts         - \u4e00\u4e9b\u5947\u602a\u7684\u4e1c\u897f\u5bfc\u81f4\u7684\u602a\u4e1c\u897f     - limited contrast         - \u56fe\u7247\u7684\u5bf9\u6bd4\u5ea6\u4e0d\u591f\u9ad8     - morphological variability,         - \u5f62\u6001\u5b66\u4e0a\u5668\u5b98\u90fd\u957f\u5f97\u4e0d\u4e00\u6837, \u5148\u9a8c\u77e5\u8bc6\u5f88\u96be\u5229\u7528\u597d - Evaluating Image Segmentation     - ground truth         - Reference or standard against a method can be compared, e.g. the optimal transformation, or a true segmentation boundary, \u4e0d\u73b0\u5b9e         - \u53ef\u80fd\u4eba\u4eec\u4f1amake up phantoms     - gold standard         - Experts manually segment         - ![image.png](../assets/image_1675439700146_0.png) - performance measures     - ![image.png](../assets/image_1675439726743_0.png)     - Confusion matrix #metrics         - ![image.png](../assets/image_1675439766890_0.png)     - Accuracy, Precision, Recall (sensitivity), Specificity #metrics         - ![image.png](../assets/image_1675439825709_0.png)         - F1 score #metrics             - ![image.png](../assets/image_1675440141000_0.png)         - Overlap             - Jaccard Index(IoU)             - ![image.png](../assets/image_1675440231460_0.png)             - Dice's Coefficient (Sorensen Index) (DSC) (Dice Similarity Coefficient)               collapsed:: true                 - ![image.png](../assets/image_1675440410629_0.png)                 - ![image.png](../assets/image_1675441381926_0.png)                 - ![image.png](../assets/image_1675441399747_0.png)         - Volume similarity           collapsed:: true             - ![image.png](../assets/image_1675441496345_0.png)         - Hausdorff distance             - ![image.png](../assets/image_1675441591113_0.png)             - A\u4e0a\u6bcf\u4e2a\u70b9\u627e\u5230B\u4e0a\u79bb\u8fd9\u4e2a\u70b9\u7684\u6700\u77ed\u8ddd\u79bb, \u8fd9\u4e9b\u8ddd\u79bb\u4e4b\u4e2d\u6700\u957f\u7684\u5c31\u662f\u6211\u4eec\u6c42\u7684\u8fd9\u4e2a\u8ddd\u79bb         - Symmetric average surface distance             - ![image.png](../assets/image_1675441634954_0.png)             - A\u5230B\u7684\u6700\u77ed\u8ddd\u79bb\u7684\u5e73\u5747\u6570         -     - Segmentation Algorithms &amp; Techniques         - \u25aa Intensity-based segmentation           collapsed:: true           \u25aa e.g., thresholding             - \u7528\u4e8e\u5f88\u660e\u663e\u80fd\u533a\u5206\u4e3b\u4f53\u548c\u80cc\u666f\u7684\u60c5\u51b5             - UL Thresholding: Select a lower and upper threshold             - Advantages               \u25aa simple                \u25aa fast             - Disadvantages               \u25aaregions must be homogeneous and distinct               \u25aadifficulty in finding consistent thresholds across images                \u25aaleakages, isolated pixels and \u2018rough\u2019 boundaries likely         - \u25aa Region-based           collapsed:: true           \u25aa e.g., region growing             - Start from (user selected) seed point(s), and grow a region according to an intensity threshold, \u4ece\u4e00\u4e2a\u70b9\u5f00\u59cb\u4e07\u7269\u751f\u957f             - Advantages               \u25aarelatively fast               \u25aayields connected region (from a seed point)             - Disadvantages               \u25aaregions must be homogeneous \u25aaleakages and \u2018rough\u2019 boundaries likely \u25aarequires (user) input for seed points         - \u25aa Graph-based segmentation           collapsed:: true           \u25aa e.g., graph cuts             - ![image.png](../assets/image_1675532483329_0.png)             - ![image.png](../assets/image_1675532497544_0.png)             - ![image.png](../assets/image_1675532523722_0.png)             - Advantages               \u25aa accurate               \u25aareasonably efficient, interactive             - Disadvantages               \u25aasemi-automatic, requires user input                \u25aadifficult to select tuning parameters         - \u25aa Active contours           \u25aa e.g., level sets         - \u25aa Atlas-based segmentation           collapsed:: true           \u25aa e.g., multi-atlas label propagation             - \u57fa\u4e8e\u56fe\u8c31\u7684\u56fe\u50cf\u5206\u5272             - \u6240\u8c13\u201c\u5730\u56fe\u96c6\u201d\uff0c\u76f4\u89c2\u6765\u8bf4\uff0cAtlas \u5c31\u662f\u4eba\u5de5\u6807\u8bb0\u5b8c\u5907\u7684\u6570\u636e\u5e93\u3002\u6bd4\u5982 BrainWeb \u7684 Atlas\uff1a\u5728\u4e09\u7ef4\u8111\u90e8CT\u6570\u636e\u4e2d\u533b\u751f\u6807\u6ce8\u5b8c\u5907\u7684\u5404\u79cd\u8111\u90e8\u7ed3\u6784\uff0c\u5982\u7070\u8d28\u3001\u767d\u8d28\u3001\u6d77\u9a6c\u7b49\u7b49\u7ed3\u6784\u3002             - \u5c06 testing image \u548c\u5f53\u524d Atlas \u5185\u7684\u6570\u636e\u8fdb\u884c\u914d\u51c6\uff0c\u7136\u540e\u7528 Label Propagation \u65b9\u6cd5\u5c06 Atlas \u6570\u636e\u7684 Label \u901a\u8fc7 registration mapping corresponding \u5173\u7cfb\u4f20\u9012\u5230 testing image \u4e2d\uff0c\u4ece\u800c\u5b8c\u6210 testing image \u7684\u5206\u5272\u4efb\u52a1\u3002             - Segmentation using registration             - ![image.png](../assets/image_1675533840911_0.png)             - \u901a\u8fc7\u4ece\u5730\u56fe\u96c6\u91cc\u4eba\u5de5\u6807\u6ce8\u7684segmentation\u4e0e\u6837\u672c\u8fdb\u884c\u6620\u5c04, \u8fd9\u4e2a\u8fc7\u7a0b\u53eb\u505aregistration             - Multi-Atlas Label Propagation             - ![image.png](../assets/image_1675534069463_0.png)             - Advantages               \u25aarobust and accurate (like ensembles)                \u25aayields plausible segmentations               \u25aafully automatic             - Disadvantages               \u25aacomputationally expensive               \u25aacannot deal well with abnormalities                \u25aanot suitable for tumour segmentation         - \u25aa Learning-based segmentation           collapsed:: true           \u25aa e.g., random forests, convolutional neural networks             - ![image.png](../assets/image_1675534349869_0.png)             - \u63d0\u53d6\u56fe\u50cf\u4e2d\u7684patches, \u7528\u6811\u4e2d\u7684\u5224\u65ad\u65b9\u6cd5\u6765\u5224\u65ad             - Advantages               \u25aaensemble classifiers are robust and accurate                \u25aacomputationally efficient               \u25aafully automatic               Disadvantages               \u25aashallow model, no hierarchical features                \u25aano guarantees on connectedness     - CNNs for Image Segmentation       collapsed:: true         - Segmentation via Dense Classification         - \u5229\u7528\u5230\u4e863D\u5377\u79ef         - ![image.png](../assets/image_1675535246807_0.png)         - ![image.png](../assets/image_1675535121849_0.png)         - \u62bd\u53d6\u4e00\u4e2apatch, \u901a\u8fc7\u6574\u4f53\u4fe1\u606f\u6765\u5b66\u4e60\u786e\u8ba4\u4e2d\u592e\u50cf\u7d20\u7684\u7c7b\u522b         - \u901a\u8fc7sliding window \u6765\u904d\u5386\u56fe\u7247, \u627e\u5230\u6bcf\u4e2a\u70b9\u7684\u7c7b\u522b, \u975e\u5e38\u7684\u4e0d\u9ad8\u6548         - Fully Connected to Convolution: \u7528conv\u66ff\u4ee3fc             - ![image.png](../assets/image_1675535590111_0.png)             - \u901a\u8fc7\u5171\u4eab\u6743\u91cd, \u5927\u5927\u51cf\u5c11\u4e86\u53c2\u6570\u91cf             - ![image.png](../assets/image_1675535748846_0.png)             - 1x1\u662fclass label, \u4f46\u662f9x9\u8fd8\u662ffeature map     - Encoder-Decoder Networks       collapsed:: true         - ![image.png](../assets/image_1675535864049_0.png)     - U-Net       collapsed:: true         - ![image.png](../assets/image_1675535884207_0.png)         - \u901a\u8fc7\u539f\u56fe\u8865\u5145\u539f\u4fe1\u606f, \u4e0b\u91c7\u6837\u518d\u4e0a\u91c7\u6837\u5b66\u4e60\u5982\u4f55\u4ea7\u751fsegmentation map     - Upsampling and transpose convolution       collapsed:: true         - ![image.png](../assets/image_1675536138599_0.png)         - ![image.png](../assets/image_1675536154098_0.png)         - ![image.png](../assets/image_1675536226508_0.png)     - Going Deeper       collapsed:: true         - Just adding more layers is inefficient (too many parameters)            \u25aaIdea: Use only layers with small kernels     - Multi-scale processing       collapsed:: true         - How can we make the network to \u201csee\u201d more context           \u25aaIdea: Add more pathways which process downsampled images         - \u5982\u4f55\u8ba9\u7f51\u7edc\u770b\u5230\u66f4\u591a\u4e0a\u4e0b\u6587\u4fe1\u606f\u5462, \u589e\u52a0pathway, \u4e24\u6761\u8def\u7ed9\u795e\u7ecf\u7f51\u7edc\u8d70, \u4e00\u4e2a\u9ad8\u5206\u8fa8\u7387\u7684\u5927\u56fe, \u4e00\u4e2a\u4f4e\u5206\u8fa8\u7387\u7684\u5c0f\u56fe, concat\u8d77\u6765\u7684\u65f6\u5019\u5c31\u6709\u4e86\u66f4\u5927\u7684\u89c6\u91ce, \u65b0\u589e\u7684\u4fe1\u606f\u589e\u5f3a\u4e86localisation capability         - ![image.png](../assets/image_1675536363791_0.png)         - ![image.png](../assets/image_1675536435030_0.png)         - \u4f8b\u5982\u4e0a\u9762\u7684\u9ad8\u5206\u8fa8\u7387\u7684\u89e3\u5256\u5b66\u4fe1\u606f, \u548c\u4e0b\u9762\u7684\u4f4d\u7f6e\u4fe1\u606f     - Week5 Image Registration, \u5750\u6807\u7cfb, transformation, intensity-based image registration, NN for regi       collapsed:: true         - Image Registration: \u56fe\u50cf\u914d\u51c6, \u5c31\u662f\u5c06\u4e0d\u540c\u7684\u7167\u7247align\u8d77\u6765, \u5339\u914d\u8d77\u6765\u5bf9\u5e94\u7684\u4f4d\u7f6e - Establish spatial correspondences between images - \u5176\u5b9e\u5c31\u662f\u627e\u5230\u4e24\u5f20\u7167\u7247\u5bf9\u5e94\u7684\u70b9, \u7b97\u5f97transformation\u7684\u65b9\u5f0f, \u8fdb\u884c\u5339\u914d\u5bf9\u5e94, \u4f8b\u5982\u4ee5\u524d\u5b66\u8fc7\u7684image warping, \u5c31\u662f\u626d\u66f2\u56fe\u7247\u8ba9\u4e24\u5f20\u7167\u7247\u62fc\u63a5\u8d77\u6765, \u90a3\u4e2a\u65f6\u5019\u7528\u7684\u662f\u7279\u5f81\u70b9\u5bf9\u5e94\u7b97\u5f97fundamental matrix, \u73b0\u5728\u7528\u7684\u65b9\u6cd5\u53ef\u80fd\u503e\u5411\u4e8e\u7528intensity \u5bf9\u5e94, \u7136\u540e\u7b97\u4e0d\u76f8\u4f3c\u5ea6, \u6162\u6162\u8c03\u6574. \u76f8\u5bf9\u4e8e\u7ebf\u6027\u7684transformation, \u8fd9\u91cc\u8fd8\u7528\u5230\u4e86deformation, \u4f1a\u6709\u66f4\u9ad8\u65e0\u9650\u7ef4\u5ea6\u7684degree of freedom         - Coordinate system           collapsed:: true - ![image.png](../assets/image_1676133104628_0.png)         - Transformation Models           collapsed:: true - ![image.png](../assets/image_1676133128197_0.png) - ![image.png](../assets/image_1676133167133_0.png) -     - ![image.png](../assets/image_1676133183172_0.png) - Free-form Deformation     - ![image.png](../assets/image_1676134427405_0.png)         - Applications           collapsed:: true - Satellite Imaging, Point Correspondences, Panoramic Image Stitching, Optical Flow - ![image.png](../assets/image_1676134672345_0.png)         - Intra-subject Registration           collapsed:: true - ![image.png](../assets/image_1676134707810_0.png) - \u540c\u4e00\u4e2a\u7269\u4f53\u7684registration, \u5bf9\u5e94\u597d\u4ee5\u540e\u5c31\u80fd\u627e\u5230\u4e0d\u4e00\u6837\u7684\u5730\u65b9, \u5c31\u6709\u53ef\u80fd\u662f\u75c5\u53d8\u7ec4\u7ec7         - Inter-subject Registration           collapsed:: true - ![image.png](../assets/image_1676134746307_0.png) - \u76ee\u6807\u662f\u6784\u5efa\u4e00\u4e2a\u5e73\u5747\u7684atlas, \u6211\u4eec\u6709\u4e00\u5806subjects, \u53d6\u81ea\u4e0d\u540c\u7684\u4eba\u7684\u8111\u5b50\u7684MR - \u9996\u5148\u6211\u4eec\u5bf9\u8fd9\u4e9b\u56fe\u7247\u5927\u81f4\u5bf9\u9f50, \u8fdb\u884c\u4e00\u4e2aavg\u7684\u64cd\u4f5c, \u4f5c\u4e3atarget image - \u7136\u540e\u7528\u5176\u4ed6\u7684\u56fe\u7247\u5bf9\u8fd9\u4e2atarget\u8fdb\u884cregistration, \u5bf9\u9f50\u540e\u91cd\u65b0avg, \u83b7\u5f97\u66f4\u6e05\u6670\u7684\u56fe\u7247, \u4e0d\u65ad\u91cd\u590d - Iterative registration    1) Rigid   2) Affine   3) Nonrigid         - Segmentation using Registration - Atlas\u662f\u5730\u56fe\u96c6, \u5df2\u7ecf\u624b\u52a8\u6807\u6ce8\u4e86label - \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5bf9atlas\u8fdb\u884cregistration, map\u5230\u60f3\u8981\u8fdb\u884c\u65b0\u7684\u6807\u6ce8\u7684\u56fe\u7247\u4e0a, \u5c31\u53ef\u4ee5propagatelabel\u5230\u9700\u8981segmentation\u7684\u56fe\u7247\u4e0a\u53bb\u4e86         - Multi-Atlas Label Propagation           collapsed:: true - ![image.png](../assets/image_1676135510352_0.png) - \u7528\u591a\u4e2aatlas\u5bf9\u76ee\u6807\u56fe\u7247\u8fdb\u884c\u914d\u51c6, \u5f97\u5230\u5bf9\u5e94\u7684segmentation label, \u7136\u540efusion \u5f97\u5230\u4e00\u4e2a\u66f4\u51c6\u7684\u7ed3\u679c         - Intensity-based Registration - Estimation of transformation parameters is driven by the appearance of the images - Images are registered when they appear similar - ![image.png](../assets/image_1676135634245_0.png) - \u5c31\u662f\u6839\u636e\u50cf\u7d20\u7684\u503c, \u8fdb\u884c\u76f8\u4f3c\u5ea6\u68c0\u67e5, \u76f8\u4f3c\u5ea6\u6700\u9ad8\u7684\u65f6\u5019\u5c31\u662f\u914d\u51c6\u51c6\u786e\u7387\u9ad8\u7684\u65f6\u5019 - Objective function (cost, energy)   collapsed:: true     - ![image.png](../assets/image_1676135773999_0.png)     - moving image \u901a\u8fc7transformation\u5f97\u5230\u7684\u56fe\u7247, \u4e0e\u76ee\u6807\u56fe\u7247\u7684\u4e0d\u76f8\u4f3c\u5ea6\u5c31\u662f\u8fd9\u4e2atransformation\u7684cost - Optimisation problem   collapsed:: true     - ![image.png](../assets/image_1676135831878_0.png) - Mono-modal vs Multi-modal     - Mono-modal registration       \u25aa Image intensities are related by a (simple) function       \u50cf\u7d20\u503c\u53ef\u80fd\u53ea\u548c\u5355\u4e00\u7b80\u5355\u7684function \u6709\u5173\u7cfb     - Multi-modal registration       \u25aa Image intensities are related by a complex function or statistical relationship       \u6709\u590d\u6742\u7684\u51fd\u6570\u5173\u7cfb, \u6216\u8005\u6709\u7edf\u8ba1\u5173\u7cfb - (Dis)similarity Measures     - Sum of squared differences (SSD)         - ![image.png](../assets/image_1676135932507_0.png)         - \u5047\u8bbe\u7684\u662fidentity relationship, \u7528\u4e8emono model, \u4f8b\u5982CT-CT     - Sum of absolute differences (SAD)         - ![image.png](../assets/image_1676135958621_0.png)         - \u5047\u8bbe\u7684\u662fidentity relationship, \u7528\u4e8emono model, \u4f8b\u5982CT-CT     - Correlation coefficient (CC)         - ![image.png](../assets/image_1676135998373_0.png)         - \u5047\u8bbe\u7684\u662flinear relationship, \u7528\u4e8emono model, \u4f8b\u5982MR-MR     - Statistical relationship         - ![image.png](../assets/image_1676136107566_0.png)         - \u5bf9\u5e94intensity\u7684\u70b9\u4f1a\u5f62\u6210\u96c6\u7fa4\u5bf9\u5e94\u7684mapping \u5173\u7cfb         - ![image.png](../assets/image_1676136145591_0.png)         - \u5355modal\u5c31\u662fy=x\u7684\u5173\u7cfb         - ![image.png](../assets/image_1676136167495_0.png)         - multi-modal \u6709\u4e00\u4e9b\u5176\u4ed6\u7684\u5bf9\u5e94\u5173\u7cfb         - ![image.png](../assets/image_1676136229289_0.png)         - [[Joint Entropy]]\u53ef\u4ee5\u5bf9\u5bf9\u5e94\u70b9\u7684\u4e00\u8d77\u51fa\u73b0\u6982\u7387\u5efa\u6982\u7387\u6a21\u578b, \u8fd9\u4e2a\u7cfb\u7edf\u8d8a\u7a33\u5b9a, \u8fd9\u4e2a\u6982\u7387\u6a21\u578b\u5bf9\u5e94\u7684\u71b5\u80af\u5b9a\u8d8a\u5c0f         - ![image.png](../assets/image_1676136293852_0.png)         - \u901a\u8fc7\u5230\u8003\u8651\u672c\u8eab\u50cf\u7d20\u70b9\u7684\u71b5, \u53ef\u4ee5\u5f97\u5230mutual information, \u4e00\u5f20\u56fe\u80fd\u88ab\u53e6\u4e00\u5f20\u56fe\u63cf\u8ff0\u7684\u53ef\u80fd\u6027         - ![image.png](../assets/image_1676136343015_0.png)         - [[NMI]]: \u66f4\u9ad8\u9636\u4e14\u5b9e\u7528\u7684\u65b9\u6cd5\u662fNormalised mutual information, \u7528joint entropy\u4f5c\u4e3anormalisation         - ![image.png](../assets/image_1676136418948_0.png)         - \u5efa\u7acb\u7684\u5047\u8bbe\u662f\u7edf\u8ba1\u5b66\u4e0a\u7684\u5173\u7cfb, \u56e0\u6b64\u53ef\u4ee5\u7528\u4e8e\u591a\u6a21\u6001\u4e86, \u4e0d\u662f\u90a3\u4e48\u9002\u7528\u4e8e\u5355\u6a21\u6001, \u53ef\u80fd\u65e0\u6cd5\u53d1\u73b0\u60f3\u8981\u7684difference     - Multi-scale, hierarchical Registration       collapsed:: true         - \u914d\u51c6\u8fc7\u7a0b\u4e2d\u4f1a\u51fa\u73b0\u7684\u95ee\u9898           collapsed:: true             - Image Overlap: \u4ec5\u5bf9overlapping \u90e8\u5206\u505a\u76f8\u4f3c\u5ea6\u68c0\u67e5, \u91cd\u5408\u90e8\u5206\u8981\u8db3\u591f\u5927\u624d\u597d\u7528             - Capture Range: \u5728capture range\u4e2d\u624d\u662f\u6709\u6548\u7684\u76f8\u4f3c\u5ea6\u68c0\u67e5, \u5206\u79bb\u5f00\u6765\u4e86\u6bd4\u5982\u5339\u914d\u7a7a\u6c14, \u90a3\u80af\u5b9a\u90fd\u662f100%match                 - ![image.png](../assets/image_1676136663474_0.png)             - Local Optima                 - ![image.png](../assets/image_1676136673974_0.png)         - \u7528\u4e8e\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898, \u4f7f\u7528\u591a\u5c3a\u5ea6\u7684\u9ad8\u65af\u91d1\u5b57\u5854             - \u25aaSuccessively increase degrees of freedom                \u25aaGaussian image pyramids             - ![image.png](../assets/image_1676136735499_0.png)     - Interpolation         - ![image.png](../assets/image_1676136769578_0.png)         - Translate \u540e\u7684\u503c\u4e0d\u662f\u4e00\u4e00\u5bf9\u5e94\u7684, \u9700\u8981\u5dee\u503c\u627e\u5230     - Registration as an Iterative Process         - ![image.png](../assets/image_1676136804437_0.png)         - Strategies: \u25aa Gradient-descent \u25aaStochastic optimisation \u25aaBayesian optimisation \u25aaDiscrete optimisation \u25aaConvex optimisation           \u25aa Downhill-simplex     - -         - Registration with Neural Networks           collapsed:: true - FlowNet: Learning Optical Flow with Convolutional Networks     - ![image.png](../assets/image_1676136881616_0.png)     - ![image.png](../assets/image_1676136889351_0.png)     - ![image.png](../assets/image_1676136898878_0.png) - FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks     - ![image.png](../assets/image_1676136931367_0.png)     - \u901a\u8fc7\u4e00\u4e2a\u4e2a\u7f51\u7edcblock, correct\u524d\u9762\u7684\u7ed3\u679c - Nonrigid Image Registration Using Multi-scale 3D CNNs     - ![image.png](../assets/image_1676136997348_0.png)     - \u591a\u5c3a\u5ea6\u7684\u5e94\u7528 - Spatial Transformer Networks (STN)     - [Spatial Transformer Networks Tutorial \u2014 PyTorch Tutorials 1.13.1+cu117 documentation](https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html) #PyTorch #Python #tutorial #ML     - STN\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u53ef\u5b66\u4e60\u7684\u7a7a\u95f4\u8f6c\u6362\u6a21\u5757\uff0c\u5b83\u53ef\u4ee5\u4f7f\u6a21\u578b\u5177\u6709\u7a7a\u95f4\u4e0d\u53d8\u6027\u3002\u8fd9\u4e2a\u53ef\u5fae\u5206\u6a21\u5757\u53ef\u4ee5\u63d2\u5165\u5230\u73b0\u6709\u7684\u5377\u79ef\u7ed3\u6784\u4e2d\uff0c\u4f7f\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u5728Feature Map\u672c\u8eab\u7684\u6761\u4ef6\u4e0b\u81ea\u52a8\u5730\u5bf9\u7279\u5f81\u8fdb\u884c\u7a7a\u95f4\u53d8\u6362\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u989d\u5916\u7684\u8bad\u7ec3\u76d1\u7763\u6216\u4f18\u5316\u8fc7\u7a0b\u7684\u4fee\u6539\u3002     - `Localisation net`\u6a21\u5757\u901a\u8fc7CNN\u63d0\u53d6\u56fe\u50cf\u7684\u7279\u5f81\u6765\u9884\u6d4b\u53d8\u6362\u77e9\u9635\u03b8 \\theta\u03b8     - `Grid generator`\u6a21\u5757\u5c31\u662f\u5229\u7528`Localisation net`\u6a21\u5757\u56de\u5f52\u51fa\u6765\u7684\u03b8 \\theta\u03b8\u53c2\u6570\u6765\u5bf9\u56fe\u7247\u4e2d\u7684\u4f4d\u7f6e\u8fdb\u884c\u53d8\u6362\uff0c\u8f93\u5165\u56fe\u7247\u5230\u8f93\u51fa\u56fe\u7247\u4e4b\u95f4\u7684\u53d8\u6362\uff0c\u9700\u8981\u7279\u522b\u6ce8\u610f\u7684\u662f\u8fd9\u91cc\u6307\u7684\u662f\u56fe\u7247\u50cf\u7d20\u6240\u5bf9\u5e94\u7684\u4f4d\u7f6e\u3002     - `Sampler`\u5c31\u662f\u7528\u6765\u89e3\u51b3`Grid generator`\u6a21\u5757\u53d8\u6362\u51fa\u73b0\u5c0f\u6570\u4f4d\u7f6e\u7684\u95ee\u9898\u7684\u3002\u9488\u5bf9\u8fd9\u79cd\u60c5\u51b5\uff0c`STN`\u91c7\u7528\u7684\u662f`\u53cc\u7ebf\u6027\u63d2\u503c(Bilinear Interpolation)`     - ![image.png](../assets/image_1676137528984_0.png) - End-to-End Unsupervised Deformable Image Registration with a CNN     - ![image.png](../assets/image_1676137622449_0.png) - An Unsupervised Learning Model for Deformable Image Registration     - ![image.png](../assets/image_1676137665251_0.png) - Supervised below   collapsed:: true     - VoxelMorph: A Learning Framework for Deformable Image Registration         - ![image.png](../assets/image_1676137693469_0.png)     - Learning Conditional Deformable Templates         - ![image.png](../assets/image_1676137724604_0.png) - Template Registration   collapsed:: true     - TeTrIS: Template Transformer Networks for Image Segmentation         - ![image.png](../assets/image_1676137748647_0.png)         - ![image.png](../assets/image_1676137761687_0.png) - Structure-Guided Registration     - ISTNs: Image-and-Spatial Transformer Networks         - ![image.png](../assets/image_1676137792291_0.png)         - ![image.png](../assets/image_1676137866601_0.png)         - ![image.png](../assets/image_1676137875129_0.png)         - ![image.png](../assets/image_1676137884096_0.png)     - Atlas-ISTNs: Joint Segmentation, Registration and Atlas Construction       collapsed:: true         - ![image.png](../assets/image_1676137916670_0.png)         - ![image.png](../assets/image_1676137932553_0.png)         - ![image.png](../assets/image_1676137950381_0.png)         -     - Week6     - Week7     - Week8     - \u8003\u5377         - 2021           collapsed:: true - logistic regression parameters \u8ba1\u7b97 includes raw pixels and a bias; e.g. 64 x 64 +1 - Receptive Field \u8ba1\u7b97   collapsed:: true     - \u4ece\u5916\u5230\u5185, \u4ece\u4e0a\u5230\u4e0b\u5199.     - \u6700\u5916\u5c42\u7684conv\u7684kernel\u5927\u5c0f, \u5c31\u662f\u4e0b\u4e00\u5c42conv\u9700\u8981\u751f\u6210\u7684\u70b9\u6570, \u4f8b\u5982\u6700\u5916\u5c42\u662f3     - \u6839\u636e\u4e0b\u4e00\u5c42\u7684kernel\u548cstride, \u753b\u753b\u56fe\u7b97\u4e00\u7b97\u627e\u5230\u4e0b\u4e00\u5c42\u9700\u8981\u7684\u50cf\u7d20\u6570\u91cf, \u4f8b\u5982\u7b2c\u4e8c\u5c42\u662f2 stride2, \u90a3\u4e48\u7b2c\u4e8c\u5c42\u7684conv\u751f\u6210\u6700\u5916\u5c42\u76843\u6240\u9700\u7684\u50cf\u7d20\u6570\u91cf\u5c31\u662f2*3=6     - \u518d\u4e3e\u4e2a\u4f8b\u5b50, \u7b2c\u4e09\u5c42\u662f3 stride1, \u56e0\u4e3astride\u662f1, \u6240\u4ee5\u9664\u4e86\u524d\u4e09\u4e2a\u50cf\u7d20\u4ee5\u5916, \u540e\u9762\u4f9d\u6b21\u589e\u52a0, \u53733 + 5 = 8     - \u8fd9\u6837\u5b50\u7b97\u5230\u6700\u540e\u4e00\u5c42\u5373\u53ef     - $RF_N=1,\\  RF_{N-1}= (RF_N - 1)\\times stride_n + k_n$ - Convolution &amp; pooling \u8ba1\u7b97     - $o=[(i-k+2p)/s]+1$ - conv     - $o = (i-1) \\times s - 2p + k$ - transposed conv - Decision boundaries \u8ba1\u7b97 - Accuracy, precision (positive predictive value, PPV), recall (sensitivity, hit rate, true positive rate, TPR), specificity (recall for neg, true negative rate, TNR), F-beta     - F-\\beta = (1 + \\beta^2) * (Precision * Sensitivity) / (\\beta^2 * Precision + Sensitivity) - SSD, SAD, CC, and NMI - Harr features: efficient using integral images within constant time; suitable for extracting structures like edges and corners, not general features, cannnot approximate any filters via convolution - Options for regularization in a neural network:     - L1 &amp; L2 penalty on the weights     - Dropout     - Stochastic Gradient Descent with small batch sizes     - Data augmentation - YOLO     - architecture and how it works       id:: 64120319-9f33-4806-90c3-6bd66a5caecc     - how to avoid computational difficulties from 1000 classes         - one could separate the object detection from the object classification. For example, one trains a generic object detector to detect regions of interest (as in R-CNN) and then a separate object classifier. - L1 &amp; L2 loss - Perceptual loss #dj w7     - Computes loss on the output \ud835\udf03 of an intermediate layer \ud835\udc59 of a pre-trained network:     - \u8ba1\u7b97\u4e2d\u95f4\u5c42\u7684feature map\u7684\u533a\u522b, \u7528\u7684L2 - Interpretability and Trustworthy ML #dj w8     - interpret an existing ML model: saliency maps (\u7a81\u51fa\u91cd\u70b9\u4fe1\u606f)         - Gradient (backpropagation) f&gt;0         - Deconvolution R&gt;0         - Guided backpropagation f &amp; R &gt;0         - f is activated feature map; R is backpropogated value from l+1 to l     - Adversarial attacks         - Perturbation             - \u4e3a\u4e86\u8ba9\u6270\u52a8\u8db3\u591f\u5c0f, \u6211\u4eec\u9650\u5236\u4e86eta\u7684\u5927\u5c0f\u4e3aepsilon, $\\eta = \\epsilon \u00b7 sign(\\theta)$             - \u5f53\u6743\u91cdtheta\u4e58\u4e0a\u8fd9\u4e2a\u6270\u52a8\u7684\u65f6\u5019, \u6700\u7ec8\u7684\u6270\u52a8\u603b\u548c\u4f1a\u662f$\\epsilon m n$             - m\u8868\u793a\u67d0\u4e2atheta\u7684\u5747\u503c, n\u8868\u793a\u7ef4\u5ea6, \u5373\u50cf\u7d20\u6570\u91cf, \u53ef\u89c1\u8fd9\u4e2a\u6270\u52a8\u4e0en\u7ebf\u6027\u76f8\u5173, \u56e0\u6b64\u5c0feta\u4e5f\u4f1a\u5e26\u6765\u5927\u53d8\u5316             - This means that the change in activation given by the perturbation increases linearly with respect to \ud835\udc5b (or the dimensionality). If \ud835\udc5b is large, one can expect even a small perturbation capped at \ud835\udf16 to produce a perturbation big enough to render the model susceptible to an adversarial attack.         - Fast Gradient Sign Method             - $\\tilde{x} = x + \\epsilon \u00b7 sign(\\nabla_{x}\\mathcal{L}(\\theta,x,y))$             - \u68af\u5ea6\u4e0b\u964d\u51cf\u5c11\u68af\u5ea6, \u6211\u4eec\u589e\u52a0\u68af\u5ea6, \u5bf9x\u589e\u52a0\u68af\u5ea6\u4e3a\u6b63\u7684\u90e8\u5206, \u6bcf\u6b21\u589e\u52a0epsilon         - 1920           collapsed:: true - L1 and L2 regularisation - Mechanisms for fixing high variance     - L1 and L2 regularisation     - dropout     - train with small batch     - early stopping     - reduce model complexity, flexibility (degree)     - reduce features     - data augmentation and more data     - bagging - Mechanisms for fixing high bias     - Increase model complexity, flexibility (degree)     - add features     - boosting     - feature engineering     - more training data - Partial Volume Effects #dj w4     - Due to the coarse sampling, both tissue types (black and white) contribute to the intensity value of the generated image (right) due to the relatively large influence area of each pixel (gray square in left image) - encoder-decoder network design     - tricks:         - Encoder: (k, s, p), (5, 2, 2), (3,2,1) \u53ef\u4ee5\u51cf\u534a(\u5411\u4e0a\u53d6\u6574); (5,1,0) \u51cf4, (3,1,0)\u51cf2         - Decoder: (4,2,1) \u53ef\u4ee5\u589e\u500d, (4,2,2)\u53ef\u4ee5\u5728\u589e\u500d\u57fa\u7840\u4e0a\u51cf2, (4,2,3)\u57fa\u7840\u4e0a\u51cf3 - Sketch the iterative process of intensity-based image registration. Include three main components of this process     - Initialize the transformation parameters.     - Apply the transformation model to the floating image using the current transformation parameters.     - Calculate the similarity measure between the transformed floating image and the reference image.     - Update the transformation parameters using the optimization strategy.     - Repeat steps 2-4 until a termination criterion is met (e.g., the maximum number of iterations is reached or the change in similarity measure falls below a threshold).     - Image transformation, Similarity measure using objective function, update the transformation parameters using optimisation strategy - spatial transformer network (STN)     - It is a spatial transformer block which performs transformation on input data, make it invariant to translation, scale, rotation. With STN, network can learn to selectively apply spatial transformations to the input data, making it more robust to variations in scale, rotation, and other spatial factors. - Haar features     - \u4f7f\u7528integral images\u8ba1\u7b97\u98de\u901f, D-B-C+A     - haar features\u53ef\u4ee5scale\u4e3a\u4efb\u610f\u5927\u5c0f, \u653e\u5728\u4efb\u610f\u4f4d\u7f6e, \u8868\u793a\u4e3a\u767d\u51cf\u9ed1\u7684\u6570\u503c - RCNN, Fast-RCNN, Faster-RCNN, YOLO - Multi-label classificatin     - This can be done by using a binary vector for each image, where each element of the vector corresponds to one of the possible classes, and a value of 1 indicates that the class is present in the image, while a value of 0 indicates that the class is absent.     - Softmax and CE do not suit this case, as they assume there is only one correct class with sum of prob to be 1, while here we may have at most all ones. This may lead to unexpected results     - Using sigmoid for each class and BCE to calculate the loss for each class and sum will be a better solution.         - 1819           collapsed:: true - implementation of gradient descent     - initialise weights, in training loop, calculate the predicted value, compute the loss, backpropogate the loss, compute the gradient for all samples and divided by number of samples, update weights by - alpha * gradient - implement stochastic gradient descent.     - gradient calculated for each sample and update once per sample. - segmentation Overlapping measures     - Jaccard Index (IoU, Jaccard Similarity Coefficient, JSC)         - Intersection over Union     - Dice\u2019s Coefficient (S\u00f8rensen Index, Dice Similarity Coefficient, DSC) F1         - $DSC = \\frac{2|A \u2229 B|}{|A|+|B|} = F1$         - ![image.png](../assets/image_1678963717311_0.png)         - Issue: Different shapes have identical DSC     - volume similarity         - ![image.png](../assets/image_1678963740289_0.png)     - Hausdorff distance         - ![image.png](../assets/image_1678963772894_0.png){:height 86, :width 287}         - A\u4e0a\u6bcf\u4e2a\u70b9\u627e\u5230B\u4e0a\u79bb\u8fd9\u4e2a\u70b9\u7684\u6700\u77ed\u8ddd\u79bb, \u8fd9\u4e9b\u8ddd\u79bb\u4e4b\u4e2d\u6700\u957f\u7684\u5c31\u662f\u6211\u4eec\u6c42\u7684\u8fd9\u4e2a\u8ddd\u79bb         - \u76f8\u53cd\u4e5f\u8981\u627e\u4e00\u904d, \u53d6\u6700\u5927. \u56e0\u4e3a b\u4e0a\u51f8\u8d77\u7684\u90e8\u5206, \u8fd9\u4e2a\u65f6\u5019\u624d\u4f1a\u53d1\u73b0\u8ddd\u79bb\u8d3c\u5927.     - average surface distance         - ![image.png](../assets/image_1678963937586_0.png)         - A\u5230B\u7684\u6700\u77ed\u8ddd\u79bb\u7684\u5e73\u5747\u6570 - Ensemble learning methods     - Decision tree, info gain, entropy, adaboost - Neural Networks     - purpose of the 1 x 1 convolutions         - Dimensionality reduction         - Feature combination across channels     - computational graph         - 1718           collapsed:: true - image modality     - NMI \u9002\u5408\u591a\u6a21\u6001multimodal similarity \u5206\u6790, \u4e0d\u9002\u5408 - challenges that might effect the accuracy of an image segmentation algorithm     - Partial volume effect     - Intensity Inhomogeneities     - Anisotropic Resolution     - Imaging Artifacts     - Limited Contrast     - Morphological Variability - Leakage in image segmentation     - 'leakage' refers to the phenomenon where the boundaries of segmented regions overlap or bleed into neighboring regions, leading to inaccurate segmentation results. Leakage can occur when the intensity or texture characteristics of neighboring regions are similar to the region of interest, leading to the misclassification of neighboring pixels as part of the segmented region. - Segmentation Algorithms     - \u25aa Intensity-based segmentation       \u25aa e.g., thresholding     - \u25aa Region-based: region growing         - Start from (user selected) seed point(s), and grow a region according to an intensity threshold     - \u25aa Graph-based: graph cuts         - Segmentation based on max-flow/min-cut algorithm     - \u25aa Active contours: \u25aa e.g., level sets     - \u25aa Atlas-based segmentation: multi-atlas label propagation         - Use multiple atlas to register a new data and get multiple segmentations. Do majority voting for the final class.     - \u25aa Learning-based segmentation       \u25aa e.g., random forests, convolutional neural networks     - Tutorials       collapsed:: true         - W2 - MLI\u91cc\u9762bias\u9700\u8981\u9ed8\u8ba4\u52a0\u4e0a - conv\u64cd\u4f5c\u9ed8\u8ba4\u4e0d\u7528flip, \u4f46\u662f\u53ef\u4ee5\u81ea\u5df1\u6ce8\u4e0a\u53bb         - W3 - Haar features \u9002\u5408\u63d0\u53d6edge, corner\u8fd9\u79cd\u4fe1\u606f - Support \u548c receptive field\u7684\u8ba1\u7b97 - conv\u7684parameters\u8ba1\u7b97: kernels * (kernel size* input channels + bias)         - W4 - Ensemble methods\u7684advantages \u548cdisadv - Segmentation evaluation: \u7ed3\u5408\u591a\u4e2a\u4e00\u8d77\u4f1a\u6bd4\u8f83\u6709\u7528\u4e00\u70b9 - Pitfalls in segmentation evaluation     - structure size     - structure shape, spatial alignment \u4e0d\u80fd\u88abDSC\u548cIoU\u4f53\u73b0, \u4f46\u53ef\u4ee5\u4eceHD\u770b\u51fa     - hole: \u6709\u4e24\u5c42boundary, \u90fd\u8981\u770b\u8fc7, \u5185\u5c42\u79bb\u5916\u8fb9\u7f18\u8ddd\u79bb\u4f1a\u8fdc\u4e00\u70b9     - annotation noise     - resolution - challenges that might affect segmentation     - \u25aa noise,       \u25aa partial volume effects,       \u25aa intensity inhomogeneities, anisotropic resolution, \u25aa imaging artifacts,       \u25aa limited contrast,       \u25aa morphological variability,         - W5 - (Dis)similarity measures     - \u5982\u679c\u5404\u4e2a\u7ec4\u7ec7\u7684intensity\u95f4\u6709\u4e00\u81f4\u7684\u7ebf\u6027\u5173\u7cfb(\u4f8b\u5982contrast change), \u6b64\u65f6\u53ef\u4ee5\u4f7f\u7528 correlation coefficient; \u5982\u679c\u7ebf\u6027\u5173\u7cfb\u4e0d\u660e\u786e, \u6216\u5b8c\u5168\u5c31\u662f\u4e24\u4e2amodality, \u90a3\u5c31\u5f97\u4f7f\u7528 NMI     - Joint Histograms: \u4e24\u4e2a\u8f74\u662f\u4e24\u5f20\u56fe\u7247\u7684\u50cf\u7d20\u5f3a\u5ea6\u503c, \u6563\u70b9\u662f\u4e24\u5f20\u56fe\u6bcf\u7ec4\u5bf9\u5e94\u50cf\u7d20\u7684\u5f3a\u5ea6\u503c\u5206\u5e03, \u5982\u679c\u6574\u4f53\u662f\u7ebf\u6027\u5206\u5e03\u7684, \u7a81\u7136\u51fa\u6765\u7684\u4e00\u5757\u5c31\u662flesion\u4e4b\u7c7b\u7684 - Spatial Transformers     - estimating the parameters of a **2D rigid** transformation for inputs of size 1 x 64 x 64. \u6700\u540e\u8981\u8f93\u51fa\u4e00\u4e2a3\u7ef4\u7684(2\u4e2atranslation, 1\u4e2arotation) (3*2\u7684affine matrix\u52a0\u4e0a\u4e86scale\u548cshear)     - ![image.png](../assets/image_1679057312383_0.png)         - W7 - L1 L2 \u90fd\u8981\u9664\u4ee5\u50cf\u7d20\u6570\u91cf - Perceptual loss \u7528\u4e86\u51e0\u4e2afilter, \u5c31\u8981\u9664\u4ee5\u51e0, \u7528\u7684\u662fL2         - W8 - saliency maps using three methods - Fast Gradient Sign Method: x + epsilon * gradient     - \u96be\u70b9         - Model complexity \u4e00\u822c\u6307\u7684\u5c31\u662fparameter\u6570\u91cf         - Receptive Field \u8ba1\u7b97 #card           id:: 6411aa7f-80ee-4a0d-bdaf-e0e6b35cec02           collapsed:: true - \u4ece\u5916\u5230\u5185, \u4ece\u4e0a\u5230\u4e0b\u5199.   id:: 6411aa8d-352f-4a94-898b-7e39bc7ac406 - \u6700\u5916\u5c42\u7684conv\u7684kernel\u5927\u5c0f, \u5c31\u662f\u4e0b\u4e00\u5c42conv\u9700\u8981\u751f\u6210\u7684\u70b9\u6570, \u4f8b\u5982\u6700\u5916\u5c42\u662f3   id:: 6411aaa6-dea5-4525-95dd-1e4b719125fb - \u6839\u636e\u4e0b\u4e00\u5c42\u7684kernel\u548cstride, \u753b\u753b\u56fe\u7b97\u4e00\u7b97\u627e\u5230\u4e0b\u4e00\u5c42\u9700\u8981\u7684\u50cf\u7d20\u6570\u91cf, \u4f8b\u5982\u7b2c\u4e8c\u5c42\u662f2 stride2, \u90a3\u4e48\u7b2c\u4e8c\u5c42\u7684conv\u751f\u6210\u6700\u5916\u5c42\u76843\u6240\u9700\u7684\u50cf\u7d20\u6570\u91cf\u5c31\u662f2*3=6   id:: 6411aad9-d4b0-45a1-91cb-dfbe745d79d4 - \u518d\u4e3e\u4e2a\u4f8b\u5b50, \u7b2c\u4e09\u5c42\u662f3 stride1, \u56e0\u4e3astride\u662f1, \u6240\u4ee5\u9664\u4e86\u524d\u4e09\u4e2a\u50cf\u7d20\u4ee5\u5916, \u540e\u9762\u4f9d\u6b21\u589e\u52a0, \u53733 + 5 = 8   id:: 6411ab48-bb6f-4b24-959b-526e78050234 - \u8fd9\u6837\u5b50\u7b97\u5230\u6700\u540e\u4e00\u5c42\u5373\u53ef - \u2022\u611f\u53d7\u91ce\u8ba1\u7b97\u516c\u5f0f\uff1a - \u9996\u5148\u4ece\u6700\u5f00\u59cb\u7684layer\u5f00\u59cb\uff0c\u8bbe\u5b9aRF(receptive field) =1, SP(Stride Product)=1\uff0c\u7136\u540e\u5f53\u524d\u5c42\u7684\u611f\u53d7\u91ce\u662f RF=RF + (kernel_size-1\uff09* SP\uff0c\u7136\u540eSP\u4e5f\u8981\u6839\u636e\u5f53\u524d\u7684stride\u66f4\u65b0\uff1aSP=SP*stride - $RF_N=1,\\  RF_{N-1}= (RF_N - 1)\\times stride_n + k_n$         - Convolution\u548cTransposed Convolution\u7684\u8ba1\u7b97 #card           collapsed:: true - $o=[(i-k+2p)/s]+1$ - conv, \u9664\u6cd5\u5411\u4e0b\u53d6\u6574 - $o = (i-1) \\times s - 2p + k$ - transposed conv (\u8f93\u5165\u5143\u7d20\u95f4\u586b\u5145s-1, \u56db\u5468\u586b\u5145k-p-1) -</code>python   def conv_output(height, width, kernel_size, padding, stride, dilation): </p> <pre><code>  height_out = int(np.floor(((height + 2 * padding - dilation * (kernel_size - 1) - 1) / stride) + 1))\n\n  width_out = int(np.floor(((width + 2 * padding - dilation * (kernel_size - 1) - 1) / stride) + 1))\n\n  print(height_out, width_out)\n\n  return height_out, width_out\n</code></pre> <p>```         - AdaBoost         - ensemble \u7684         - W7 W8         - W2           collapsed:: true - Gradient descent     -      -  - Stochastic gradient descent     - Update once per sample - One-vs-all: learn \ud835\udc58-binary classifiers, \u6bcf\u4e2a\u7c7b\u90fd\u8bad\u7ec3\u4e00\u4e2abinary classifier -         - W3           collapsed:: true - features: Distinctive, local, illumination invariant; intensity, gradient, SIFT, Harr - LBP: \u4e5d\u5bab\u683c\u7b2c\u4e8c\u4e2a\u5f00\u59cb\u9006\u65f6\u9488\u5199vector, \u6bd4\u4e2d\u95f4\u5927\u4e3a1 - Haar - Ensemble classifiers     - Error(x\uff09 =Bias^2 + Variance + Irreducible Error     - Homogenous, Heterogenous: using same or different ML model classes     - Decision Stump         - a one-level decision tree that is axis-aligned, \u5728\u67d0\u4e2a\u8f74\u4e0a\u5927\u4e8e\u6216\u5c0f\u4e8e, \u5f97\u5230binary\u89e3     - Bagging: bootstrap, \u5c06\u8bad\u7ec3\u96c6\u5206\u51e0\u90e8\u5206\u8bad\u7ec3\u51e0\u4e2aweak learner, aggregating, \u6c47\u603b\u5927\u5bb6\u7684\u7b54\u6848     - Boosting: build weak learners in serial, adaptively reweight training data prior to training each new weak learner in order to give a higher weight to previously misclassified examples, training the next learner on the mistakes of the previous one         -          - Adaboost: equal weighted samples, \u5206\u7c7b, upweight misclassified, use loss to weight this classifier, \u91cd\u590dt\u6b21, \u4f7f\u7528weighted classifiers \u5f97\u5230\u6700\u540e\u7ed3\u679c     - Decision tree         -          -          - Gini index: how mixed the classes are, 0 is best             -              -  - Neural networks     - multilayer perceptron \u7684\u8bdd \u4f1a\u9700\u8981 neuron\u6570\u91cf\u4e2abias     -      -      - sigmoid': 0-0.25; tanh': 0-1     - Stochastic gradient descent         -      - Residual connection         - H(x)=F(x)+x -         - W4           collapsed:: true - Challenges for Image Segmentation - Performance Measures     - accuracy, precision, recall, specificity, F1 Score, Jaccard Index, Dice Similarity Coefficient (DSC), volume similarity, - Segmentation Algorithms &amp; Techniques     - - Fully Convolutional networks - Boundary Effects in transpose convolution     - padding with 0 or nearest neighbour may give strange boundaries - Use only layers with small kernels to deal with deep issues     - 2 3x3 instead of 1 5x5 - Multi-Scale Processing     - Two pathways, one using normal resolution, another with high resolution, concat.     - This adds additional spatial info which enhances network's localisation capability         - W5           collapsed:: true -  - \u6ce8\u610frigid\u662f2+1=3\u4e2a\u53c2\u6570, affine\u662f6\u4e2a - Free-Form Deformations: non-linear - Atlas construction     - Iteration:     - 1) Rigid     - 2) Affine     - 3) Nonrigid - Intensity-based Registration     - Estimation of transformation parameters is driven by the appearance of the images     - \u627e\u5230\u8ba9intensity relation \u5dee\u5f02\u6700\u5c0f\u7684\u90a3\u4e2aT     - (Dis)similarity Measures D         - SSD, SAD (identity relationship, CTCT) mono-modal         - Correlation coefficient(CC, linear relationship, MRMR)mono-modal         - multi-modal have statistical relationship, CTMR, multi-modal             - \\(p(i,j)=\\frac{h(i,j)}{N}\\) : \u4e24\u5f20\u56fe\u5bf9\u5e94\u50cf\u7d20\u70b9\u7684\u5f3a\u5ea6\u4e3ai\u548cj\u7684\u6982\u7387             -              - \u71b5\u8d8a\u5c0f\u8d8a\u597d, joint entropy\u4e5f\u53ef\u4ee5\u4f5c\u4e3aD             -              - mutual info, \u8868\u8fbe\u4e86\u4e00\u5f20\u56fe\u88ab\u53e6\u5916\u4e00\u5f20\u56fe\u7684\u63cf\u8ff0\u6027, \u8d8a\u5927\u8d8a\u597d, \u7528\u4f5cD\u65f6\u53d6\u8d1f             -              - normalised mutual info, \u4e0eoverlap \u65e0\u5173, \u540c\u6837\u8d8a\u5927\u8d8a\u597d, \u7528\u4f5cD\u65f6\u53d6\u8d1f     - \u56e0\u4e3a\u8981\u5728overlap\u7684\u90e8\u5206\u8861\u91cf, \u5982\u679c\u521a\u597d\u53ea\u6709\u80cc\u666f\u91cd\u5408\u4e5f\u4f1a\u6709\u5f88\u597d\u7684\u6210\u7ee9, \u6240\u4ee5\u8981\u5c0f\u5fc3\u7684init     -  - Registration with Neural Networks - STN         - W6           collapsed:: true - K-means     - handles anisotropic data and non-linearities poorly - Gaussian Mixture Model (GMM)     -  - Dimensionality reduction / data encoding     - PCA         -          - PCA is a linear encoder/decoder     - Autoencoders         - Auto-Encoders are general encoder/decoder architectures with non-linearities         - non trivial to:           \u2022 visualize data in the latent space of Autoencoders           \u2022 generate new samples: perturb latent code of training example and iterate - Generative Modelling     - Model the process through which data is generated       Capture the structure in the data         - W7 inverse problem           collapsed:: true - Inverse problems     - Inpainting, Deblurring, Denoising, Super-resolution - Classical approach: Least square solution, \u7528y\u548cA\u76f4\u63a5\u89e3\u51fax - - General approach:     -      - A\u662f\u5df2\u77e5\u7684, \u60f3\u8981\u4ecey\u5b66\u53d8\u56dex     - R\u662f\u6839\u636eprior\u5199\u7684\u4e00\u4e2aregulariser, \u7528\u6765\u9650\u5236\u6bd4\u5982\u8bf4\u50cf\u7d20\u7684\u5927\u5c0f, \u5206\u5e03\u7b49\u7b49, \u8fd9\u91cc\u662f\u4eba\u4e3a\u4f7f\u7528\u50cf\u7d20\u7684\u5927\u5c0f, \u540e\u7eed\u4e5f\u53ef\u4ee5\u7528\u795e\u7ecf\u7f51\u7edc\u6765\u627e\u4e00\u4e2a\u5408\u9002\u7684\u5148\u9a8c, \u4f8b\u5982\u56fe\u7247\u5c31\u5e94\u8be5\u662f\u7c7b\u4f3c\u8fd9\u6837\u5206\u5e03\u7684 - Deep Learning approach     - Model agnostic (ignores forward model)         - \u76f4\u63a5\u5728\u9006\u5411\u8fc7\u7a0b\u4e2d\u63a5\u795e\u7ecf\u7f51\u7edc\u53bb\u5b66\u4e60, \u4ecey\u5230x\u7684\u76f4\u63a5\u6620\u5c04     - Decoupled (First learn, then reconstruct)         - Deep proximal gradient: z^k\u4f5c\u4e3a\u4e2d\u95f4\u9690\u53d8\u91cf, \u8868\u793a\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u4ee5\u540e\u7684x^, \u65b0\u7684x^\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u4ecez^k\u4e2d\u751f\u6210, \u5b66\u4e60\u4ece\u4e00\u4e2a\u4e0d\u5065\u5168\u7684z\u751f\u6210\u79d1\u5b66\u7684x\u7684\u8fc7\u7a0b, denoising         - GANs: \u4eceG\u751f\u6210\u7684\u56fe\u7247\u91cc\u627e\u5230\u6700\u9002\u5408data\u7684\u90a3\u4e2a     - Unrolled optimisation         - Gradient descent networks, R\u6b63\u5219\u9879\u4e5f\u662f\u4e2a\u53ef\u5bfc\u7684\u795e\u7ecf\u7f51\u7edc, \u901a\u8fc7\u4e0d\u65ad\u8fed\u4ee3\u6765\u5bfb\u627e\u5408\u9002\u7684\u91cd\u6784\u548cdenoising\u65b9\u5f0f - Super-Resolution     - Post-upsampling super-resolution: \u76f4\u63a5\u7aef\u5230\u7aef, \u9700\u8981\u5b66\u4e60\u6574\u4e2apipeline, \u9650\u5236\u5728\u4e86\u7279\u5b9a\u7684\u4e0a\u91c7\u6837factor     - Pre-upsampling super-resolution: \u5148\u4f7f\u7528\u4f20\u7edf\u65b9\u6cd5upsample\u5230\u7279\u5b9a\u5927\u5c0f, \u518d\u7528nn refine, \u53ef\u4ee5\u4f7f\u7528\u5404\u79cdupscaling factors\u548cimages sizes, \u9700\u8981\u66f4\u9ad8\u7684\u8ba1\u7b97\u529b\u548c\u5185\u5b58     - Progressive upsampling super-resolution: multi-stage, upsample to a higher resolution and refined at each stage, efficient, but difficult to train deep models     - Iterative up-and-down sampling super-resolution: Alternate between upsampling and downsampling (back-projection) operations, \u4e14\u4e0a\u4e0b\u91c7\u6837stages\u4eec\u662f\u76f8\u4e92\u8fde\u63a5\u7684, superior performance by allowing error feedback, easier to train with deep - Perceptual loss: \u4e8b\u5b9e\u548c\u4f30\u8ba1\u90fd\u7528\u540c\u4e00\u4e2a\u7f51\u7edc\u7279\u5f81\u63d0\u53d6\u540e\u7684map\u8fdb\u884cL2 norm, \u7528\u4e86\u51e0\u4e2a\u7f51\u7edc\u5c31\u628a\u8fd9\u51e0\u4e2a\u7ed3\u679c\u52a0\u8d77\u6765\u53d6\u5e73\u5747 - GAN:     -      -  - Image Reconstruction     - X-ray computed tomography (CT)         - Sinogram to CT image     - Magnetic Resonance (MR)         - Slow acquisition, bad for moving objects         - performed in k-space by sequentially traversing sampling trajectories. need to inverse to signal space, IFT -         - W7 Object detection           collapsed:: true - classification(softmax) + localisation(L2) - \u56e0\u4e3a\u4e0d\u53ef\u80fdsliding window\u5168\u90e8\u904d\u5386, \u6240\u4ee5\u9700\u8981region proposal - R-CNN     -  - Spatial Pyramid Pooling (SPP-net)     -      - \u5728FM\u4e0aregion proposal, \u53ea\u8ba1\u7b97\u4e00\u6b21feature, \u901a\u8fc7SPP\u628aregions\u53d8\u6210\u7edf\u4e00\u957f\u5ea6\u5411\u91cf; SPP\u4e0b\u65e0\u6cd5\u66f4\u65b0 - Fast R-CNN     - \u5728\u56fe\u7247\u91ccregion proposal, \u6620\u5c04\u5230FM\u4e2d, \u901a\u8fc7ROI pooling\u53d8\u62107x7, FC\u7ed9\u5206\u7c7b\u548c\u56de\u5f52, \u8fd9\u91cc\u7684\u5206\u7c7b\u7528\u7684\u4e5f\u662fFC\u548csoftmax     -  - Faster R-CNN     - Insert Region Proposal Network (RPN) to predict proposals from features     - 1. RPN classify object / not object       2. RPN regress box coordinates       3. Final classification score (object classes)       4. Final box coordinates     - \u7528RPN\u6765\u9884\u6d4bfeature map\u4e0a\u662f\u5426\u4e3aobject, \u4ee5\u53cabounding box - YOLO     - \u5206\u5272\u56fe\u7247\u62107x7\u4e2a\u683c\u5b50, \u6bcf\u4e2a\u683c\u5b50\u8d1f\u8d23\u9884\u6d4b\u4e00\u5b9a\u6570\u91cf\u7684bounding box, \u67094\u4e2a coordinates, 1\u4e2aconfidence score, \u4ee5\u53ca1\u4e2a\u7c7b\u522b\u6570dim\u7684\u5206\u7c7bvector         - 7 x 7 x (2 x 5 + 20)     - perform NMS and threshold \u6765\u9009\u62e9\u597d\u7528\u7684, increase the confidence for the best ones, decrease others         - W8 trustworthy - Federated learning: train a ML model across decentralized clients with local data, without exchanging them - Federated learning: \u5206\u53d1\u6a21\u578b\u7ed9clients, \u5404\u81ea\u7528\u81ea\u5df1\u6570\u636e\u8ba1\u7b97\u6a21\u578b\u66f4\u65b0, \u53d1\u56deowner\u6765aggregate     -      - \u4e5f\u53ef\u4ee5\u76f4\u63a5\u66f4\u65b0weights, server\u90a3\u4e5f\u76f4\u63a5average weights     - \u6311\u6218: non-iid, unbalanced, commu cost - - Homomorphic encryption which enables learning from encrypted data - Secure multi-party computing where processing is performed on encrypted data shares, split among them in a way that no single party can retrieve the entire data on their own. \u5171\u540c\u8ba1\u7b97\u4e00\u4e2a\u5e73\u5747\u5206, \u7b2c\u4e00\u4e2a\u4eba\u5148\u52a0\u4e2a\u968f\u673a\u6570, \u5355\u5411\u4f20\u64ad, \u56de\u5230\u81ea\u5df1\u7684\u65f6\u5019\u53ef\u4ee5\u8ba1\u7b97\u5e73\u5747\u5206 - Interpretability     - social issue, debuging     - Ablation test: How important is a data point or feature?     - Fit functions (use first derivatives, sensitivity and saliency maps)     - visualize activations generated by kernels     - Mask out region in the input image and observe network output     - DeconvNet: Chose activation at one layer, \u53d6gradient\u4e2d\u975e\u8d1f\u7684\u6570     - Gradient (backprogagation): differentiate activation with respect to input pixels, \u53d6gradient\u91ccactivation\u6fc0\u6d3b\u7684\u5bf9\u5e94\u4f4d\u7f6e\u9879     - Guided backpropagation: gradient\u91cc\u9762\u4e3a\u8d1f\u7684\u4e5f\u4e0d\u53d6     -      - Inverting codes via deep image priors: \u751f\u6210\u7f51\u7edc\u4ecez0\u751f\u6210x, loss\u7531\u53e6\u4e00\u4e2a\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u5bf9\u539f\u56fe\u548c\u751f\u6210\u56fe\u7684\u7ed3\u679c\u5dee\u5f97\u5230\u3001     - Learning the inverter from data: \u751f\u6210\u7f51\u7edc\u4ece\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u5f97\u5230\u7684\u7279\u5f81\u5f00\u59cb\u751f\u6210x, \u76f4\u63a5\u4e0e\u539f\u56fe\u8ba1\u7b97loss - DeepDream\u548cInversion\u662f\u975e\u5e38\u76f8\u4f3c\u7684\u65b9\u6cd5\uff0c\u5b83\u4eec\u8bd5\u56fe\u5c06\u795e\u7ecf\u7f51\u7edc\u7684\u5de5\u4f5c\u5185\u5bb9\u548c\u5de5\u4f5c\u65b9\u5f0f\u53ef\u89c6\u5316\u3002Deep Image\u7684\u5148\u9a8c\u662f\u4e0d\u540c\u7684\uff0c\u5b83\u4e3a\u56fe\u50cf\u6307\u5b9a\u4e86\u4e00\u4e2a\u7279\u5b9a\u7684\u5148\u9a8c\uff08\u800c\u4e0d\u662f\u4f7f\u7528L1 norm\u8fd9\u6837\u7684\u5148\u9a8c\uff09\u3002DeepDream\u548cInversion\u53ef\u4ee5\u4f7f\u7528\u4e0d\u540c\u7684\u5148\u9a8c\u3002 - Adversarial Methods     -      -      -  -     - \u7591\u70b9:         - - ## Coursework   collapsed:: true     - Age Regression from Brain MRI       \u25aa mini-project on a real-world medical imaging task       \u25aa implement two different machine learning approaches \u25aa work in groups of two     - Start: Friday, February 17 (week 6)     - End: Thursday, March 2 (week 8)     - - ## Info   collapsed:: true     - 8:2     - \u25aa Fridays, 14:00, LT308: Q&amp;A, invited talks, quizzes       \u25aa Fridays, 15:00, LT202/206/210 : programming tutorials - ## Syllabus   collapsed:: true     - Introduction to machine learning for imaging     - Image classification     - Image segmentation     - Object detection &amp; localisation     - Image registration     - Generative models and representation learning     - Application to real-world problems - ## Links   collapsed:: true     - Scientia     - Panopto     - Spatial Transformer Networks Tutorial \u2014 PyTorch Tutorials 1.13.1+cu117 documentation     - GitHub - ecekayan/mli-cw: Machine Learning for Imaging CW     - GitHub - marektopolewski/ic-mli-cw     - GitHub - Cy-r0/mli_cw2: Second coursework of Machine Learning for Imaging     -</p> </li> </ul> </li> </ul>"},{"location":"ml/PyTorch/","title":"PyTorch","text":"<p>Pytorch</p> <p>tags::  python, package, ML alias:: Torch</p> <ul> <li> <p>Installation #\u5b89\u88c5</p> <ul> <li>```   Linux   conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia   pip3 install torch torchvision torchaudio</li> </ul> <p>Mac   conda install pytorch torchvision torchaudio -c pytorch   pip3 install torch torchvision torchaudio   <code>- - Tensor   collapsed:: true -</code>python   # \u6570\u636e\u7c7b\u578b\u7684\u79cd\u7c7b   torch.float64(torch.double), </p> <p>torch.float32(torch.float), </p> <p>torch.float16, </p> <p>torch.int64(torch.long), </p> <p>torch.int32(torch.int), </p> <p>torch.int16, </p> <p>torch.int8, </p> <p>torch.uint8, </p> <p>torch.bool</p> <p>#\u6570\u636e\u7c7b\u578b\u7684\u5224\u65ad   x = torch.tensor(2.0);print(x,x.dtype)   tensor(2.) torch.float32</p> <p>#\u6570\u636e\u7c7b\u578b\u7684\u6307\u5b9a   i = torch.tensor(1,dtype = torch.int32)</p> <p>#\u7279\u5b9a\u6570\u636e\u7c7b\u578b\u7684\u6784\u9020\u5668   i = torch.IntTensor(1);print(i,i.dtype)   x = torch.Tensor(np.array(2.0));print(x,x.dtype) #\u7b49\u4ef7\u4e8etorch.FloatTensor   b = torch.BoolTensor(np.array([1,0,2,0])); print(b,b.dtype)</p> <p>#\u6570\u636e\u7c7b\u578b\u7684\u8f6c\u6362   i = torch.tensor(1); print(i,i.dtype)   x = i.float(); print(x,x.dtype) #\u8c03\u7528 float\u65b9\u6cd5\u8f6c\u6362\u6210\u6d6e\u70b9\u7c7b\u578b   y = i.type(torch.float); print(y,y.dtype) #\u4f7f\u7528type\u51fd\u6570\u8f6c\u6362\u6210\u6d6e\u70b9\u7c7b\u578b   z = i.type_as(x);print(z,z.dtype) #\u4f7f\u7528type_as\u65b9\u6cd5\u8f6c\u6362\u6210\u67d0\u4e2aTensor\u76f8\u540c\u7c7b\u578b</p> <p># \u5f20\u91cf\u7684\u7ef4\u5ea6   \u6807\u91cf\u4e3a0\u7ef4\u5f20\u91cf\uff0c\u5411\u91cf\u4e3a1\u7ef4\u5f20\u91cf\uff0c\u77e9\u9635\u4e3a2\u7ef4\u5f20\u91cf\u3002 \u6709\u51e0\u5c42\u4e2d\u62ec\u53f7\uff0c\u5c31\u662f\u591a\u5c11\u7ef4\u7684\u5f20\u91cf\u3002</p> <p># 0\u7ef4\u5f20\u91cf   scalar = torch.tensor(True)   print(scalar)   print(scalar.dim())  # \u6807\u91cf\uff0c0\u7ef4\u5f20\u91cf</p> <p># 1\u7ef4\u5f20\u91cf   vector = torch.tensor([1.0,2.0,3.0,4.0]) #\u5411\u91cf\uff0c1\u7ef4\u5f20\u91cf   print(vector)   print(vector.dim())</p> <p># 2\u7ef4\u5f20\u91cf   matrix = torch.tensor([[1.0,2.0],[3.0,4.0]]) #\u77e9\u9635, 2\u7ef4\u5f20\u91cf   print(matrix)   print(matrix.dim())</p> <p># 4\u7ef4\u5f20\u91cf   tensor4 = torch.tensor([[[[1.0,1.0],[2.0,2.0]],[[3.0,3.0],[4.0,4.0]]],                           [[[5.0,5.0],[6.0,6.0]],[[7.0,7.0],[8.0,8.0]]]])  # 4\u7ef4\u5f20\u91cf   print(tensor4)   print(tensor4.dim())</p> <p># \u5f20\u91cf\u7684\u5c3a\u5bf8\u5f62\u72b6   shape\u5c5e\u6027\u548csize()\u65b9\u6cd5\u53ef\u4ee5\u7528\u6765\u67e5\u770b, view()\u548creshape()\u65b9\u6cd5\u53ef\u4ee5\u7528\u6765\u6539\u53d8</p> <p># 0\u7ef4\u5f20\u91cf\u5c3a\u5bf8   torch.Size([])</p> <p># 1\u7ef4\u5f20\u91cf\u5c3a\u5bf8   vector = torch.tensor([1.0,2.0,3.0,4.0])   print(vector.size())   print(vector.shape)   -&gt; torch.Size([4])</p> <p># \u4f7f\u7528view\u53ef\u4ee5\u6539\u53d8\u5f20\u91cf\u5c3a\u5bf8   vector = torch.arange(0,12)   print(vector)   print(vector.shape)</p> <p>matrix34 = vector.view(3,4)   print(matrix34)   print(matrix34.shape)</p> <p>matrix43 = vector.view(4,-1) #-1\u8868\u793a\u8be5\u4f4d\u7f6e\u957f\u5ea6\u7531\u7a0b\u5e8f\u81ea\u52a8\u63a8\u65ad   print(matrix43)   print(matrix43.shape)</p> <p># \u6709\u4e9b\u64cd\u4f5c\u4f1a\u8ba9\u5f20\u91cf\u5b58\u50a8\u7ed3\u6784\u626d\u66f2\uff0c\u76f4\u63a5\u4f7f\u7528view\u4f1a\u5931\u8d25\uff0c\u53ef\u4ee5\u7528reshape\u65b9\u6cd5   # python - What does .contiguous() do in PyTorch? - Stack Overflow   # \u5373index\u7684\u6446\u653e\u65b9\u6cd5\u548c\u5185\u5b58\u4e2d\u7684\u6446\u653e\u65b9\u5f0f\u4e0d\u4e00\u81f4\u4e86   matrix26 = torch.arange(0,12).view(2,6)   print(matrix26)   print(matrix26.shape)</p> <p># \u8f6c\u7f6e\u64cd\u4f5c\u8ba9\u5f20\u91cf\u5b58\u50a8\u7ed3\u6784\u626d\u66f2   matrix62 = matrix26.t()   print(matrix62.is_contiguous())</p> <p># \u76f4\u63a5\u4f7f\u7528view\u65b9\u6cd5\u4f1a\u5931\u8d25\uff0c\u53ef\u4ee5\u4f7f\u7528reshape\u65b9\u6cd5   #matrix34 = matrix62.view(3,4) #error!   matrix34 = matrix62.reshape(3,4) #\u7b49\u4ef7\u4e8ematrix34 = matrix62.contiguous().view(3,4)   print(matrix34)</p> <p># \u5f20\u91cf\u548cnumpy\u4e4b\u95f4\u7684\u4e92\u76f8\u8f6c\u6362\u548c\u8054\u7cfb   numpy()\u65b9\u6cd5\u548cfrom_numpy(arr)\u53ef\u4ee5\u4e92\u76f8\u8f6c\u6362\u4e24\u79cd\u7c7b\u578b, \u4f46\u662f\u5171\u4eab\u5185\u5b58, \u6539\u4e00\u4e2a\u4e24\u4e2a\u90fd\u53d8   arr = np.zeros(3)   tensor = torch.from_numpy(arr)   print(\"before add 1:\")</p> <p>print(\"\\nafter add 1:\")   np.add(arr,1, out = arr) #\u7ed9 arr\u589e\u52a01\uff0ctensor\u4e5f\u968f\u4e4b\u6539\u53d8</p> <p>tensor = torch.zeros(3)   arr = tensor.numpy()   print(\"before add 1:\")</p> <p>print(\"\\nafter add 1:\")</p> <p>#\u4f7f\u7528\u5e26\u4e0b\u5212\u7ebf\u7684\u65b9\u6cd5\u8868\u793a\u8ba1\u7b97\u7ed3\u679c\u4f1a\u8fd4\u56de\u7ed9\u8c03\u7528 \u5f20\u91cf   tensor.add_(1) #\u7ed9 tensor\u589e\u52a01\uff0carr\u4e5f\u968f\u4e4b\u6539\u53d8    #\u6216\uff1a torch.add(tensor,1,out = tensor)</p> <p>#clone()\u65b9\u6cd5\u53ef\u4ee5\u62f7\u8d1d\u800c\u975e\u94fe\u63a5   tensor = torch.zeros(3)</p> <p>#\u4f7f\u7528clone\u65b9\u6cd5\u62f7\u8d1d\u5f20\u91cf, \u62f7\u8d1d\u540e\u7684\u5f20\u91cf\u548c\u539f\u59cb\u5f20\u91cf\u5185\u5b58\u72ec\u7acb   arr = tensor.clone().numpy() # \u4e5f\u53ef\u4ee5\u4f7f\u7528tensor.data.numpy()</p> <p># item\u65b9\u6cd5\u548ctolist\u65b9\u6cd5\u53ef\u4ee5\u5c06\u5f20\u91cf\u8f6c\u6362\u6210Python\u6570\u503c\u548c\u6570\u503c\u5217\u8868   scalar = torch.tensor(1.0)   s = scalar.item()   print(s)   print(type(s))     <p>tensor = torch.rand(2,2)   t = tensor.tolist()   print(t)   print(type(t))     <p><code>- \u5f20\u91cf\u7ed3\u6784\u64cd\u4f5c   collapsed:: true -</code>python   tensor, arange, linspace, zeros, ones, zeros_like, fill_, full, rand, normal, randn, randperm, eye, diag   import numpy as np   import torch </p> <p># \u9ed8\u8ba4\u521b\u5efa\u65b9\u5f0f   a = torch.tensor([1,2,3],dtype = torch.float)   tensor([1., 2., 3.])</p> <p>b = torch.arange(1,10,step = 2)   tensor([1, 3, 5, 7, 9])</p> <p>c = torch.linspace(0.0,2*3.14,10)   tensor([0.0000, 0.6978, 1.3956, 2.0933, 2.7911, 3.4889, 4.1867, 4.8844, 5.5822,           6.2800])</p> <p>d = torch.zeros((3,3))   tensor([[0., 0., 0.],           [0., 0., 0.],           [0., 0., 0.]])</p> <p>a = torch.ones((3,3),dtype = torch.int)   b = torch.zeros_like(a,dtype = torch.float)   tensor([[1, 1, 1],           [1, 1, 1],           [1, 1, 1]], dtype=torch.int32)   tensor([[0., 0., 0.],           [0., 0., 0.],           [0., 0., 0.]])</p> <p>torch.fill_(b,5)   tensor([[5., 5., 5.],           [5., 5., 5.],           [5., 5., 5.]])</p> <p>x = torch.full((3, 5), 3.0)   tensor([[3., 3., 3., 3., 3.],           [3., 3., 3., 3., 3.],           [3., 3., 3., 3., 3.]])</p> <p>#\u5747\u5300\u968f\u673a\u5206\u5e03 uniform distribution   torch.manual_seed(0)   minval,maxval = 0,10   a = minval + (maxval-minval)*torch.rand([5])   tensor([4.9626, 7.6822, 0.8848, 1.3203, 3.0742])</p> <p>#\u6b63\u6001\u5206\u5e03\u968f\u673a normal distribution   b = torch.normal(mean = torch.zeros(3,3), std = torch.ones(3,3))   tensor([[ 0.5507,  0.2704,  0.6472],           [ 0.2490, -0.3354,  0.4564],           [-0.6255,  0.4539, -1.3740]])</p> <p>#\u6b63\u6001\u5206\u5e03\u968f\u673a   mean,std = 2,5   c = std*torch.randn((3,3))+mean   tensor([[16.2371, -1.6612,  3.9163],           [ 7.4999,  1.5616,  4.0768],           [ 5.2128, -8.9407,  6.4601]])</p> <p>#\u6574\u6570\u968f\u673a\u6392\u5217   d = torch.randperm(20)   tensor([ 3, 17,  9, 19,  1, 18,  4, 13, 15, 12,  0, 16,  7, 11,  2,  5,  8, 10,            6, 14])</p> <p>#\u7279\u6b8a\u77e9\u9635   I = torch.eye(3,3) #\u5355\u4f4d\u77e9\u9635   print(I)   t = torch.diag(torch.tensor([1,2,3])) #\u5bf9\u89d2\u77e9\u9635   print(t)   tensor([[1., 0., 0.],           [0., 1., 0.],           [0., 0., 1.]])   tensor([[1, 0, 0],           [0, 2, 0],           [0, 0, 3]])</p> <p><code>- \u5f20\u91cf\u7d22\u5f15\u548c\u5207\u7247   collapsed:: true -</code>python   # \u7d22\u5f15\u548c\u5207\u7247   #\u7b2c0\u884c   print(t[0])   #\u5012\u6570\u7b2c\u4e00\u884c   print(t[-1])   #\u7b2c1\u884c\u7b2c3\u5217   print(t[1,3])   print(t[1][3])   #\u7b2c1\u884c\u81f3\u7b2c3\u884c [1,4)   print(t[1:4,:])   #\u7b2c1\u884c\u81f3\u6700\u540e\u4e00\u884c\uff0c\u7b2c0\u5217\u5230\u6700\u540e\u4e00\u5217\u6bcf\u9694\u4e24\u5217\u53d6\u4e00\u5217   print(t[1:4,:4:2])   #\u53ef\u4ee5\u4f7f\u7528\u7d22\u5f15\u548c\u5207\u7247\u4fee\u6539\u90e8\u5206\u5143\u7d20   x = torch.Tensor([[1,2],[3,4]])   x.data[1,:] = torch.tensor([0.0,0.0])</p> <p>a = torch.arange(27).view(3,3,3)   tensor([[[ 0,  1,  2],            [ 3,  4,  5],            [ 6,  7,  8]],</p> <pre><code>      [[ 9, 10, 11],\n       [12, 13, 14],\n       [15, 16, 17]],\n\n      [[18, 19, 20],\n       [21, 22, 23],\n       [24, 25, 26]]])\n</code></pre> <p>#\u7701\u7565\u53f7\u53ef\u4ee5\u8868\u793a\u591a\u4e2a\u5192\u53f7, \u6240\u6709\u5757, \u6240\u6709\u5217\u7684\u7b2c1\u5217   print(a[:,:,1]) == print(a[...,1])   tensor([[ 1,  4,  7],           [10, 13, 16],           [19, 22, 25]])</p> <p>\u5bf9\u4e8e\u4e0d\u89c4\u5219\u7684\u5207\u7247\u63d0\u53d6,\u53ef\u4ee5\u4f7f\u7528torch.index_select, torch.masked_select, torch.take, torch.gather   \u8003\u8651\u73ed\u7ea7\u6210\u7ee9\u518c\u7684\u4f8b\u5b50\uff0c\u67094\u4e2a\u73ed\u7ea7\uff0c\u6bcf\u4e2a\u73ed\u7ea75\u4e2a\u5b66\u751f\uff0c\u6bcf\u4e2a\u5b66\u751f7\u95e8\u79d1\u76ee\u6210\u7ee9\u3002\u53ef\u4ee5\u7528\u4e00\u4e2a4\u00d75\u00d77\u7684\u5f20\u91cf\u6765\u8868\u793a\u3002   tensor([[[55, 95,  3, 18, 37, 30, 93],            [17, 26, 15,  3, 20, 92, 72],            [74, 52, 24, 58,  3, 13, 24],            [81, 79, 27, 48, 81, 99, 69],            [56, 83, 20, 59, 11, 15, 24]],</p> <pre><code>      [[72, 70, 20, 65, 77, 43, 51],\n       [61, 81, 98, 11, 31, 69, 91],\n       [93, 94, 59,  6, 54, 18,  3],\n       [94, 88,  0, 59, 41, 41, 27],\n       [69, 20, 68, 75, 85, 68,  0]],\n\n      [[17, 74, 60, 10, 21, 97, 83],\n       [28, 37,  2, 49, 12, 11, 47],\n       [57, 29, 79, 19, 95, 84,  7],\n       [37, 52, 57, 61, 69, 52, 25],\n       [73,  2, 20, 37, 25, 32,  9]],\n\n      [[39, 60, 17, 47, 85, 44, 51],\n       [45, 60, 81, 97, 81, 97, 46],\n       [ 5, 26, 84, 49, 25, 11,  3],\n       [ 7, 39, 77, 77,  1, 81, 10],\n       [39, 29, 40, 40,  5,  6, 42]]], dtype=torch.int32)\n</code></pre> <p>#\u62bd\u53d6\u6bcf\u4e2a\u73ed\u7ea7\u7b2c0\u4e2a\u5b66\u751f\uff0c\u7b2c2\u4e2a\u5b66\u751f\uff0c\u7b2c4\u4e2a\u5b66\u751f\u7684\u5168\u90e8\u6210\u7ee9; \u9009\u5b66\u751f, \u6240\u4ee5\u53d6dim 1   torch.index_select(scores,dim = 1,index = torch.tensor([0,2,4]))   tensor([[[55, 95,  3, 18, 37, 30, 93],            [74, 52, 24, 58,  3, 13, 24],            [56, 83, 20, 59, 11, 15, 24]],</p> <pre><code>      [[72, 70, 20, 65, 77, 43, 51],\n       [93, 94, 59,  6, 54, 18,  3],\n       [69, 20, 68, 75, 85, 68,  0]],\n\n      [[17, 74, 60, 10, 21, 97, 83],\n       [57, 29, 79, 19, 95, 84,  7],\n       [73,  2, 20, 37, 25, 32,  9]],\n\n      [[39, 60, 17, 47, 85, 44, 51],\n       [ 5, 26, 84, 49, 25, 11,  3],\n       [39, 29, 40, 40,  5,  6, 42]]], dtype=torch.int32)\n</code></pre> <p>#\u62bd\u53d6\u6bcf\u4e2a\u73ed\u7ea7\u7b2c0\u4e2a\u5b66\u751f\uff0c\u7b2c2\u4e2a\u5b66\u751f\uff0c\u7b2c4\u4e2a\u5b66\u751f\u7684\u7b2c1\u95e8\u8bfe\u7a0b\uff0c\u7b2c3\u95e8\u8bfe\u7a0b\uff0c\u7b2c6\u95e8\u8bfe\u7a0b\u6210\u7ee9   q = torch.index_select(torch.index_select(scores,dim = 1,index = torch.tensor([0,2,4]))                      ,dim=2,index = torch.tensor([1,3,6]))   tensor([[[95, 18, 93],            [52, 58, 24],            [83, 59, 24]],</p> <pre><code>      [[70, 65, 51],\n       [94,  6,  3],\n       [20, 75,  0]],\n\n      [[74, 10, 83],\n       [29, 19,  7],\n       [ 2, 37,  9]],\n\n      [[60, 47, 51],\n       [26, 49,  3],\n       [29, 40, 42]]], dtype=torch.int32)\n</code></pre> <p>#\u62bd\u53d6\u7b2c0\u4e2a\u73ed\u7ea7\u7b2c0\u4e2a\u5b66\u751f\u7684\u7b2c0\u95e8\u8bfe\u7a0b\uff0c\u7b2c2\u4e2a\u73ed\u7ea7\u7684\u7b2c3\u4e2a\u5b66\u751f\u7684\u7b2c1\u95e8\u8bfe\u7a0b\uff0c\u7b2c3\u4e2a\u73ed\u7ea7\u7684\u7b2c4\u4e2a\u5b66\u751f\u7b2c6\u95e8\u8bfe\u7a0b\u6210\u7ee9   #take\u5c06\u8f93\u5165\u770b\u6210\u4e00\u7ef4\u6570\u7ec4\uff0c\u8f93\u51fa\u548cindex\u540c\u5f62\u72b6   s = torch.take(scores,torch.tensor([057+0,257+37+1,357+47+6]))   tensor([55, 52, 42], dtype=torch.int32)</p> <p>#\u62bd\u53d6\u5206\u6570\u5927\u4e8e\u7b49\u4e8e80\u5206\u7684\u5206\u6570\uff08\u5e03\u5c14\u7d22\u5f15\uff09   #\u7ed3\u679c\u662f1\u7ef4\u5f20\u91cf   g = torch.masked_select(scores,scores&gt;=80)   tensor([95, 93, 92, 81, 81, 99, 83, 81, 98, 91, 93, 94, 94, 88, 85, 97, 83, 95,           84, 85, 81, 97, 81, 97, 84, 81], dtype=torch.int32)</p> <p>\u4ee5\u4e0a\u8fd9\u4e9b\u65b9\u6cd5\u4ec5\u80fd\u63d0\u53d6\u5f20\u91cf\u7684\u90e8\u5206\u5143\u7d20\u503c\uff0c\u4f46\u4e0d\u80fd\u66f4\u6539\u5f20\u91cf\u7684\u90e8\u5206\u5143\u7d20\u503c\u5f97\u5230\u65b0\u7684\u5f20\u91cf\u3002</p> <p>\u5982\u679c\u8981\u901a\u8fc7\u4fee\u6539\u5f20\u91cf\u7684\u90e8\u5206\u5143\u7d20\u503c\u5f97\u5230\u65b0\u7684\u5f20\u91cf\uff0c\u53ef\u4ee5\u4f7f\u7528torch.where,torch.index_fill \u548c torch.masked_fill</p> <p>torch.where\u53ef\u4ee5\u7406\u89e3\u4e3aif\u7684\u5f20\u91cf\u7248\u672c\u3002</p> <p>torch.index_fill\u7684\u9009\u53d6\u5143\u7d20\u903b\u8f91\u548ctorch.index_select\u76f8\u540c\u3002</p> <p>torch.masked_fill\u7684\u9009\u53d6\u5143\u7d20\u903b\u8f91\u548ctorch.masked_select\u76f8\u540c\u3002   #\u5982\u679c\u5206\u6570\u5927\u4e8e60\u5206\uff0c\u8d4b\u503c\u62101\uff0c\u5426\u5219\u8d4b\u503c\u62100   ifpass = torch.where(scores&gt;60,torch.tensor(1),torch.tensor(0))</p> <p>#\u5c06\u6bcf\u4e2a\u73ed\u7ea7\u7b2c0\u4e2a\u5b66\u751f\uff0c\u7b2c2\u4e2a\u5b66\u751f\uff0c\u7b2c4\u4e2a\u5b66\u751f\u7684\u5168\u90e8\u6210\u7ee9\u8d4b\u503c\u6210\u6ee1\u5206   torch.index_fill(scores,dim = 1,index = torch.tensor([0,2,4]),value = 100)   #\u7b49\u4ef7\u4e8e scores.index_fill(dim = 1,index = torch.tensor([0,2,4]),value = 100)</p> <p>#\u5c06\u5206\u6570\u5c0f\u4e8e60\u5206\u7684\u5206\u6570\u8d4b\u503c\u621060\u5206   b = torch.masked_fill(scores,scores&lt;60,60)   #\u7b49\u4ef7\u4e8eb = scores.masked_fill(scores&lt;60,60)</p> <p><code>- \u5f20\u91cf\u7ef4\u5ea6\u53d8\u6362   collapsed:: true -</code>python   \u7ef4\u5ea6\u53d8\u6362\u76f8\u5173\u51fd\u6570\u4e3b\u8981\u6709 torch.reshape(\u6216\u8005\u8c03\u7528\u5f20\u91cf\u7684view\u65b9\u6cd5), torch.squeeze, torch.unsqueeze, torch.transpose</p> <p>torch.reshape \u53ef\u4ee5\u6539\u53d8\u5f20\u91cf\u7684\u5f62\u72b6\u3002</p> <p>torch.squeeze \u53ef\u4ee5\u51cf\u5c11\u7ef4\u5ea6\u3002</p> <p>torch.unsqueeze \u53ef\u4ee5\u589e\u52a0\u7ef4\u5ea6\u3002</p> <p>torch.transpose/torch.permute \u53ef\u4ee5\u4ea4\u6362\u7ef4\u5ea6\u3002</p> <p>minval,maxval = 0,255   a = (minval + (maxval-minval)*torch.rand([1,3,3,2])).int()   torch.Size([1, 3, 3, 2])   tensor([[[[126, 195],             [ 22,  33],             [ 78, 161]],</p> <pre><code>       [[124, 228],\n        [116, 161],\n        [ 88, 102]],\n\n       [[  5,  43],\n        [ 74, 132],\n        [177, 204]]]], dtype=torch.int32)\n</code></pre> <p># \u6539\u6210 \uff083,6\uff09\u5f62\u72b6\u7684\u5f20\u91cf   b = a.view([3,6]) #torch.reshape(a,[3,6])   tensor([[126, 195,  22,  33,  78, 161],           [124, 228, 116, 161,  88, 102],           [  5,  43,  74, 132, 177, 204]], dtype=torch.int32)</p> <p>\u5982\u679c\u5f20\u91cf\u5728\u67d0\u4e2a\u7ef4\u5ea6\u4e0a\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\uff0c\u5229\u7528torch.squeeze\u53ef\u4ee5\u6d88\u9664\u8fd9\u4e2a\u7ef4\u5ea6\u3002   a = torch.tensor([[1.0,2.0]])   s = torch.squeeze(a)   tensor([[1., 2.]])   tensor([1., 2.])   torch.Size([1, 2])   torch.Size([2])</p> <p>d = torch.unsqueeze(s,axis=0)   tensor([1., 2.])   tensor([[1., 2.]])   torch.Size([2])   torch.Size([1, 2])</p> <p>torch.transpose\u53ef\u4ee5\u4ea4\u6362\u5f20\u91cf\u7684\u7ef4\u5ea6\uff0ctorch.transpose\u5e38\u7528\u4e8e\u56fe\u7247\u5b58\u50a8\u683c\u5f0f\u7684\u53d8\u6362\u4e0a\u3002</p> <p>\u5982\u679c\u662f\u4e8c\u7ef4\u7684\u77e9\u9635\uff0c\u901a\u5e38\u4f1a\u8c03\u7528\u77e9\u9635\u7684\u8f6c\u7f6e\u65b9\u6cd5 matrix.t()\uff0c\u7b49\u4ef7\u4e8e torch.transpose(matrix,0,1)\u3002   minval=0   maxval=255   # Batch,Height,Width,Channel   data = torch.floor(minval + (maxval-minval)*torch.rand([100,256,256,4])).int()   print(data.shape)</p> <p># \u8f6c\u6362\u6210 Pytorch\u9ed8\u8ba4\u7684\u56fe\u7247\u683c\u5f0f Batch,Channel,Height,Width    # \u9700\u8981\u4ea4\u6362\u4e24\u6b21   data_t = torch.transpose(torch.transpose(data,1,2),1,3)   print(data_t.shape)</p> <p>data_p = torch.permute(data,[0,3,1,2]) #\u5bf9\u7ef4\u5ea6\u7684\u987a\u5e8f\u505a\u91cd\u65b0\u7f16\u6392   data_p.shape </p> <p><code>- \u5f20\u91cf\u5408\u5e76\u5206\u5272   collapsed:: true -</code>python   \u53ef\u4ee5\u7528torch.cat\u65b9\u6cd5\u548ctorch.stack\u65b9\u6cd5\u5c06\u591a\u4e2a\u5f20\u91cf\u5408\u5e76\uff0c\u53ef\u4ee5\u7528torch.split\u65b9\u6cd5\u628a\u4e00\u4e2a\u5f20\u91cf\u5206\u5272\u6210\u591a\u4e2a\u5f20\u91cf\u3002</p> <p>torch.cat\u548ctorch.stack\u6709\u7565\u5fae\u7684\u533a\u522b\uff0ctorch.cat\u662f\u8fde\u63a5\uff0c\u4e0d\u4f1a\u589e\u52a0\u7ef4\u5ea6\uff0c\u800ctorch.stack\u662f\u5806\u53e0\uff0c\u4f1a\u589e\u52a0\u7ef4\u5ea6\u3002</p> <p>a = torch.tensor([[1.0,2.0],[3.0,4.0]])   b = torch.tensor([[5.0,6.0],[7.0,8.0]])   c = torch.tensor([[9.0,10.0],[11.0,12.0]])</p> <p>abc_cat = torch.cat([a,b,c],dim = 0)   torch.Size([6, 2])   tensor([[ 1.,  2.],           [ 3.,  4.],           [ 5.,  6.],           [ 7.,  8.],           [ 9., 10.],           [11., 12.]])</p> <p>abc_stack = torch.stack([a,b,c],axis = 0) #torch\u4e2ddim\u548caxis\u53c2\u6570\u540d\u53ef\u4ee5\u6df7\u7528   print(abc_stack.shape)   print(abc_stack)   torch.Size([3, 2, 2])   tensor([[[ 1.,  2.],            [ 3.,  4.]],</p> <pre><code>      [[ 5.,  6.],\n       [ 7.,  8.]],\n\n      [[ 9., 10.],\n       [11., 12.]]])\n</code></pre> <p>torch.cat([a,b,c],axis = 1)   tensor([[ 1.,  2.,  5.,  6.,  9., 10.],           [ 3.,  4.,  7.,  8., 11., 12.]])</p> <p>torch.stack([a,b,c],axis = 1)   tensor([[[ 1.,  2.],            [ 5.,  6.],            [ 9., 10.]],</p> <pre><code>      [[ 3.,  4.],\n       [ 7.,  8.],\n       [11., 12.]]])\n</code></pre> <p>torch.split\u662ftorch.cat\u7684\u9006\u8fd0\u7b97\uff0c\u53ef\u4ee5\u6307\u5b9a\u5206\u5272\u4efd\u6570\u5e73\u5747\u5206\u5272\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u6bcf\u4efd\u7684\u8bb0\u5f55\u6570\u91cf\u8fdb\u884c\u5206\u5272\u3002   print(abc_cat)   a,b,c = torch.split(abc_cat,split_size_or_sections = 2,dim = 0) #\u6bcf\u4efd2\u4e2a\u8fdb\u884c\u5206\u5272   print(a)   print(b)   print(c)   tensor([[ 1.,  2.],           [ 3.,  4.],           [ 5.,  6.],           [ 7.,  8.],           [ 9., 10.],           [11., 12.]])   tensor([[1., 2.],           [3., 4.]])   tensor([[5., 6.],           [7., 8.]])   tensor([[ 9., 10.],           [11., 12.]])</p>"},{"location":"ml/PyTorch/#_1","title":"```","text":"<ul> <li>\u6a21\u578b #model #GitHub #ML</li> <li>vision/torchvision/models at main \u00b7 pytorch/vision \u00b7 GitHub</li> <li>AI Models - Computer Vision, Conversational AI, and More | NVIDIA NGC</li> <li>For Researchers | PyTorch</li> <li>https://modelzoo.co/framework/pytorch</li> <li>vision/torchvision/models at main \u00b7 pytorch/vision \u00b7 GitHub</li> <li>GitHub - Cadene/pretrained-models.pytorch: Pretrained ConvNets for pytorch: NASNet, ResNeXt, ResNet, InceptionV4, InceptionResnetV2, Xception, DPN, etc.</li> <li> </li> <li>PyTorch Container #Docker #Container #Linux #unix</li> <li>PyTorch Release Notes :: NVIDIA Deep Learning Frameworks Documentation</li> <li> </li> <li>PyTorch Documentation</li> <li>ResNet \u2014 Torchvision main documentation</li> <li> </li> </ul>"},{"location":"ml/PyTorch/#httpsmodelzoocomodelpretrained-modelspytorch","title":"https://modelzoo.co/model/pretrained-modelspytorch","text":"-"},{"location":"ml/PyTorch/#pytorch-nvidia-ngc","title":"PyTorch | NVIDIA NGC","text":""},{"location":"ml/PyTorch/#tensors-pytorch-tutorials-1131cu117-documentation","title":"Tensors \u2014 PyTorch Tutorials 1.13.1+cu117 documentation","text":"-"},{"location":"ml/numpy/","title":"NumPy","text":"<p>tags:: python, package,</p> <p>- - \u5b9e\u7528\u65b9\u6cd5     - ```python       numpy \u4e0e torch \u7c7b\u4f3c, \u770b\u62ec\u53f7\u7684\u4e2a\u6570\u53ef\u4ee5\u77e5\u9053\u7ef4\u5ea6</p> <pre><code>  a = np.array(999)\n  a.ndim\n  -&gt; 0\n\n  # shape \u53ef\u4ee5\u5f97\u77e5\u5f62\u72b6\n  b = np.array([[1],[1]])\n  b.shape\n  -&gt; (2, 1)\n\n  # size \u53ef\u4ee5\u5f97\u77e5\u603b\u5171\u5143\u7d20\u6570\u91cf\n  b.size\n  -&gt; 2\n\n  # dtype \u53ef\u4ee5\u5f97\u77e5\u6570\u636e\u7c7b\u578b\n  b.dtype\n  -&gt; int64\n\n  # reshape \u53ef\u4ee5\u66f4\u6539\u5f62\u72b6\n  b.reshape(1,2)\n  -&gt; [[1 1]]\n\n  # full \u6307\u5b9a\u586b\u5145\u503c\n  nines = np.full((2,3,4), 9)\n\n  # \u5229\u7528dtype\u5b9a\u4e49\u6570\u636e\u7c7b\u578b\n  arr = np.array([1,2,3], dtype=np.int64)\n\n  # where \u51fd\u6570 \u7b5b\u9009\u6761\u4ef6, \u4ece\u54ea\u91cc\u7b5b\u9009, \u4e0d\u7b26\u5408\u7684\u663e\u793a\n  np.where(mask, student_marks, np.nan)\n\n  # argwhere \u5f97\u5230\u7d22\u5f15\n  np.argwhere(mask)\n\n  # \u8f74\u5411, axis \u5728max\u7b49\u51fd\u6570\u4e2d\u7684\u7406\u89e3\n  max\u51fd\u6570\u4e3a\u4f8b, axis=0, \u5c31\u662f\u57280\u8f74\u53d8\u5316\u65b9\u5411\u538b\u7f29\u6210\u53ea\u6709\u4e00\u4e2a\u503c\n  \u4e8c\u7ef4\u6570\u7ec4\u5c31\u4f1a\u53ea\u5269\u4e0b\u4e00\u884c, \u4f46\u662f\u5217\u4fdd\u6301\u4e0d\u53d8\n  \u4e09\u7ef4\u6570\u7ec4\u5c31\u4f1a\u53ea\u5269\u4e0b\u4e00\u4e2a\u4e8c\u7ef4\u6570\u7ec4, \u4f46\u662f\u6bcf\u4e2a\u4e8c\u7ef4\u6570\u7ec4\u7684\u884c\u548c\u5217\u4e0d\u53d8\n\n  # copy\u65b9\u6cd5\u521b\u5efa\u4e00\u4e2a\u72ec\u7acb\u5185\u5b58\u7684\u62f7\u8d1d, \u4e0d\u4f1a\u4e92\u76f8\u5f71\u54cd\n  c = b.copy()\n\n  # unique \u53bb\u91cd\n  d = np.unique(c)\n\n  # concatenate \u5c3e\u9996\u62fc\u63a5\n  np.concatenate((arr1, arr2))\n\n  # insert \u63d2\u5165\u6307\u5b9a\u4f4d\u7f6e\n  np.insert(arr2, 1, arr1)\n\n  # delete \u5220\u9664\u5143\u7d20\n  np.delete(arr3, 0) \u4f1a\u8fd4\u56de\u88ab\u5220\u9664\u5143\u7d20\n\n  ```\n</code></pre> <ul> <li>Tutorial<ul> <li>Introduction to NumPy and Matplotlib &gt; Chapter 1: Introduction | Python Programming (70053 Autumn Term 2022/2023) | Department of Computing | Imperial College London</li> <li> </li> </ul> </li> </ul>"},{"location":"ml/numpy/#python-numpy-tutorial-with-jupyter-and-colab","title":"Python Numpy Tutorial (with Jupyter and Colab)","text":"-"},{"location":"networking/","title":"Networking","text":""},{"location":"networking/Cloudflare/","title":"Cloudflare","text":"<ul> <li>\u4e4b\u524d\u7ed9\u624b\u673a\u91cc\u4e0b\u8f7d\u4e86\u4e00\u4e2a1.1.1.1, \u662f\u53ef\u4ee5\u7528\u6765\u5efa\u7acb\u5b89\u5168\u94fe\u63a5, \u914d\u5408telegram bot\u53ef\u4ee5\u83b7\u5f97\u7528\u4e0d\u5b8c\u7684\u6d41\u91cf</li> <li>Cloudflare\u6709\u4e00\u4e2azero trust\u670d\u52a1\u53ef\u4ee5\u7528\u6765tunnel, \u53ef\u4ee5\u5b9e\u73b0\u7279\u5b9a\u7aef\u53e3\u5e94\u7528\u751a\u81f3\u6574\u4e2a\u7f51\u7edc\u73af\u5883\u7684\u5185\u7f51\u7a7f\u900f<ul> <li>Via the command line \u00b7 Cloudflare Zero Trust docs</li> <li>\u4f7f\u7528cloudflare tunnel\u6253\u6d1e\uff0c\u968f\u65f6\u968f\u5730\u8bbf\u95ee\u5185\u7f51\u670d\u52a1 - yunyuyuan blog</li> </ul> </li> <li>\u7c7b\u4f3c\u4e8evercel\u548cgithub page, cloudflare\u4e5f\u63d0\u4f9b\u6258\u7ba1\u7f51\u9875\u7684\u670d\u52a1<ul> <li>Cloudflare Pages</li> </ul> </li> <li>\u5173\u4e8e[[SSL]] \u7684\u7406\u89e3, cloudflare\u91cc\u9762\u6709edge\u548corigin\u4e24\u4e2acertificate, \u76f8\u5f53\u4e8ecf\u4ee3\u66ff\u6211\u4eec\u53bb\u8bbf\u95eeorigin. \u6211\u4eec\u8bbf\u95eecloudflare\u7684\u65f6\u5019\u7528\u7684PKI\u5206\u53d1\u7684\u516c\u94a5\u9a8c\u8bc1cf\u8eab\u4efd, \u800ccf\u8bbf\u95eeorigin\u65f6\u540c\u6837\u4e5f\u7528\u5230\u4e86\u6211\u4eec\u81ea\u5df1\u521b\u5efa\u7684\u79c1\u94a5\u548c\u516c\u94a5\u5bf9, cf\u7528\u516c\u94a5\u9a8c\u8bc1origin\u7b7e\u540d\u4fe1\u606f\u5efa\u7acbhttps\u94fe\u63a5<ul> <li></li> </ul> </li> <li>[[CSR]] Certificate Signing Request<ul> <li>\u4ec0\u4e48\u662fCSR\uff0cCSR\u6587\u4ef6\u7684\u4f5c\u7528\u548c\u751f\u6210</li> <li>\u751f\u6210\u79c1\u94a5\u7684\u540c\u65f6, \u751f\u6210\u4e00\u4e2aCSR\u53d1\u9001\u7ed9CA, \u751f\u6210\u8bc1\u4e66\u516c\u94a5\u6587\u4ef6, \u662f\u53d1\u7ed9\u7528\u6237\u7684\u7528\u6765\u9a8c\u8bc1\u6211\u4eec\u81ea\u5df1\u670d\u52a1\u5668\u7684\u8bc1\u4e66</li> </ul> </li> </ul>"},{"location":"networking/Nginx%20Proxy%20Manager/","title":"Nginx Proxy Manager","text":""},{"location":"networking/Nginx%20Proxy%20Manager/#_1","title":"\u5b89\u88c5","text":"<ul> <li>```bash   \u6ce8\u610f\u8981\u4f7f\u7528\u6700\u65b0\u7248\u672c\u7684docker-compose, \u4e0d\u7136\u4e0d\u9002\u914d   \u9996\u5148\u628a\u4e0b\u9762\u7684\u6587\u4ef6\u53d8\u6210 docker-compose.yml   version: '3.8'   services:     app:       image: 'jc21/nginx-proxy-manager:latest'       restart: always       ports:         - '80:80'         - '81:81'         - '443:443'       volumes:         - ./data:/data         - ./letsencrypt:/etc/letsencrypt</li> </ul> <p>\u7136\u540eup\u5b83    docker-compose up -d   ``` - #manual - 127.0.0.1 \u4e3a\u9ed8\u8ba4\u8bbf\u95ee\u70b9 - https://nginx.shiliangchen.xyz\u53ef\u4ee5\u628a\u81ea\u5df1\u53cd\u4ee3\u4e00\u4e0b\u76f4\u63a5\u8bbf\u95ee -  -  - proxy_buffering off; -</p>"},{"location":"networking/Nginx/","title":"Nginx","text":""},{"location":"networking/Nginx/#_1","title":"#\u5b89\u88c5","text":"<ul> <li>```bash   sudo apt-get update   sudo apt-get upgrade   sudo apt-get install nginx</li> </ul> <p><code>- \u6210\u529f\u914d\u7f6e\u4e86\u53cd\u5411\u4ee3\u7406\u955c\u50cf\u7f51\u7ad9, nginx manager \u7684data/nginx\u4e0b\u53ef\u4ee5\u914d\u7f6e   collapsed:: true -</code>bash   # ------------------------------------------------------------   # gh.csl122.com   # ------------------------------------------------------------</p> <p>server       {           listen 80;           listen 443 ssl;           ssl on;           # Custom SSL           ssl_certificate /data/custom_ssl/npm-3/fullchain.pem;           ssl_certificate_key /data/custom_ssl/npm-3/privkey.pem;           ssl_session_cache shared:SSL:10m;           ssl_session_timeout  10m;               proxy_ssl_server_name on;               proxy_ssl_protocols TLSv1 TLSv1.1 TLSv1.2;           server_name gh.csl122.com;           add_header Strict-Transport-Security \"max-age=31536000\";</p> <pre><code>      if ( $scheme = http ){\n          return 301 https://$server_name$request_uri;\n      }\n\n      if ($http_user_agent ~* (baiduspider|360spider|haosouspider|googlebot|soso|bing|sogou|yahoo|sohu-search|yodao|YoudaoBot|robozilla|msnbot|MJ12bot|NHN|Twiceler)) {\n      return  403;\n      }\n\n      location / {\n      sub_filter github.com gh.csl122.com;\n      sub_filter_once off;\n      proxy_set_header X-Real-IP $remote_addr;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header Referer https://github.com;\n      proxy_set_header Host github.com;\n      proxy_pass https://github.com;\n      proxy_set_header Accept-Encoding \"\";\n      }\n</code></pre> <p>}</p> <p>server {           listen 443 ssl;        server_name gh.csl122.com;           return 301 https://gh.csl122.com$request_uri;</p> <pre><code>  ssl_certificate /data/custom_ssl/npm-3/fullchain.pem;\n  ssl_certificate_key /data/custom_ssl/npm-3/privkey.pem;\n</code></pre> <p>}</p> <p>server {       if (\\(host = gh.csl122.com) {           return 301 https://\\)host$request_uri;       } # managed by Certbot</p> <pre><code>      listen 80;\n      listen [::]:80; \n  server_name gh.csl122.com;\n      return 404; # managed by Certbot\n</code></pre> <p>}</p> <p><code>- \u60f3\u8981\u6240\u6709\u5b50\u57df\u540d\u90fd\u539f\u6837\u8df3\u8f6c\u5230\u5bf9\u5e94\u9875\u9762   collapsed:: true -</code>bash   # ------------------------------------------------------------   # *.korin.eu.org   # ------------------------------------------------------------</p> <p>server {     listen 80;     listen [::]:80;</p> <pre><code>listen 443 ssl http2;\nlisten [::]:443 ssl http2;\n\nserver_name \"~^(?!cat)(?&lt;subdomain&gt;.+)\\.korin\\.eu\\.org$\";\nreturn 301 $scheme://$subdomain.csl122.com$request_uri;\n\n# Custom SSL\nssl_certificate /data/custom_ssl/npm-1/fullchain.pem;\nssl_certificate_key /data/custom_ssl/npm-1/privkey.pem;\n</code></pre> <p>}   <code>- \u914d\u7f6e - `/etc/nginx/nginx.conf`\u4e2d\u914d\u7f6e     - gzip \u6ce8\u91ca\u6389     - include\u9700\u8981\u7684server\u914d\u7f6e     -</code>bash       user www-data;       worker_processes auto;       pid /run/nginx.pid;       include /etc/nginx/modules-enabled/*.conf;</p> <pre><code>  events {\n    worker_connections 768;\n    # multi_accept on;\n  }\n\n  http {\n\n    ##\n    # Basic Settings\n    ##\n\n    sendfile on;\n    tcp_nopush on;\n    tcp_nodelay on;\n    keepalive_timeout 65;\n    types_hash_max_size 2048;\n    # server_tokens off;\n\n    # server_names_hash_bucket_size 64;\n    # server_name_in_redirect off;\n\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n\n    ##\n    # SSL Settings\n    ##\n\n    ssl_protocols TLSv1 TLSv1.1 TLSv1.2 TLSv1.3; # Dropping SSLv3, ref: POODLE\n    ssl_prefer_server_ciphers on;\n\n    ##\n    # Logging Settings\n    ##\n\n    access_log /var/log/nginx/access.log;\n    error_log /var/log/nginx/error.log;\n\n    ##\n    # Gzip Settings\n    ##\n\n    # gzip on;\n\n    # gzip_vary on;\n    # gzip_proxied any;\n    # gzip_comp_level 6;\n    # gzip_buffers 16 8k;\n    # gzip_http_version 1.1;\n    # gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;\n\n    ##\n    # Virtual Host Configs\n    ##\n\n    include /etc/nginx/conf.d/*.conf;\n    include /etc/nginx/sites-enabled/*;\n  }\n\n  #mail {\n  # # See sample authentication script at:\n  # # [Using a PHP Script on an Apache Server as the IMAP Auth Backend | NGINX](http://wiki.nginx.org/ImapAuthenticateWithApachePhpScript)\n  # \n  # # auth_http localhost/auth.php;\n  # # pop3_capabilities \"TOP\" \"USER\";\n  # # imap_capabilities \"IMAP4rev1\" \"UIDPLUS\";\n  # \n  # server {\n  #     listen     localhost:110;\n  #     protocol   pop3;\n  #     proxy      on;\n  # }\n  # \n  # server {\n  #     listen     localhost:143;\n  #     protocol   imap;\n  #     proxy      on;\n  # }\n  #}\n\n  ```\n</code></pre> <ul> <li>\u6ce8\u610f\u4e8b\u9879</li> <li> <p><code>/etc/nginx/sites-enabled/</code>\u4e2d\u5b58\u7740default</p> <ul> <li>80\u7aef\u53e3\u8046\u542chttp\u8bf7\u6c42, 443\u7aef\u53e3\u8046\u542cssl, \u52a0ssl\u6307\u4ee4enable ssl\u52a0\u5bc6</li> <li>default_server \u544a\u8bc9nginx\u8fd9\u4e2aserver block\u4f5c\u4e3a\u9ed8\u8ba4\u7684server, \u6ca1\u6709\u5728\u522b\u7684\u5730\u65b9\u6307\u5b9a\u7684\u57df\u540d\u90fd\u4f1a\u88ab\u5bfc\u5411\u8fd9\u4e2aserver</li> <li>\u8bc1\u4e66\u9700\u8981\u63d0\u524d\u653e\u5230\u670d\u52a1\u5668\u4e2d, \u5e76\u4e14\u5728\u4e0b\u9762\u6307\u5b9a\u4f4d\u7f6e</li> <li> <p>```bash   server {     listen 80 default_server;     listen [::]:80 default_server;</p> </li> </ul> <p>server {       listen 80;       listen [::]:80;       listen 443 ssl;       listen [::]:443 ssl;</p> <pre><code>  ssl_certificate /home/ubuntu/shiliangchen.xyz.pem;\n  ssl_certificate_key /home/ubuntu/shiliangchen.xyz.key;\n\n  server_name mongo.shiliangchen.xyz;\n\n  location / {\n      proxy_pass http://localhost:8081;\n      # proxy_set_header Host $host;\n      # proxy_set_header X-Real-IP $remote_addr;\n      proxy_buffering off;\n  }\n</code></pre> <p>}   ```</p> </li> </ul>"},{"location":"networking/Nginx/#ssl-configuration","title":"SSL configuration","text":""},{"location":"networking/Nginx/#_2","title":"Nginx","text":"<p>listen 443 ssl default_server; listen [::]:443 ssl default_server; ssl_certificate /home/ubuntu/shiliangchen.xyz.pem; ssl_certificate_key /home/ubuntu/shiliangchen.xyz.key;</p>"},{"location":"networking/Nginx/#_3","title":"Nginx","text":""},{"location":"networking/Nginx/#note-you-should-disable-gzip-for-ssl-traffic","title":"Note: You should disable gzip for SSL traffic.","text":""},{"location":"networking/Nginx/#see-773332-default-nginxconf-leaves-sites-vulnerable-to-breach-debian-bug-report-logs","title":"See: #773332 - Default nginx.conf leaves sites vulnerable to BREACH - Debian Bug report logs","text":""},{"location":"networking/Nginx/#_4","title":"Nginx","text":""},{"location":"networking/Nginx/#read-up-on-ssl_ciphers-to-ensure-a-secure-configuration","title":"Read up on ssl_ciphers to ensure a secure configuration.","text":""},{"location":"networking/Nginx/#see-765782-nginx-the-sample-tls-config-should-recommend-a-better-cipher-list-debian-bug-report-logs","title":"See: #765782 - nginx: The sample TLS config should recommend a better cipher list - Debian Bug report logs","text":""},{"location":"networking/Nginx/#_5","title":"Nginx","text":""},{"location":"networking/Nginx/#self-signed-certs-generated-by-the-ssl-cert-package","title":"Self signed certs generated by the ssl-cert package","text":""},{"location":"networking/Nginx/#dont-use-them-in-a-production-server","title":"Don't use them in a production server!","text":""},{"location":"networking/Nginx/#_6","title":"Nginx","text":""},{"location":"networking/Nginx/#include-snippetssnakeoilconf","title":"include snippets/snakeoil.conf;","text":"<p>root /var/www/html;</p>"},{"location":"networking/Nginx/#add-indexphp-to-the-list-if-you-are-using-php","title":"Add index.php to the list if you are using PHP","text":"<p>index index.html index.htm index.nginx-debian.html;</p> <p>server_name shiliangchen.xyz;</p> <p>location / {     # First attempt to serve request as file, then     # as directory, then fall back to displaying a 404.     try_files $uri $uri/ =404; }           <code>- `/etc/nginx/conf.d/chat3000.conf`\u662f\u81ea\u5b9a\u4e49\u7684server block\u6587\u4ef6         - \u53ef\u4ee5\u6307\u5b9a\u591a\u4e2aserver, \u4f8b\u5982\u7b2c\u4e00\u4e2a\u662fchatgpt\u7684, \u7b2c\u4e8c\u4e2a\u662fmongo db\u7684         -</code>bash           server {   listen 80;   listen [::]:80;   listen 443 ssl;   listen [::]:443 ssl;</p> <p>ssl_certificate /home/ubuntu/shiliangchen.xyz.pem;   ssl_certificate_key /home/ubuntu/shiliangchen.xyz.key;</p> <p>server_name shiliangchen.xyz www.shiliangchen.xyz;</p> <p>location / {       proxy_pass http://localhost:3000;       # proxy_set_header Host $host;       # proxy_set_header X-Real-IP $remote_addr;       proxy_buffering off;   }           }</p>"},{"location":"nlp/","title":"NLP","text":""},{"location":"nlp/Natural%20Language%20Processing/","title":"Natural Language Processing","text":"<p>tags:: IC, Course, ML, Uni-S10 alias:: NLP, \u81ea\u7136\u8bed\u8a00\u5904\u7406 Ratio: 7:3 Time: \u661f\u671f\u4e00 16:00 - 18:00, \u661f\u671f\u56db 11:00 - 13:00</p> <ul> <li>"},{"location":"nlp/Natural%20Language%20Processing/#_1","title":"\u5bb9\u6613\u5fd8\u8bb0\u7684\u4e1c\u897f &amp; \u65b0\u4e1c\u897f","text":"<ul> <li>BERT\u7684embedding\u7528\u7684\u662fToken Embeddings \u662f\u968f\u7f51\u7edc\u4e00\u8d77\u8bad\u7ec3\u7684, \u7528\u7684\u662fnn.Embedding. \u5e76\u6ca1\u6709\u7528\u5176\u4ed6\u9884\u8bad\u7ec3\u7684embedding \u4f8b\u5982word2vec. torch.nn\u5305\u4e0b\u7684Embedding\uff0c\u4f5c\u4e3a\u8bad\u7ec3\u7684\u4e00\u5c42\uff0c\u968f\u6a21\u578b\u8bad\u7ec3\u5f97\u5230\u9002\u5408\u7684\u8bcd\u5411\u91cf\u3002</li> <li>Transformer\u4e2d\u7684\u5f88\u591a\u5c42, \u5f88\u591ahead, \u4f1a\u6709\u5404\u81ea\u7684\u4fa7\u91cd\u70b9, \u4f8b\u5982\u4f1a\u6bcf\u4e2a\u8bcdattend to \u524d\u4e00\u4e2a, \u540e\u4e00\u4e2a, \u81ea\u5df1, \u6216\u8005\u53e5\u53f7\u7b49\u7b49. \u5f88\u795e\u5947\u7684\u662fSEP\u7684\u6ce8\u610f\u529b\u4f1a\u88ab\u653e\u5f88\u591a   collapsed:: true<ul> <li></li> <li></li> <li>Words in a sentence tend to attend to the special token [SEP] using BERT because [SEP] is used to indicate the end of one sentence and the beginning of another. By attending to [SEP], BERT can better understand the context and relationship between the two sentences. Additionally, the [SEP] token helps BERT to differentiate between different segments of the input text, which is important for tasks such as sentence classification and question answering.</li> </ul> </li> <li>FlashAttention<ul> <li>\u8981\u70b9\u5728\u4e8e\u5206\u89e3qkv, \u6bcf\u4e00\u4e2a\u5411\u91cf\u90fd\u5206\u6210\u5f88\u591a\u7247\u6765\u5206\u522b\u8ba1\u7b97, \u8fd9\u6837\u5b50\u53ef\u4ee5\u653e\u5230\u66f4\u5feb\u901f\u7684SRAM\u4e2d, \u800c\u4e0d\u662f\u5728HBM\u91cc\u9762\u5904\u7406, \u6bd4\u5982\u8bf4q\u7684\u524d\u4e24\u4e2a, k\u7684\u524d\u4e24\u4e2a, v\u7684\u524d\u4e24\u4e2a, \u5148\u4e00\u8d77\u8fdb\u884c\u4e00\u4e2aatt\u64cd\u4f5c, \u6700\u540e\u5408\u5e76\u6240\u6709</li> <li></li> <li>\u5728\u4f20\u7edf\u7b97\u6cd5\u4e2d\uff0c\u4e00\u79cd\u65b9\u5f0f\u662f\u5c06Mask\u548cSoftMax\u90e8\u5206\u878d\u5408\uff0c\u4ee5\u6e1b\u5c11\u8bbf\u5b58\u6b21\u6570\u3002\u7136\u800c\uff0cFlashAttention\u5219\u66f4\u52a0\u6fc0\u8fdb\uff0c\u5b83\u5c06\u4ece\u8f93\u5165Q,\u533a\uff0cV\u5230\u8f93\u51fa \u3002\u7684\u6574\u4e2a\u8fc7\u7a0b\u8fdb\u884c\u878d\u5408\uff0c\u4ee5\u907f\u514d S\uff0c\u536b\u77e9\u9635\u7684\u5b58\u50a8\u5f00\u9500\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u5ef6\u8fdf\u7f29\u51cf\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8f93\u5165\u7684\u957f\u5ea6\u516b\u901a\u5e38\u5f88\u957f\uff0c\u65e0\u6cd5\u5b8c\u5168\u5c06\u5b8c\u6574\u7684 Q,K,V,0\u53ca\u4e2d\u95f4\u8ba1\u7b97\u7ed3\u679c\u5b58\u50a8\u5728SRAM\u4e2d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4f9d\u8d56HBM\u8fdb\u884c\u8bbf\u5b58\u64cd\u4f5c\uff0c\u4e0e\u539f\u59cb\u8ba1\u7b97\u5ef6\u8fdf\u76f8\u6bd4\u6ca1\u6709\u592a\u5927\u5dee\u5f02\uff0c\u751a\u81f3\u4f1a\u53d8\u6162\uff08\u6ca1\u5177\u4f53\u6d4b\uff09\u3002</li> </ul> </li> <li> <p>sequence (\u53e5\u5b50) -&gt; <code>inputs = tokenizer(sequence)</code> -&gt;<code>encoded_sequence = inputs[\"input_ids\"]</code> -&gt; <code>[101, 138, 18696, 155, 1942, 3190</code> (\u8bcd\u5178\u4e2d\u7684indices) -&gt; <code>tokenizer.decode</code> -&gt; sequence</p> <ul> <li>tokenizer.tokenizer(sequence) \u4f1a\u8f93\u51fa\u65ad\u5f00\u7684\u8bcd\u6c47\u5217\u8868<code>['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']</code></li> <li>input_ids \u662fencoder \u6a21\u578b\u7684\u8f93\u5165</li> <li>Glossary</li> <li>```python   # %%   from PIL import Image   import requests</li> </ul> <p>from transformers import CLIPTokenizer, CLIPTextModel</p> <p>version = \"openai/clip-vit-large-patch14\"</p> <p>transformer = CLIPTextModel.from_pretrained(version).eval()   tokenizer = CLIPTokenizer.from_pretrained(version)</p> <p># %%   prompts = [\"a photo of a cat\", \"a photo of a dog\"]</p> <p>batch_encoding = tokenizer(prompts, truncation=True, max_length=77, return_length=True,                                           return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")</p> <p>tokens = batch_encoding[\"input_ids\"].to('cpu')   out = transformer(input_ids=tokens).last_hidden_state</p> <p># %%   print(out.shape)   torch.Size([2, 77, 768])</p> <p>print(batch_encoding.keys())   dict_keys(['input_ids', 'length', 'attention_mask'])</p> <p>print(tokenizer.decode(batch_encoding[\"input_ids\"][0]))   &lt;|startoftext|&gt;a photo of a cat &lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;</p> <p>print(tokenizer.tokenize(prompts[0]))   ['a', 'photo', 'of', 'a', 'cat']</p> <p>print(transformer(input_ids=tokens).keys())   odict_keys(['last_hidden_state', 'pooler_output'])   ``` - ## Notes     - \u672f\u8bed\u8868       collapsed:: true - [[LM]]: Language model - [[NLI]]: Natural Language Inference, \u7ed9\u51fa\u4e24\u53e5\u8bdd\u4e4b\u95f4\u7684\u5173\u7cfb - [[TF-IDF]]: Term Frequency-Inverse Document Frequency - [[FFNN]]: Feed Forward Neural Network - [[RNN]] : Recurrent Neural Network - [[LSTM]]: Long Short-Term Memory - [[BPTT]]: back-propagate through time - [[BiRNN]]: BiDirectional RNN - [[Auto-regressive]]: \u81ea\u56de\u5f52 - [[MT]]: Machine Translation - [[Attention]]: \u6ce8\u610f\u529b\u673a\u5236   collapsed:: true     - a dynamic weighted average     - In the immediate context, it allows us to dynamically look at individual tokens in       the input and decide how much weighting a token should have with respect to the current timestep of decoding.     - Types of attention       collapsed:: true         - additive/MLP         - Multiplicative         - Self-attention - [[MLP Attention]]: \u52a0\u6027attention   collapsed:: true     - {{embed ((63e1a7c2-05bb-4762-b60f-e7ca15370cd7))}} - [[Self-attention]]: \u81ea\u6ce8\u610f\u529b   collapsed:: true     -      - \u7ed9\u8fdb\u4e00\u4e2a\u5e8f\u5217, \u5e8f\u5217\u4e2d\u6709\u82e5\u5e72\u8bcd\u8bed, \u8bcd\u8bed\u4eec\u5df2\u7ecf\u88ab\u8868\u793a\u6210\u4e86embeddings, \u4e3a\u4e86\u5f97\u5230\u6bcf\u4e2a\u8bcd\u8bed\u5728\u8bed\u5883\u4e2d\u7684embedding, \u6211\u4eec\u8fdb\u884cself-attention, \u6bcf\u4e2a\u8bcd\u8bed\u672c\u8eab\u7684embedding\u7528\u53ef\u5b66\u4e60\u7684w\u8f6c\u6362\u6210kqv, \u7528q \u53bbquery\u5176\u4ed6\u8bcd\u8bed\u7684k\u6765\u83b7\u5f97score, \u7528\u8fd9\u4e2asoftmax\u4ee5\u540e\u7684score\u6765\u5408\u6210\u81ea\u5df1\u7684\u65b0value, \u4f5c\u4e3a\u5b83\u7684\u5177\u6709\u8bed\u5883\u610f\u4e49\u7684\u65b0\u8868\u793a. - [[Multi-head Self-attention]]: \u591a\u5934\u81ea\u6ce8\u610f\u529b   collapsed:: true     - \u5728\u8fd9\u4e2a\u57fa\u7840\u4e0a\u7528\u4e86\u591a\u4e2aattention head, \u4e0d\u540c\u7684weight\u6765\u91cd\u65b0\u5efa\u7acbqkv, \u4ee5\u671f\u671b\u83b7\u5f97\u6ce8\u610f\u529b\u673a\u5236\u7ec4\u5408\u4f7f\u7528\u67e5\u8be2\u3001\u952e\u548c\u503c\u7684\u4e0d\u540c \u5b50\u7a7a\u95f4\u8868\u793a(representation subspaces)\u53ef \u80fd\u662f\u6709\u76ca\u7684\u3002     - \u5e0c\u671b\u6a21\u578b\u53ef\u4ee5\u57fa\u4e8e\u76f8\u540c\u7684\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u5230\u4e0d\u540c\u7684\u884c \u4e3a\uff0c\u7136\u540e\u5c06\u4e0d\u540c\u7684\u884c\u4e3a\u4f5c\u4e3a\u77e5\u8bc6\u7ec4\u5408\u8d77\u6765\uff0c\u6355\u83b7\u5e8f\u5217\u5185\u5404\u79cd\u8303\u56f4\u7684\u4f9d\u8d56\u5173\u7cfb(\u4f8b\u5982\uff0c\u77ed\u8ddd\u79bb\u4f9d\u8d56\u548c\u2ed3\u8ddd\u79bb\u4f9d \u8d56\u5173\u7cfb)\u3002     -  - [[BERT]]: Bidirectional Encoder Representations from Transformers   collapsed:: true     - BERT\u662f\u4e00\u4e2a\u7531Transformer\u800c\u6765\u7684\u53cc\u5411\u7f16\u7801\u5668, \u4ec5\u5305\u542b\u7f16\u7801\u8fc7\u7a0b, \u76ee\u7684\u662f\u4eff\u7167cv\u9886\u57df\u7684feature extractor, \u505a\u5230\u80fd\u591f\u7ed9\u4e0b\u6e38\u4efb\u52a1\u590d\u7528, \u56e0\u6b64\u66f4\u50cf\u662f\u4e00\u4e2a\u6587\u672c\u7406\u89e3\u5668, \u8fd0\u7528\u7684\u4e5f\u662f\u591a\u5934\u81ea\u6ce8\u610f\u529b, \u5176\u5b9e\u5c31\u662ftransformer\u7684\u5757block.     - \u4e0d\u540c\u4e4b\u5904\u5728\u4e8e, BERT\u4f7f\u7528\u4e86segment embedding\u7247\u6bb5\u5d4c\u5165\u548c\u53ef\u5b66\u4e60\u7684positional embedding, BERT\u7684\u8f93\u5165\u9700\u8981\u9884\u5148\u5904\u7406\u8fc7, \u53ef\u4ee5\u662f\u4e00\u53e5\u8bdd, \u4e5f\u53ef\u4ee5\u662f\u4e24\u53e5\u8bdd, \u7b2c\u4e00\u53e5\u8bddsegment \u4f1a\u6807\u8bb0\u62100000, \u7b2c\u4e8c\u53e5\u8bdd\u4f1a\u6807\u8bb0\u62101111. \u8f93\u5165\u7684\u6700\u524d\u9762\u4f1aprepend \u4e0a token, \u7528\u6765\u540e\u7eed\u8f93\u51fa\u5206\u7c7b\u4efb\u52a1\u7528, \u6bcf\u53e5\u8bdd\u7684\u7ed3\u5c3e\u4f1aappend\u4e0a token \u7528\u6765\u8868\u793a\u53e5\u5b50\u4e4b\u95f4\u7684\u5206\u5272. \u8fdb\u5165\u7f51\u7edc\u7684\u771f\u6b63\u8f93\u5165\u662fembedded tokens + embedded segments + embedded position.     - segment embedding: \u75280\u6807\u8bb0\u7b2c\u4e00\u53e5\u8bdd\u7684token, 1\u6807\u8bb0\u7b2c\u4e8c\u53e5\u8bdd\u7684token; \u4f8b\u5982[00001111]     - BERT\u5bf9\u4e0e\u8bcd\u8bed\u906e\u853d\u9884\u6d4b(\u7ed9\u5b9amask\u4f4d\u6765\u5bf9vocab\u9884\u6d4b\u6982\u7387) \u548c\u4e0b\u4e00\u53e5\u5bf9\u4e0d\u5bf9\u9884\u6d4b\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3, \u5982\u679c\u6211\u4eec\u8981\u8fdb\u884cfine tuning \u5de5\u4f5c\u7684\u8bdd\u53ea\u9700\u8981\u628aBERT\u7684\u6700\u540e\u5c42\u62ff\u6389, \u53ea\u62ff\u7b2c\u4e00\u4e2aclass token\u7684\u5b66\u4e60\u7ed3\u679c     - \u4f8b: vovab = 10000, num_hiddens = 768; \u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u4e3a8, \u4e00\u5171\u4e24\u4e2asamples batch size = 2     - BERT\u4f1atake (2, 8)\u7684tokens, \u5bf9\u4ed6\u8fdb\u884c\u4e00\u7cfb\u5217embedding, \u8f93\u5165\u5230transformer blocks\u91cc, \u8f93\u51fa(2, 8, 768) - [[MLM]]: Masked Language Modeling - [[BPE]]: Byte Pair Encoding, \u4e00\u79cd\u7528\u4e8e\u628a\u6587\u672c\u4e2d\u8bcd\u8bed\u5206\u6210subparts\u6765\u8fdb\u884cstemming \u548clemmatisation\u7684\u624b\u6bb5 - [[ELMo]]: Embeddings from Language Models, \u4e24\u4e2a\u4ece\u524d\u5230\u540e\u548c\u4ece\u540e\u5230\u524d\u7684recurrent language model - [[LLM]]: Large Language Model - [[Zero-shot]]: \u7ed9\u51fainstruction without examples - [[Few-shot]]: \u7ed9\u51fainstruction with few examples - [[CoT]]: chain-of-thought - [[RLHF]]: Reinforcement learning from human feedback - [[POS]]: Part of Speech, \u8bcd\u6027\u7684\u610f\u601d, \u6bcf\u4e2a\u5143\u7d20\u7684tag, label, \u6709\u5f88\u591a\u79cd\u6807\u51c6 - INTJ (interjection): psst, ouch, hello, ow, \u611f\u53f9\u8bcd - PROPN (proper noun): UK, Jack, London, \u4e13\u6709\u540d\u8bcd - AUX (auxiliary verb): can, shall, must, \u52a9\u52a8\u8bcd - CCONJ (coordinating conjunction): and, or, \u8fde\u63a5\u8bcd - DET: (determiner): a, the, this, which, my, an, \u9650\u5b9a\u8bcd - [[NER]]: Named Entity Recognition, \u53c8\u79f0\u4f5c\u4e13\u540d\u8bc6\u522b\u3001\u547d\u540d\u5b9e\u4f53\uff0c\u662f\u6307\u8bc6\u522b\u6587\u672c(\u4e2d\u5177\u6709\u7279\u5b9a\u610f\u4e49\u7684\u5b9e\u4f53\uff0c\u4e3b\u8981\u5305\u62ec\u4eba\u540d\u3001\u5730\u540d\u3001\u673a\u6784\u540d\u3001\u4e13\u6709\u540d\u8bcd\u7b49\uff0c\u4ee5\u53ca\u65f6\u95f4\u3001\u6570\u91cf\u3001\u8d27\u5e01\u3001\u6bd4\u4f8b\u6570\u503c\u7b49\u6587\u5b57\u3002 - [[HMM]]: Hidden Markov Model, \u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b, Hidden Markov Chains: states are not given, but hidden   collapsed:: true   \u25cb Words are observed   \u25cb POS are hidden     - \u6709\u9690\u542b\u7684state\u6765\u51b3\u5b9a\u6982\u7387\u7684, \u9690\u542b\u7684states\u53ef\u4ee5\u4eceobservation\u4e2d\u63a8\u65ad\u51fa\u6765 - [[Viterbi]]: \u7ef4\u7279\u6bd4\u7b97\u6cd5\uff08\u82f1\u8bed\uff1aViterbi algorithm\uff09\u662f\u4e00\u79cd\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u3002\u5b83\u7528\u4e8e\u5bfb\u627e\u6700\u6709\u53ef\u80fd\u4ea7\u751f\u89c2\u6d4b\u4e8b\u4ef6\u5e8f\u5217\u7684\u7ef4\u7279\u6bd4\u8def\u5f84\u2014\u2014\u9690\u542b\u72b6\u6001\u5e8f\u5217\uff0c\u7279\u522b\u662f\u5728\u9a6c\u5c14\u53ef\u592b\u4fe1\u606f\u6e90\u4e0a\u4e0b\u6587\u548c\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u4e2d\u3002 - [[CNF]]: Chomsky Normal Form, X -&gt; YZ, X -&gt; w - [[CKY]] algorithm: bottom-up\u7684\u52a8\u6001\u89c4\u5212\u7b97\u6cd5, \u7528\u4e8e\u53e5\u5b50\u6210\u5206\u5206\u6790, \u5217\u8868\u5408\u6210, \u518d\u5408\u6210 - [[PCFG]]: Probabilistic/stochastic phrase structure grammar is a   context-free grammar PCFG = (T, N, S, R, q), where:   \u25cf T is set of terminals   \u25cf N is set of nonterminals   \u25cf S is the start symbol (non-terminal)   \u25cf R is set of rules X\u2192 ,where X is a nonterminal and is a sequence of terminals &amp; nonterminals   \u25cf q = P(R) gives the probability of each rule -     - Week2 intro &amp; word representations, word2vec, skip-gram, CBOW       collapsed:: true - Lecture 1 Introduction   collapsed:: true     - Natural language processing       collapsed:: true         - NLP is the processing of natural language by computers for a task         - Natural language is a language that has developed naturally in use         - Ultimate goal: systems to understand NL (in written language), execute requested tasks &amp; produce NL         - Key words: Machine Learning, Linguistics, Cognitive Science, Computer Science     - Subtasks of language understanding       collapsed:: true         - Lexicon           collapsed:: true             - \u8bcd\u6c47             - morphological analysis \u5f62\u6001\u5b66\u5206\u6790               collapsed:: true                 - What is a \u2018word\u2019 and what is it composed of?                   collapsed:: true                     - segmentation (\u5206\u5272\u8bcd\u6c47), Word normalisation (\u5c06\u4e0d\u540c\u5f62\u5f0f\u7684\u8bcd\u8bednormalise, \u4f8b\u5982\u5927\u5c0f\u5199, \u6709\u6ca1\u6709\u70b9, \u8fde\u4e0d\u8fde\u8d77\u6765, \u82f1\u5f0f\u7f8e\u5f0f),                     - Lemmatisation (\u5355\u590d\u6570, ing\u7b49\u5230base form), Stemming (reduce \u5230 root, \u4f8b\u5982connect, connction),                     - Part-of-speech tagging (\u8bc6\u522b\u8bcd\u8bed\u7684\u7c7b\u522b, \u4f8b\u5982\u540d\u8bcd, \u4ee3\u8bcd)                     - Morphological analysis (recognise/generate word variants): Number, gender, tense, \u524d\u7f00, \u540e\u7f00         - Syntax           collapsed:: true             - \u53e5\u6cd5, \u8bed\u6cd5, \u53e5\u5b50\u7684\u7ed3\u6784, how words are put together             -              -          - Semantics           collapsed:: true             - \u8bed\u4e49, \u8bcd\u8bed\u53e5\u5b50\u7684\u610f\u601d, Meaning of words in context             - Word Sense Disambiguation: given a word and its context, assign the correct meaning given a set of candidates, \u6b63\u786e\u7406\u89e3\u4e00\u4e2a\u8bcd\u5728\u4e00\u4e2a\u8bed\u5883\u4e2d\u7684\u542b\u4e49             - Understanding who did what to whom, when, where, how and why? \u7406\u89e3\u8c01\u5bf9\u8c01\u505a\u4e86\u4ec0\u4e48\u4e8b               collapsed:: true                 - Semantic role labelling: assign semantic roles to words in sentence; fail a sentence if that is not possible \u5bf9\u8bed\u4e49\u89d2\u8272\u8fdb\u884c\u6807\u8bb0         - Discourse           collapsed:: true             - \u8bba\u8ff0, \u53e5\u5b50\u4e0e\u53e5\u5b50\u95f4\u7684\u8054\u7cfb, How do sentences relate to each other?             - References &amp; relationships within and across sentences. \u524d\u540e\u53e5\u5b50\u4e4b\u95f4\u7684\u8054\u7cfb, \u4ee3\u8bcd\u7684\u6307\u4ee3         - Pragmatics           collapsed:: true             - \u8bed\u7528\u8bba, \u8bed\u8a00\u7684\u7528\u610f, What is the intent of the text? How to react to it?             -              - \u4f8b\u5982\u4e0a\u56fe\u95ee\u7684\u4e0d\u662fyes or no, \u800c\u662f\u6709\u5176\u4ed6\u7684\u9690\u542b\u610f\u601d, \u6307\u793a\u5bf9\u65b9\u505a\u4e9b\u4ec0\u4e48\u7684         - \u66fe\u7ecf\u8fd9\u4e9b\u662f\u88ab\u5355\u72ec\u5206\u522b\u89e3\u51b3\u7684, \u73b0\u5728\u6211\u4eec\u7528\u4e00\u4e2a\u7aef\u5230\u7aef\u6a21\u578b\u6765\u5904\u7406\u6240\u6709     - History       collapsed:: true         - 19050: Foundational work: automata theory, information theory         - 1960-1970: Rule-based models         - 1980-1990: the empirical revolution         - 2000\u2019s: better statistical and machine learning methods         - 2014+: the neural revolution         - 2018+: the era of pre-trained language models     - Why ML       collapsed:: true         - Creation and maintenance of linguistic rules often infeasible or impractical         - Learning functions from data instead creating rules based on intuition         - Optimizing weights and balances automatically instead of tuning           them manually         - Data/examples can be abundant for certain applications and languages, e.g. human translations, movie reviews, ...     - Why DL       collapsed:: true         -      - Applications       collapsed:: true         - Spam filtering         - Translation         - Grammar correction         - text prediction         - Stable diffusion         - ChatGPT - Lecture 1.2 Word Representations   collapsed:: true     - Words as vectors, word embeddings, skip-gram, Byte pair coding     - Similar words have similar vectors, \u76f8\u4f3c\u542b\u4e49\u7684\u8bcd\u8bed, \u6709\u76f8\u8fd1\u7684vector     - 1-hot vectors: \u72ec\u70ed\u7f16\u7801, 170k\u4e2a\u5355\u8bcd, 170k\u4e2abit, \u76f8\u4f3c\u7684\u8bcd\u8bed\u548c\u5b8c\u5168\u4e0d\u540c\u7684\u8bcd\u8bed\u6709\u76f8\u540c\u7684distance     - WordNet: map words to broader concepts       collapsed:: true         - \u201ccat\u201d, \u201ckitten\u201d \u2192\u201dfeline mammal\u201d           \u201cLondon\u201d, \u201cParis\u201d, \u201cTallinn\u201d \u2192 \u201cnational capital\u201d \u201cyellow\u201d, \u201cgreen\u201d \u2192 \u201ccolour\u201d         - \u4f46\u662f, rely on manual curate, miss out rare and new meanings, vectors are still \u4e92\u76f8\u5782\u76f4\u8ddd\u79bb\u76f8\u540c, Disambiguating word meanings is difficult     - Distributed vectors, distributed representations: Each element represents a property and these are shared between the words.       collapsed:: true         -          -          - cos\u662fsimilarity, \u8d8a\u5927\u8d8a\u597d, \u7528\u4e8e\u9ad8\u7eac; L2 norm\u662f\u8ddd\u79bb, \u8d8a\u5c0f\u8d8a\u597d         - \u4f46\u662f\u6211\u4eec\u4e0d\u60f3\u77e5\u9053\u8fd9\u4e9blabel, \u5e0c\u671b\u4ed6\u4eec\u53ef\u4ee5\u81ea\u52a8\u751f\u6210     - Distributional hypothesis       collapsed:: true         - Words which are similar in meaning occur in similar contexts. \u76f8\u4f3c\u7684\u8bcd\u8bed\u4f1a\u6709\u76f8\u8fd1\u7684\u966a\u4f34\u8bcd\u4eec         - Count-based vectors           collapsed:: true             - count how often a word occurs together with specific other words (within a context window of a particular size). \u901a\u8fc7\u4e00\u4e2a\u4e0e\u9644\u8fd1\u5176\u4ed6\u8bcd\u8bed\u540c\u65f6\u51fa\u73b0\u7684\u8bcd\u8bed\u7684\u6b21\u6570, \u4f8b\u5982 magazine \u548cnewspaper\u548c read\u7ecf\u5e38\u4e00\u8d77\u51fa\u73b0             - TF-IDF:               collapsed:: true                 - down-weighting \u90a3\u4e9b\u7ecf\u5e38\u51fa\u73b0\u5728\u6240\u6709\u5730\u65b9\u7684\u8bcd\u8bed                 -              - \u975e\u5e38sparse, very large 170k             - \u6240\u4ee5\u6211\u4eec\u6700\u597d\u5e0c\u671b\u80fd\u6709\u4e2a\u56fa\u5b9a\u5927\u5c0f\u7684parameters, \u8ba9\u795e\u7ecf\u7f51\u7edc\u53bb\u5b66, 300-1k\u7684\u957f\u5ea6     - Word2vec     - Continuous Bag-of-Words (CBOW)     - Skip-gram       collapsed:: true         - Predict the context words based on the target word wt, \u6216\u8005\u6839\u636ewt, \u9884\u6d4b\u5176\u5468\u56f4\u7684\u8bcd\u8bed         - target x \u76841-hot vector\u7ed9\u5230\u7f51\u7edc, \u7528weight W\u5f97\u5230\u4ed6\u7684embedding, \u518d\u7528W'\u5f97\u5230\u6574\u4e2a\u8bcd\u5178\u957f\u5ea6\u7684vector y, \u7528softmax\u627e\u5230\u53ef\u80fd\u7684\u8bcd\u6c47, \u7528\u7684\u662fcategorical cross-entropy         - \u6574\u4e2a\u6a21\u578b\u53ea\u6709\u4e24\u4e2aW, W'         -          - \u4f46\u662f, \u6211\u4eec\u9700\u8981\u5bf9\u6574\u4e2a\u8bcd\u5178\u8fdb\u884csoftmax \u7684\u8ba1\u7b97, \u8fd9\u4e00\u70b9downside,         - \u4f18\u5316           collapsed:: true             - \u4e0d\u75281-hot, \u76f4\u63a5\u83b7\u53d6\u4e00\u884cembedding             - negative sampling               collapsed:: true                 - \u8ba9\u53ef\u80fd\u7684\u8bcd\u8bed\u53ef\u80fd\u6027\u6700\u5927\u5316, \u4e0d\u5e94\u8be5\u7684\u8bcd\u8bed\u6700\u5c0f\u5316                 -                  - \u5c0f\u7684\u6570\u636e\u96c6, \u9700\u89815-20\u4e2a\u8d1f\u9762\u8bcd\u8bed, \u5927\u7684\u53ea\u9700\u89812-5\u4e2a, \u5927\u5927\u51cf\u5c11\u4e86\u53c2\u6570\u91cf     - Analogy Recovery, \u76f8\u4f3c\u5ea6\u6062\u590d       collapsed:: true         - \u4f8b\u5982queen = king - man + woman     - Multilingual word embeddings       collapsed:: true         - \u4e0d\u540c\u8bed\u8a00\u8bcd\u8bed\u653e\u5230\u4e00\u8d77     - Multimodal word embeddings       collapsed:: true         - \u56fe\u548c\u8bcd\u8bed     -     - - Tutorial Preprocessing and embedding   collapsed:: true     - Q: What is a token and why do we need to tokenize?       A: A token is a string of contiguous characters between two spaces, or between a space and punctuation marks. For segmentation.     - Capitalization inducing sparsity in the dataset.       A: Process of normalization (removing capitalization, etc.)     - \u51e0\u4e2aPre-processing methods, \u867d\u7136\u4ed6\u4eec\u4e0d\u5728neural-based NLP anymore, \u4f46extensively employed in rule-based and statistical NLP.       collapsed:: true         - ### Stop Word removal <pre><code>      Stop words are generally the most common words in the language which who's meaning in a sequenece is ambiguous. Some examples of stop words are: The, a, an, that.\n    - ### Punctuation removal\n      collapsed:: true\n\n      Old school NLP techniques (and some modern day ones) struggle to understand the semantics of punctuation. Thus, they were also removed.\n        - ```python\n          import re # regex\n          re_punctuation_string = '[\\s,/.\\']'\n          tokenized_sentence_PR = re.split(re_punctuation_string, sentence)\n          tokenized_sentence_PR = list(filter(None, tokenized_sentence_PR)) # return none empty strings\n          ```\n    - ### Stemming\n      collapsed:: true\n\n      In the case of stemming, we want to normalise all words to their stem (or root). The stem is the part of the word to which affixes (suffixes or prefixes) are assigned. Stemming a word may result in the word not actually being a word. For example, some stemming algorithms may stem [trouble, troubling, troubled] as \"troubl\". \u628a\u8bcd\u8bed\u7b80\u5316\u5230\u5176\u8bcd\u6839\n        - ```python\n          from nltk.stem import PorterStemmer\n          porter.stem(word)\n          ```\n    - ### Lemmatization\n      collapsed:: true\n\n      Lemmatization attempts to properly reduce unnormalized tokens to a word that belongs in the language. The root word is called a **lemma**, and is the canonical form of a set of words. For example, [runs, running, ran] are all forms of the word \"run. \u628a\u8bcd\u8bed\u7b80\u5316\u5230\u5176\u57fa\u7840\u5f62\u6001, the lemmatizer requires parts of speech (POS) context about the word it is currently parsing.\n        - ```python\n          from nltk.stem import WordNetLemmatizer\n          wordnet_lemmatizer = WordNetLemmatizer()\n          wordnet_lemmatizer.lemmatize(word, pos=\"v\")\n          ```\n    - Note that other NLP tools, such as [SpaCy](https://spacy.io/) or [Stanza](https://stanfordnlp.github.io/stanza/) are popular alternatives which provide higher levels of abstractions than NLTK.\n- #### Implementing a Word2Vec algorithm from scratch\n  collapsed:: true\n    - collapsed:: true\n      1. \u521b\u5efavocabulary\n        - ```\n          vocabulary = []\n          vocabulary.append(token)\n          ```\n    - collapsed:: true\n      2. \u521b\u5efaword2idx \u548c idx2word \u4e24\u4e2a\u5b57\u5178\n        - ```python\n          word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n          idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n          ```\n    - collapsed:: true\n      3. \u521b\u5efa look-up table, \u8bcd\u8bed\u52301hot\n        - ```\n          def look_up_table(word_idx):\n             x = torch.zeros(vocabulary_size).float()\n             x[word_idx] = 1.0\n             return x\n          ```\n    - collapsed:: true\n      4. Extracting contexts and the focus word\n        - \u5bf9\u53e5\u5b50[0, 2, 3, 6, 7] (\u6570\u5b57\u662f\u5b57\u5178\u91cc\u7684\u4f4d\u7f6e)\u91cc\u7684\u6bcf\u4e2a\u8bcd, \u627e\u5230\u5bf9\u5e94\u7684window size\u8303\u56f4\u5185\u7684\u5468\u56f4\u7684\u8bcd, \u5199\u6210[\u8fd9\u4e2a\u8bcdindex, \u5468\u56f4\u8bcdindex]\n    - collapsed:: true\n      5. \u7528w1 \u6765\u6620\u5c04\u5230embedding, w2\u6620\u5c04\u5230\u8bcd\u6c47\u8868\u6bcf\u4e2a\u8bcd\u7684\u76f8\u4f3c\u5ea6\u6982\u7387\n        - y \u662f\u5468\u56f4\u8bcdindex, \u4f1a\u88ab\u7f51\u7edc\u5f3a\u5316, \u76f8\u5f53\u4e8e\u54ea\u4e00\u7c7b\n        - \u6700\u540e\u5c31\u662f\u8f93\u5165\u8fdb\u67d0\u4e2a\u8bcd\u8bed, \u53ef\u4ee5\u7ed9\u51fa\u8bcd\u6c47\u8868\u4e2d\u6bcf\u4e2a\u8bcd\u4e0e\u5176\u7684\u76f8\u4f3c\u7a0b\u5ea6\n    - 6. skipgram \u4e2d\u7528\u4e86negative sampling, \u6765\u51cf\u5c0f\u975e\u76f8\u5173\u8bcd\u7684\u53ef\u80fd\u6027\n- **\u9884\u8bad\u7ec3\u6a21\u578b**, \u5982glove, \u4f1a\u67096billion\u4e2atokens, 40k\u4e2a\u8bcd\u6c47, \u6709\u591a\u4e2adimension\u7248\u672c, \u598250, 300\u7b49, \u5df2\u7ecf\u4e8b\u5148\u7ed9\u6bcf\u4e2a\u8bcd\u8bed\u505a\u597d\u4e86word representation\n- Week3 classification, \u6734\u7d20\u8d1d\u53f6\u65af, \u7f57\u8f91\u56de\u5f52, CNN, debiasing\n  collapsed:: true\n</code></pre> <ul> <li>Lecture 2 Classification   collapsed:: true<ul> <li>\u4ec0\u4e48\u662fclassification:   collapsed:: true<ul> <li>\\(y_{predicted} = argmax_yP(y|x)\\)</li> <li>\u9884\u6d4b\u7684label\u662f\u8ba9given x\u8fd9\u4e2a\u6570\u636e\u96c6\u65f6, \u53d1\u751f\u6982\u7387\u6700\u9ad8\u7684\u90a3\u4e2alabely, \u4e0a\u9762\u7684\u5f0f\u5b50\u4e5f\u53eb\u505a\u540e\u9a8c\u6982\u7387</li> <li>NLP tasks   collapsed:: true<ul> <li></li> </ul> </li> </ul> </li> <li>Naive Bayes Classifier [[\u6734\u7d20\u8d1d\u53f6\u65af]]   collapsed:: true<ul> <li>\u4f8b\u5982\u6570\u636e\u96c6\u4e2d\u6709\u51e0\u53e5\u8bdd, \u6211\u4eec\u53ea\u5bf9\u67d0\u4e09\u4e2a\u8bcd\u8bed\u8fdb\u884c\u7edf\u8ba1\u5e76\u4e14\u4f5c\u4e3afeature, \u6bcf\u53e5\u8bdd\u90fd\u6709\u4e00\u4e2alabel, good or bad. \u8fd9\u4e2a\u6570\u636e\u96c6\u4e2d\u4e00\u5171\u4e94\u53e5\u8bdd, \u4e09\u4e2agood, \u4e24\u4e2abad, \u56e0\u6b64\u5148\u9a8cp(y)\u5c31\u662f3/5\u548c2/5\u5bf9\u4e8egood\u7c7b\u548cbad\u7c7b. \u6211\u4eec\u4e5f\u53ef\u4ee5\u7b97\u51fa, given\u662fgood\u7c7b, \u51fa\u73b0\u67d0\u4e2a\u8bcd\u8bed\u7684\u6982\u7387, \u8fd9\u4e2a\u53eb\u505alikelihood, \u65b9\u6cd5\u662fgood\u7c7b\u4e2d\u603b\u5171\u7684\u8bcd\u8bed\u6570\u91cf\u4f5c\u4e3a\u5206\u6bcd, \u5206\u5b50\u662f\u8fd9\u4e2a\u8bcd\u8bed\u7684\u51fa\u73b0\u6b21\u6570. \u901a\u5e38\u60c5\u51b5\u4e0b\u4f1asmoothing</li> <li></li> <li>\u56e0\u4e3a\u6211\u4eec\u5047\u8bbe\u5404\u4e2a\u8bcd\u8bed\u72ec\u7acb\u5206\u5e03, \u56e0\u6b64\u603b\u7684\u4f3c\u7136p(x|y)\u53ef\u4ee5\u7528\u8fde\u52a0log\u540e\u5404\u81ea\u7684\u503c\u6765\u8868\u73b0</li> <li></li> </ul> </li> <li>\u8fd9\u4e9b\u65b9\u6cd5\u6709\u95ee\u9898\u7684, \u56e0\u4e3a\u76f4\u63a5\u7528\u8bcd\u9891, \u5b9e\u9645\u4e0a\u5e76\u6ca1\u6709\u8003\u8651\u5230\u9650\u5b9a\u8bcd\u7684\u8bed\u4e49\u76f8\u53cd\u95ee\u9898   collapsed:: true<ul> <li>Conditional independence assumption</li> <li>Context not taken into account</li> <li>New words (not seen at training) cannot be used</li> </ul> </li> <li>Input representation   collapsed:: true<ul> <li>\u53ef\u4ee5\u4f7f\u7528bag of words\u6765\u8868\u793a\u4e00\u53e5\u8bdd, \u4f8b\u5982\u7edf\u8ba1\u4e00\u53e5\u8bdd\u4e2d\u7684\u8bcd\u8bed\u4eec\u7684\u51fa\u73b0\u9891\u7387\u6765\u4f5c\u4e3a\u4e00\u4e2avector</li> </ul> </li> <li>Discriminative vs Generative algorithms #card #ML   collapsed:: true<ul> <li>\u5224\u522b\u5f0f\u6a21\u578b\u901a\u8fc7\u751f\u6210\u5224\u522bboundary\u6765\u5224\u522b\u65b0\u7684\u6570\u636e\u7684\u7c7b\u522b. \u5224\u522b\u5f0f\u6a21\u578b\u662f\u5bf9\u6761\u4ef6\u6982\u7387\u5efa\u6a21\uff0c\u5b66\u4e60\u4e0d\u540c\u7c7b\u522b\u4e4b\u95f4\u7684\u6700\u4f18\u8fb9\u754c\uff0c\u65e0\u6cd5\u53cd\u6620\u8bad\u7ec3\u6570\u636e\u672c\u8eab\u7684\u7279\u6027\uff0c\u80fd\u529b\u6709\u9650\uff0c\u5176\u53ea\u80fd\u544a\u8bc9\u6211\u4eec\u5206\u7c7b\u7684\u7c7b\u522b. \u51b3\u7b56\u8fb9\u754c, SVM, NN, \u56de\u5f52, \u51b3\u7b56\u6811</li> <li>\u751f\u6210\u5f0f\u6a21\u578b\u901a\u8fc7\u751f\u6210\u4e00\u4e2a\u7b26\u5408\u6570\u636e\u7684\u5206\u5e03\u6765\u8fdb\u884c\u6982\u7387\u5224\u65ad, \u5728\u54ea\u4e2a\u7c7b\u522b\u4e2d\u7684\u6982\u7387\u66f4\u9ad8. \u4e00\u822c\u4f1a\u5bf9\u6bcf\u4e00\u4e2a\u7c7b\u5efa\u7acb\u4e00\u4e2a\u6a21\u578b\uff0c\u6709\u591a\u5c11\u4e2a\u7c7b\u522b\uff0c\u5c31\u5efa\u7acb\u591a\u5c11\u4e2a\u6a21\u578b\u3002\u6bd4\u5982\u8bf4\u7c7b\u522b\u6807\u7b7e\u6709\uff5b\u732b\uff0c\u72d7\uff0c\u732a\uff5d\uff0c\u90a3\u9996\u5148\u6839\u636e\u732b\u7684\u7279\u5f81\u5b66\u4e60\u51fa\u4e00\u4e2a\u732b\u7684\u6a21\u578b\uff0c\u518d\u6839\u636e\u72d7\u7684\u7279\u5f81\u5b66\u4e60\u51fa\u72d7\u7684\u6a21\u578b\uff0c\u4e4b\u540e\u5206\u522b\u8ba1\u7b97\u65b0\u6837\u672cX\u8ddf\u4e09\u4e2a\u7c7b\u522b\u7684\u8054\u5408\u6982\u7387 P\uff08y|x\uff09\uff0c\u7136\u540e\u6839\u636e\u8d1d\u53f6\u65af\u516c\u5f0f\uff1a\u5206\u522b\u8ba1\u7b97 P\uff08y|x\uff09\uff0c\u9009\u62e9\u4e09\u7c7b\u4e2d\u6700\u5927\u7684 P\uff08y|x\uff09\u4f5c\u4e3a\u6837\u672c\u7684\u5206\u7c7b\u3002\u8d1d\u53f6\u65af\u7f51\u7edc, \u6734\u7d20\u8d1d\u53f6\u65af, KNN</li> <li>\u4e24\u8005\u7684\u7279\u70b9   collapsed:: true<ul> <li>\u5224\u522b\u5f0f\u6a21\u578b\u7279\u70b9\uff1a</li> <li>\u5224\u522b\u5f0f\u6a21\u578b\u76f4\u63a5\u5b66\u4e60\u51b3\u7b56\u51fd\u6570Y=f(X)\uff0c\u6216\u8005\u6761\u4ef6\u6982\u7387P\uff08Y|X\uff09\uff0c\u4e0d\u80fd\u53cd\u6620\u8bad\u7ec3\u6570\u636e\u672c\u8eab\u7684\u7279\u6027\uff0c\u4f46\u5b83\u5bfb\u627e\u4e0d\u540c\u7c7b\u522b\u4e4b\u95f4\u7684\u6700\u4f18\u5206\u88c2\u9762\uff0c\u53cd\u6620\u7684\u662f\u5f02\u7c7b\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u76f4\u63a5\u9762\u5bf9\u9884\u6d4b\u5f80\u5f80\u5b66\u4e60\u51c6\u786e\u5ea6\u66f4\u9ad8\u3002\u5177\u4f53\u6765\u8bf4\u6709\u4ee5\u4e0b\u7279\u70b9\uff1a</li> <li>\u5bf9\u6761\u4ef6\u6982\u7387\u5efa\u6a21\uff0c\u5b66\u4e60\u4e0d\u540c\u7c7b\u522b\u4e4b\u95f4\u7684\u6700\u4f18\u8fb9\u754c\u3002\u6355\u6349\u4e0d\u540c\u7c7b\u522b\u7279\u5f81\u7684\u5dee\u5f02\u4fe1\u606f\uff0c\u4e0d\u5b66\u4e60\u672c\u8eab\u5206\u5e03\u4fe1\u606f\uff0c\u65e0\u6cd5\u53cd\u5e94\u6570\u636e\u672c\u8eab\u7279\u6027\u3002\u5b66\u4e60\u6210\u672c\u8f83\u4f4e\uff0c\u9700\u8981\u7684\u8ba1\u7b97\u8d44\u6e90\u8f83\u5c11\u3002\u9700\u8981\u7684\u6837\u672c\u6570\u53ef\u4ee5\u8f83\u5c11\uff0c\u5c11\u6837\u672c\u4e5f\u80fd\u5f88\u597d\u5b66\u4e60\u3002\u9884\u6d4b\u65f6\u62e5\u6709\u8f83\u597d\u6027\u80fd\u3002\u65e0\u6cd5\u8f6c\u6362\u6210\u751f\u6210\u5f0f\u3002\u751f\u6210\u5f0f\u6a21\u578b\u7684\u7279\u70b9\uff1a</li> <li>\u751f\u6210\u5f0f\u6a21\u578b\u5b66\u4e60\u7684\u662f\u8054\u5408\u6982\u7387\u5bc6\u5ea6\u5206\u5e03P\uff08X,Y\uff09\uff0c\u53ef\u4ee5\u4ece\u7edf\u8ba1\u7684\u89d2\u5ea6\u8868\u793a\u5206\u5e03\u7684\u60c5\u51b5\uff0c\u80fd\u591f\u53cd\u6620\u540c\u7c7b\u6570\u636e\u672c\u8eab\u7684\u76f8\u4f3c\u5ea6\uff0c\u5b83\u4e0d\u5173\u5fc3\u5230\u5e95\u5212\u5206\u4e0d\u540c\u7c7b\u7684\u8fb9\u754c\u5728\u54ea\u91cc\u3002\u751f\u6210\u5f0f\u6a21\u578b\u7684\u5b66\u4e60\u6536\u655b\u901f\u5ea6\u66f4\u5feb\uff0c\u5f53\u6837\u672c\u5bb9\u91cf\u589e\u52a0\u65f6\uff0c\u5b66\u4e60\u5230\u7684\u6a21\u578b\u53ef\u4ee5\u66f4\u5feb\u7684\u6536\u655b\u5230\u771f\u5b9e\u6a21\u578b\uff0c\u5f53\u5b58\u5728\u9690\u53d8\u91cf\u65f6\uff0c\u4f9d\u65e7\u53ef\u4ee5\u7528\u751f\u6210\u5f0f\u6a21\u578b\uff0c\u6b64\u65f6\u5224\u522b\u5f0f\u65b9\u6cd5\u5c31\u4e0d\u884c\u4e86\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6709\u4ee5\u4e0b\u7279\u70b9\uff1a</li> <li>\u5bf9\u8054\u5408\u6982\u7387\u5efa\u6a21\uff0c\u5b66\u4e60\u6240\u6709\u5206\u7c7b\u6570\u636e\u7684\u5206\u5e03\u3002\u5b66\u4e60\u5230\u7684\u6570\u636e\u672c\u8eab\u4fe1\u606f\u66f4\u591a\uff0c\u80fd\u53cd\u5e94\u6570\u636e\u672c\u8eab\u7279\u6027\u3002\u5b66\u4e60\u6210\u672c\u8f83\u9ad8\uff0c\u9700\u8981\u66f4\u591a\u7684\u8ba1\u7b97\u8d44\u6e90\u3002\u9700\u8981\u7684\u6837\u672c\u6570\u66f4\u591a\uff0c\u6837\u672c\u8f83\u5c11\u65f6\u5b66\u4e60\u6548\u679c\u8f83\u5dee\u3002\u63a8\u65ad\u65f6\u6027\u80fd\u8f83\u5dee\u3002\u4e00\u5b9a\u6761\u4ef6\u4e0b\u80fd\u8f6c\u6362\u6210\u5224\u522b\u5f0f\u3002\u603b\u4e4b\uff0c\u5224\u522b\u5f0f\u6a21\u578b\u548c\u751f\u6210\u5f0f\u6a21\u578b\u90fd\u662f\u4f7f\u540e\u9a8c\u6982\u7387\u6700\u5927\u5316\uff0c\u5224\u522b\u5f0f\u662f\u76f4\u63a5\u5bf9\u540e\u9a8c\u6982\u7387\u5efa\u6a21\uff0c\u800c\u751f\u6210\u5f0f\u6a21\u578b\u901a\u8fc7\u8d1d\u53f6\u65af\u5b9a\u7406\u8fd9\u4e00\u201c\u6865\u6881\u201d\u4f7f\u95ee\u9898\u8f6c\u5316\u4e3a\u6c42\u8054\u5408\u6982\u7387\u3002</li> </ul> </li> </ul> </li> <li>Logistic regression   collapsed:: true<ul> <li></li> <li>\u4ee50.5\u4e3a\u754c, \u9009\u62e9\u7c7b\u522b1 \u8fd8\u662f0</li> <li>\u4e5f\u53ef\u4ee5\u5bf9\u6bcf\u4e2a\u7c7b\u8fdb\u884c\u4e00\u6b21\u903b\u8f91\u56de\u5f52, \u5f97\u5230\u7684\u503c\u7528softmax</li> </ul> </li> <li>\u903b\u8f91\u56de\u5f52\u5bf9\u6bd4\u6734\u7d20\u8d1d\u53f6\u65af   collapsed:: true<ul> <li></li> </ul> </li> <li>Learnt feature representations - Neural networks   collapsed:: true<ul> <li>Inputs (very basic model), one-hot representation</li> <li>Inputs (better model), Automatically learnt dense feature representations or Pre-trained dense representations</li> <li></li> <li>Advantages: Automatically learned features, Flexibility to fit highly complex relationships in data</li> </ul> </li> <li>Document representation   collapsed:: true<ul> <li> <ol> <li>\u53ef\u4ee5\u7528\u53e5\u5b50\u91cc\u9762\u7684\u6240\u6709\u8bcd\u7684\u6bcf\u4e2adimension\u7684\u5747\u503caverage\u4f5c\u4e3a\u8fd9\u4e2adocument\u8fd9\u4e2adimension\u7684\u503c, work well, \u4f46\u4e0d\u591f\u597d</li> </ol> </li> <li>\u4e0d\u53ef\u4ee5\u56fa\u5b9adocument words\u957f\u5ea6\u6765\u5f62\u6210\u4e00\u4e2a\u8868, \u8fd9\u6837\u5b50\u4e00\u65b9\u9762\u5927\u5c0f\u56fa\u5b9a, \u53e6\u4e00\u65b9\u9762\u4f1a\u6709\u56fa\u5b9a\u7684word position\u4fe1\u606f      3.</li> </ul> </li> <li>Recurrent Neural Networks   collapsed:: true<ul> <li></li> <li></li> <li>W\u662fhidden to hidden, H\u662finput to hidden</li> <li>Vanishing gradient problem   collapsed:: true<ul> <li>The model is less able to learn from earlier inputs</li> <li>\u9065\u8fdc\u8fc7\u53bb\u7684gradient\u4f1a\u6d88\u5931\u6216\u8005\u7206\u70b8, \u56e0\u4e3atanh\u548csigmoid\u7684\u5bfc\u6570\u57280-1\u548c0-0.25\u4e4b\u95f4</li> </ul> </li> </ul> </li> <li>CNN   collapsed:: true<ul> <li>CNNs are composed of a series of convolution layers, pooling layers and fully connected layers</li> <li>Convolution to detect</li> <li>pooling to reduce dimensionality</li> <li>FC train weights of learned representation for a specific task</li> <li></li> </ul> </li> <li>RNN vs CNN   collapsed:: true<ul> <li>CNNs can perform well if the task involves key phrase recognition, CNN\u4e3b\u8981\u8bc6\u522b\u5173\u952ephrase</li> <li>RNNs perform better when you need to understand longer range dependencies, \u662f\u522b\u7684\u4e8b\u66f4\u957f\u8fdc\u7684dependency</li> </ul> </li> </ul> </li> <li>Lecture 2.2 Model [[de-biasing]]   collapsed:: true<ul> <li>preventing a model learning from shallow heuristics, \u963b\u6b62\u6a21\u578b\u5b66\u4e60\u4e00\u4e9b\u6bd4\u8f83\u6d45\u5c42\u8fc7\u4e8e\u663e\u8457\u7684\u77e5\u8bc6, \u5e0c\u671bNLP\u6a21\u578b\u53ef\u4ee5\u66f4\u52a0\u597d\u7684\u6cdb\u5316</li> <li>\u51e0\u79cd\u53ef\u80fd\u7684\u7b56\u7565   collapsed:: true<ul> <li>Augment with more data so that it \u2018balances\u2019 the bias, \u5bf9\u6570\u636e\u96c6\u589e\u5f3a, \u4f7f\u7528\u66f4\u591a\u7684\u6570\u636e</li> <li>Filter your data, \u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u8fc7\u6ee4\u64cd\u4f5c, \u51cf\u5c11bias\u5c5e\u6027</li> <li>Make your model predictions different from predictions based on the bias (robust &amp; shallow models), \u4f7f\u5f97\u6211\u4eec\u7684\u6a21\u578b\u7684\u9884\u6d4b\u4e0d\u540c\u4e8e\u90a3\u4e9b\u57fa\u4e8ebias\u7684\u9884\u6d4b, \u4e5f\u5c31\u662f\u8bf4\u6487\u5f00\u4f7f\u7528bias\u7684\u9884\u6d4b</li> <li>Prevent a classifier finding the bias in your model representations, \u963b\u6b62\u5206\u7c7b\u5668\u627e\u5230\u5b58\u5728\u4e8e\u6a21\u578b\u4e2d\u7684bias</li> </ul> </li> <li>Bias\u6307\u7684\u662f\u4ec0\u4e48?   collapsed:: true<ul> <li>\u53ef\u4ee5\u662f\u6bd4\u5982long sentence, \u53e5\u5b50\u957f\u5ea6, \u67d0\u4e9b\u8bcd\u8bed, \u5c31\u662f\u4efb\u4f55\u4f60\u4e0d\u60f3\u8ba9\u6a21\u578b\u5b66\u4e60\u7684\u4e00\u4e9b\u6d45\u5c42\u7684\u6ca1\u7528\u7684\u4e1c\u897f, \u4efb\u4f55\u5e2e\u52a9\u6211\u4eec\u6cdb\u5316\u6a21\u578b\u7684\u4e1c\u897f, \u8fd9\u4e2a\u662f\u4f9d\u8d56\u4e8e\u7ecf\u9a8c\u7684. \u8fd9\u4e2abias\u4e0d\u540c\u4e8e\u6027\u522b\u7684bias, \u6027\u522b\u8fd9\u4e2a\u65f6\u5019\u53ef\u80fd\u66f4\u9002\u5408\u76f4\u63a5\u88abhide, \u800c\u4e0d\u662fdebias</li> </ul> </li> <li>Notation: \u5373\u5c06\u7528\u5230\u7684\u4e24\u4e2a\u6807\u6ce8   collapsed:: true<ul> <li>The probabilities of each class given our bias (bi) p(c|bias), \u7ed9\u5b9abias, \u662f\u8fd9\u4e2aclass\u7684\u6982\u7387, \u6211\u4eec\u79f0\u4e4b\u4e3a Bias probability</li> <li>probabilities from our model (pi)</li> </ul> </li> <li>\u6211\u4eec\u628a\u4e0a\u9762\u51e0\u79cd\u53ef\u80fd\u7684\u7b56\u7565\u7ed3\u5408\u8d77\u6765, \u5f97\u5230\u4e0b\u9762\u7684\u8fd9\u4e2a\u7b56\u7565 [[Product of Experts]] POE</li> <li></li> <li></li> <li>\u8fd9\u4e2a\u601d\u8def\u4ece\u9ad8\u5c42\u6765\u770b, \u5c31\u662f\u5e0c\u671b\u6211\u4eec\u7684\u6a21\u578b\u53bb\u5b66\u4e60\u9664\u4e86bias\u4ee5\u5916\u7684\u4efb\u4f55\u4e1c\u897f, \u5b9e\u73b0\u4e86\u53bbbias\u7684\u76ee\u6807, \u4e5f\u5c31\u662f\u5b66\u4e60everything around the bias, \u4f1a\u5c11\u8003\u8651bias\u7684sample, feature</li> <li>\u4ece\u5e95\u5c42\u6765\u770b, \u5c31\u662f\u52a0\u5165\u4e86\u6765\u81ea\u4e8ebias\u7684\u60e9\u7f5a\u9879, \u7ea6\u675f\u6a21\u578b\u4ecebias\u4e2d\u5b66\u4e1c\u897f</li> <li>\u90a3\u4e48\u6211\u4eec\u5982\u4f55\u627e\u5230\u8fd9\u4e2abias probability bi\u5462   collapsed:: true<ul> <li>Target a specific known \u2018biased\u2019 feature</li> <li>creating a \u2018biased\u2019 model</li> </ul> </li> <li>Creating a \u2018biased\u2019 model   collapsed:: true<ul> <li> <ol> <li>Use a model not powerful enough for the task (e.g. BoW model, or TinyBERT)      \u4f7f\u7528\u5f31\u5c0f\u7684\u6a21\u578b\u6765\u9884\u6d4b, \u627e\u5230\u4e00\u4e9bsimple\u4e14\u6d45\u5c42\u7684patterns</li> </ol> </li> <li>Use incomplete information (e.g. only the hypothesis in NLI)      \u4f7f\u7528\u4e0d\u5b8c\u5168\u7684\u4fe1\u606f</li> <li>Train a classifier on a very small number of observations      \u7528\u5f88\u5c0f\u7684\u89c2\u6d4b\u96c6\u6765\u8fdb\u884c\u8bad\u7ec3</li> </ul> </li> <li>\u9664\u4e86\u4e0a\u8ff0\u7684\u76f4\u63a5\u52a0bias\u5230\u4ea4\u53c9\u71b5\u91cc\u9762\u7684\u65b9\u6cd5\u4ee5\u5916, \u8fd8\u6709\u4e00\u79cdweight the loss of examples based on performance of our biased model   collapsed:: true<ul> <li></li> <li>bi\u503c\u8d8a\u5927 \u8fd9\u4e2a1-bi \u5c31\u8d8a\u5c0f, \u628a\u8fd9\u4e2a\u4e58\u4e0aloss\u4e5f\u5c31\u8d8a\u5c0f; \u610f\u601d\u5c31\u662f\u8bf4 bi\u5982\u679c\u8d8a\u81ea\u4fe1, loss\u5c31\u8d8a\u5c0f, \u5c31\u8d8a\u4e0d\u7528\u5b66\u4e60</li> </ul> </li> <li></li> <li>\u6b63\u786e\u7684label\u662fcontradiction, p^\u662f\u91cd\u65b0softmax\u540e\u7ed3\u5408\u4e86b\u548cp\u7684\u6982\u7387\u7ed3\u679c, \u4f1a\u4f5c\u4e3aloss \u7684\u4f9d\u636e, \u53ef\u4ee5\u770b\u5230bias\u9879\u88ab\u62c9\u9ad8\u4e86, \u4e0e\u771f\u5b9elabel\u7684\u5dee\u8ddd\u53d8\u5927\u4e86, loss\u4e5f\u76f8\u5e94\u589e\u52a0\u4e86, \u6a21\u578b\u4f1a\u66f4\u52a0\u8003\u8651\u60e9\u7f5a\u8fd9\u4e2alabel\u80cc\u540e\u7684\u903b\u8f91, \u4f8b\u5982\u7b80\u5355\u7684\u770b\u4e24\u4e2a\u53e5\u5b50\u4e4b\u95f4overlapping\u7684\u8bcd\u6c47\u6570\u8fd9\u79cd\u7b80\u5355\u7c97\u66b4\u6613\u51fa\u9519\u7684\u903b\u8f91feature, \u4ece\u800c\u5b9e\u73b0\u5bf9bias\u7684\u62b5\u5236, \u771f\u6b63\u6a21\u578b\u5b66\u4e60\u4e2d\u4e5f\u5c31\u4f1a\u51cf\u5c11\u5bf9\u8be5\u7279\u5f81\u7684\u8003\u8651, \u66f4\u591a\u8003\u8651\u5230unbiased\u7684\u5185\u5bb9, \u589e\u52a0\u6839\u636e\u771f\u6b63\u6709\u7528\u7684feature\u9884\u6d4b\u5230\u5bf9\u7684label\u7684\u6982\u7387. \u5728\u771f\u6b63\u7684\u9884\u6d4b\u8fc7\u7a0b\u4e2d, \u6211\u4eec\u53ea\u4f1a\u4f7f\u7528\u4e2d\u95f4\u7684\u8fd9\u4e2ap. debiasing\u4f1a\u6709regularisation\u7684\u4f5c\u7528, \u56e0\u6b64\u7c7b\u4f3c\u4e8e\u672c\u8bad\u7ec3\u6570\u636e\u96c6\u5206\u5e03\u7684\u6d4b\u8bd5\u96c6\u7684\u7ed3\u679c\u4f1a\u7a0d\u5fae\u6bd4\u901a\u5e38\u7684\u8bad\u7ec3\u7ed3\u679c\u5dee, \u4f46\u662f\u4f1a\u5bf9\u6cdb\u5316\u7684\u6570\u636e\u96c6\u6709\u66f4\u597d\u7684\u6548\u679c</li> <li></li> </ul> </li> <li> <p>Tutorial Text classification: Sentiment analysis using [[FFNN]]   collapsed:: true</p> <ul> <li>\u600e\u4e48\u8bad\u7ec3\u4e00\u4e2aFeed Forward Neural Network \u6765\u5bf9\u53e5\u5b50\u7684sentiment\u8fdb\u884c\u5206\u7c7b</li> <li>\u53e5\u5b50\u548clabel\u957f\u8fd9\u4e2a\u6837\u5b50</li> <li>```python   train = ['i like his paper !',            'what a well-written essay !',            'i do not agree with the criticism on this paper',            'well done ! it was an enjoyable reading',            'it was very good . send me a copy please .',            'the argumentation in the paper is very weak',            'poor effort !',            'the methodology could have been more detailed',            'i am not impressed',            'could have done better .',            ]</li> </ul> <p>train_labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]   <code>- \u7b2c\u4e00\u6b65\u5148\u8981\u628a\u53e5\u5b50\u5206\u6210\u4e00\u4e2a\u4e2atoken, \u5e76\u4e14\u4fdd\u8bc1\u6bcf\u4e2a\u53e5\u5b50\u91cc\u7684token\u6570\u4e00\u81f4, \u4e5f\u5c31\u662f\u90fd\u662f\u6700\u957f\u7684\u90a3\u53e5\u8bdd\u7684length, \u91c7\u75280 padding - \u7136\u540e\u5bf9\u6bcf\u4e2a\u8bcd\u8bed\u90fd\u8fdb\u884c\u5b57\u5178\u5f52\u7eb3, \u6bcf\u4e2a\u8bcd\u8bed\u90fd\u6709\u4e00\u4e2a\u5b57\u5178\u7684index - \u7136\u540e\u5c31\u628a\u8fd9\u4e9b\u53e5\u5b50, \u7528\u8fd9\u4e9b\u8bcd\u8bed\u5728\u5b57\u5178\u91cc\u7684index\u8868\u793a\u51fa\u6765\u4e86, \u6700\u540e\u7684\u6548\u679c\u5c31\u662f10\u53e5\u8bdd, list\u91cc\u9996\u5148\u6709\u5341\u4e2a\u5143\u7d20, \u6bcf\u4e2a\u5143\u7d20\u53c8\u662f\u4e00\u4e2alist, \u8fd9\u4e9blist\u90fd\u662f\u7b49\u957f\u7684\u8bcd\u8bed\u5728\u8bcd\u5178\u4e2d\u7684index\u4eec -</code>python   tensor([[ 1,  2,  3,  4,  5,  0,  0,  0,  0,  0,  0],           [ 6,  7,  8,  9,  5,  0,  0,  0,  0,  0,  0],           [ 1, 10, 11, 12, 13, 14, 15, 16, 17,  4,  0],           [18, 19,  5, 20, 21, 22, 23, 24,  0,  0,  0],           [20, 21, 25, 26, 27, 28, 29,  7, 30, 31, 27],           [14, 32, 33, 14,  4, 34, 25, 35,  0,  0,  0],           [36, 37,  5,  0,  0,  0,  0,  0,  0,  0,  0],           [14, 38, 39, 40, 41, 42, 43,  0,  0,  0,  0],           [ 1, 44, 11, 45,  0,  0,  0,  0,  0,  0,  0],           [39, 40, 19, 46, 27,  0,  0,  0,  0,  0,  0]])   <code>- \u8981\u653e\u5230\u795e\u7ecf\u7f51\u7edc\u91cc, \u6211\u4eec\u8fd8\u9700\u8981\u7ed9\u8fd9\u4e9b\u53e5\u5b50\u8fdb\u884cembedding, embedding\u51fd\u6570\u4f1a\u7ed9\u53e5\u5b50\u4e2d\u7684\u6bcf\u4e2a\u8bcd\u8bed\u4e00\u4e2aembedding, \uff08\u51e0\u53e5\u8bdd\uff0c\u6bcf\u53e5\u8bdd\u7684\u957f\u5ea6\uff0c \u6bcf\u4e2a\u8bcd\u8bed\u7684embedding\u957f\u5ea6(batch size, max_sent_len, embedding dim) - \u7136\u540e\u8981\u628a\u53e5\u5b50\u91cc\u7684\u8fd9\u4e48\u591a\u8bcd\u8bed\u7684embedding\u6574\u5408\u6210\u4e00\u4e2a\u4f5c\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u7684\u8868\u793a, \u6211\u4eec\u7528avg\u6765\u8fdb\u884c\u8fd9\u4e2a\u64cd\u4f5c, embedding \u7684\u6bcf\u4e00\u4e2afeature\u90fd\u662f\u53e5\u4e2d\u6240\u6709\u8bcd\u8bed\u8be5feat\u7684\u5747\u503c - \u7f51\u7edc\u7ed3\u6784\u5f88\u7b80\u5355, \u53ef\u4ee5\u968f\u610f\u8bbe\u8ba1, \u51e0\u4e2afc relu \u5c31\u597d\u4e86,     - Week4 Evaluation, language modeling, N-gram, RNN, LSTM       collapsed:: true         - Lecture 2.3 Evaluation           collapsed:: true - Accuracy   collapsed:: true     - ![image.png](../assets/image_1675162139398_0.png) - F1   collapsed:: true     - ![image.png](../assets/image_1675162170636_0.png)     - macro averaging: average of each class F1 scores, \u6bcf\u4e2a\u7c7b\u7684F1\u5355\u72ec\u6c42\u51fa\u4ee5\u540e, \u76f4\u63a5\u6c42\u5747\u503c     - micro averaging: TPs, TNs, FNs, FPs are summed across each class, F1\u7684\u57fa\u672c\u5143\u7d20\u90fd\u5404\u81ea\u52a0\u8d77\u6765\u4ee5\u540e\u7b97\u4e00\u4e2aF1       collapsed:: true         - ![image.png](../assets/image_1675162434073_0.png)         - \u5e95\u4e0b\u8fd9\u5806, \u5176\u5b9e\u5c31\u662fdataset \u7684\u5927\u5c0f, \u4e5f\u5176\u5b9e\u5c31\u662faccuracy\u4e86         - Lecture 3 [[Language Modeling]]           collapsed:: true - What is language modeling? and why?   collapsed:: true     - assigning probabilities to sequences of words. \u7ed9\u8bcd\u5e8f\u5217\u5206\u914d\u53ef\u80fd\u6027, \u4f8b\u5982\u5bf9\u4e0b\u4e00\u4e2a\u8bcd\u8bed\u7684\u9884\u6d4b\u548c\u5bf9\u6316\u7a7a\u8bcd\u7684\u9884\u6d4b (next word, masked word)       Language modelling is the task of training a computer to predict the probability of a sequence of words. It involves teaching the computer to understand and generate human language by using statistical and machine learning techniques. Essentially, it\u2019s about teaching a computer to write and speak like a human.     - \u8bed\u8a00\u5efa\u6a21\u7684\u610f\u4e49\u5728\u4e8e\u6211\u4eec\u9700\u8981generating language responses, rather than choosing a specific class, \u751f\u6210\u8bed\u8a00\u53cd\u9988, \u800c\u975e\u4ec5\u4ec5\u53ea\u662f\u9009\u62e9\u4e00\u4e2a\u7279\u5b9a\u7684\u7c7b\u522b     - \u5e94\u7528\u5982\u4e0b       collapsed:: true         - \u25cf Word completion (on phones)           \u25cf Machine translation           \u25cf Summarization           \u25cf Copilot coding assistants           \u25cf Chatbots           \u25cf And more.... - N-gram models   collapsed:: true     - Counting the likelihood of the next word       collapsed:: true         - We aim to compute P(w|h) where: w is the word (or symbol) and h is the history         - $P(w_n|w_{1}^{n-1})$ \u5229\u7528w1 - wn-1 \u7684\u4fe1\u606f\u53bb\u9884\u6d4bwn (full context)         - ![image.png](../assets/image_1675163094346_0.png)     - N-gram models approximate history by just the few last words, \u8fd9\u91cc\u7684N\u5c31\u662f\u6307\u4e00\u5171\u591a\u5c11\u8bcd       collapsed:: true         - ![image.png](../assets/image_1675163205665_0.png)         - $P(w_n|w_{1}^{n-1}) \\approx P(w_n|w_{n-N+1}^{n-1})$         - MLE as relative frequencies, \u7528\u5728\u6570\u636e\u96c6\u4e2d\u51fa\u73b0\u7684\u9891\u7387\u4f5c\u4e3a\u4f3c\u7136         - \u8bed\u6599\u5e93\u8d8a\u5927, \u6548\u679c\u8d8a\u597d, \u901a\u5e38\u60c5\u51b5\u4e0bTrigram \u8db3\u591f\u4e86         - ![image.png](../assets/image_1675163481721_0.png)     - Evaluating language models: Perplexity \u56f0\u60d1\u5ea6       collapsed:: true         - Estimate joint probability of an entire word sequence by multiplying together a number of conditional probabilities \u901a\u8fc7joint probability \u6765\u8ba1\u7b97\u6574\u4e2aword sequence\u7684\u51fa\u73b0\u6982\u7387           collapsed:: true             - ![image.png](../assets/image_1675164588330_0.png)             - \u6ce8\u610f\u8fd9\u91cccondition\u4e00\u76f4\u90fd\u662f\u4ece\u4e00\u5230\u4e0a\u4e00\u4e2a         - Bi-gram language model: probabilities           collapsed:: true             - ![image.png](../assets/image_1675165013664_0.png)             - \u8fd9\u91cc\u7528\u7684\u662f\u4e00\u4e2a\u5386\u53f2\u8bb0\u5f55\u4f5c\u4e3acondition         - ![image.png](../assets/image_1675165068107_0.png)         - Issue: longer output, lower likelihood, \u957f\u7684\u8f93\u51fa\u4f1a\u53ea\u6709\u5f88\u5c11\u7684\u4f3c\u7136\u503c           Solution: perplexity: inverse probability of a text, normalized by the # of words         - ![image.png](../assets/image_1675165595396_0.png)         - \u5373prob\u7684-1/n\u6b21\u65b9, It\u2019s a measure of the surprise in a LM when seeing new text; \u8861\u91cf\u4e86\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\u5728\u770b\u5230\u65b0\u7684text\u7684\u65f6\u5019\u7684\u60ca\u559c\u7a0b\u5ea6, \u60ca\u559c\u7a0b\u5ea6\u8d8a\u4f4e\u8d8a\u597d, \u4f46\u662fPerplexity is specific to the test-set         - FAQ           collapsed:: true             - If we are finding the perplexity of a single word, what is the best possible score? 1, perplexity\u6700\u5c0f\u6700\u5c0f\u5c31\u662f1             - If our model uniformly picks words across a vocabulary of size |V|, what is the perplexity of a single word? |V|, prob\u662f1/|V|, \u505ainverse\u5c31\u662f|v|             - ![image.png](../assets/image_1675166098898_0.png)     - Cross Entropy       collapsed:: true         - ![image.png](../assets/image_1675165743814_0.png)         - \u5bf9\u4e8e\u6211\u4eec\u7684\u8bed\u8a00\u6a21\u578b\u800c\u8a00, T\u5c31\u662f\u7ed9\u5b9a\u7684training set, \u8bed\u6599\u5e93, q\u662f\u4e00\u4e32\u8bcd\u7684\u51fa\u73b0\u6982\u7387         - Perplexity\u5c31\u662f\u4ee5\u67d0\u4e2a\u503c\u4e3a\u5e95\u7684H\u5bf9\u6570           collapsed:: true             - ![image.png](../assets/image_1675165964107_0.png)               collapsed:: true                 - - GPT3 tangent   collapsed:: true     - \u6211\u4eec\u4eba\u5de5\u7ed9\u5b9a\u4e00\u4e9b\u4f8b\u5b50, \u4f1a\u5f71\u54cdgpt3\u7684\u751f\u6210     - Extrinsic vs Intrinsic evaluation       collapsed:: true         - \u25cf If the goal of your language model is to support with another task           \u25cb The best choice of language model is the one that improves downstream task performance the most (extrinsic evaluation)\u5916\u90e8\u8bc4\u4f30\u5c31\u662f\u5728\u4e0b\u6e38\u4efb\u52a1\u7684\u8868\u73b0\u60c5\u51b5           \u25cf Perplexity is less useful in this case (intrinsic evaluation) \u5185\u90e8\u8bc4\u4f30\u5c31\u662f\u5728\u5185\u90e8\u7684\u8868\u73b0\u60c5\u51b5     - Evaluating GPT-3       collapsed:: true         - ![image.png](../assets/image_1675166247007_0.png)         - ![image.png](../assets/image_1675166256493_0.png) - Sparsity in N-gram models   collapsed:: true     - \u7a00\u758f\u6027\u600e\u4e48\u89e3\u51b3\u5462, \u56e0\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u6599\u5e93\u4e2d\u4f1a\u6709\u5f88\u591a\u4e0d\u600e\u4e48\u51fa\u73b0\u7684\u8bcd\u8bed, \u6bd4\u5982\u5973\u738b\u8d70\u4e4b\u540e\u5c31\u51fa\u73b0\u4e86\u5bf9\u56fd\u738b\u7684\u79f0\u547c, \u800c\u5728\u8fc7\u53bb\u7684\u51e0\u5341\u5e74\u5185\u90fd\u6ca1\u6709\u8fd9\u4e2a, \u4f1a\u88ab\u6807\u8bc6\u4e3a&lt;UNK&gt;     - Techniques to mitigate sparsity:       collapsed:: true         - Add-1 smoothing           collapsed:: true             - ![image.png](../assets/image_1675166362362_0.png)             - ![image.png](../assets/image_1675166394788_0.png)             - \u5bf9\u4e8e\u90a3\u4e9b\u7a00\u758f\u7684\u8bcd\u8bed, \u5927\u91cf\u7684\u5176\u4ed6\u76840\u53d8\u62101\u4f1a\u5bfc\u81f4\u539f\u672c\u9ad8\u6982\u7387\u7684\u4e1c\u897f\u5927\u5927\u964d\u4f4e\u6982\u7387             - \u25cf Easy to implement               \u25cf But takes too much probability mass from more likely occurrences               \u25cf Assigns too much probability to unseen events               \u25cf Could try +k smoothing with a smaller value of k         - Back-off           collapsed:: true             - We could back-off and see how many occurrences there are of \u2018royal highness\u2019 \u51cf\u5c11\u8bcd, \u51cf\u5f31specification             - ![image.png](../assets/image_1675166514023_0.png)         - Interpolation           collapsed:: true             - ![image.png](../assets/image_1675166531466_0.png)         - Lecture 3.1 Neural language models, [[RNN]] &amp; [[LSTM]]           collapsed:: true - Neural language models   collapsed:: true     - Improvements of Neural-based language models       collapsed:: true         - Avoids n-gram sparsity issue \u907f\u514d\u4e86n-gram\u7a00\u758f\u95ee\u9898, \u90a3\u4e9b\u4e0d\u600e\u4e48\u51fa\u73b0\u7684\u4f4e\u6982\u7387\u8bcd\u8bed         - Contextual word representations i.e. embeddings #Question     - 4-gram Feed-forward LM (FFLM)       collapsed:: true         - ![image.png](../assets/image_1675689441008_0.png)         - \u5c06\u4e09\u4e2a\u4e0a\u6587\u8bcd\u8bedembedding concat\u8d77\u6765\u4f5c\u4e3acontext\u8f93\u5165\u4ee5\u540e\u6fc0\u6d3b\u5f97\u5230\u8f93\u51fa\u5c42\u8fdb\u884csoftmax\u5f97\u5230\u8bcd\u5178\u4e2d\u6240\u6709\u8bcd\u7684\u6982\u7387\u5206\u5e03, \u6765\u51b3\u5b9a\u4e0b\u4e00\u4e2a\u8bcd\u662f\u54ea\u4e00\u4e2a         - \u76f8\u5bf9\u4e8e\u666e\u901a3gram\u63d0\u5347\u5f88\u5927, \u4f46\u662f\u4e0d\u654cRNN     - RNN       collapsed:: true         - ![image.png](../assets/image_1675691349757_0.png)         - Many to one: sentiment classification         - Many to many: Machine translation         - many to many: video classification on frame level         - ![image.png](../assets/image_1675691432247_0.png)         - \u5176\u4e2d\u4f7f\u7528\u7684function\u548cparameters\u90fd\u662f\u4e00\u6837\u7684         - ![image.png](../assets/image_1675691477170_0.png)         - ![image.png](../assets/image_1675691526325_0.png)         - In theory RNN retains information from the infinite past.           \u2013 All past hidden state has influence on the future state.         - In practice RNN has little response to the early states.           \u2013 Little memory over what seen before.           \u2013 The hidden outputs blowup or shrink to zeros.           \u2013 The \u201cmemory\u201d also depends on activation functions.           \u2013 ReLU and Sigmoid do not work well. Tanh is OK but still not \u201cmemorize\u201d for too long.         - Loss and back propagation through time (BPTT)           collapsed:: true             - ![image.png](../assets/image_1675691956352_0.png)             -     - Vanilla RNNs for language modelling in classification       collapsed:: true         - ![image.png](../assets/image_1675689685681_0.png)         - \u4e0a\u56fe\u662f\u7528\u4e8e\u5206\u7c7b\u5de5\u4f5c\u7684RNN, \u6bcf\u4e2a\u65f6\u95f4\u70b9\u7ed9\u5165\u4e00\u4e2a\u8bcd\u8bed, \u5386\u53f2\u4fe1\u606f\u4f1a\u901a\u8fc7f\u4e0e\u5f53\u524d\u8f93\u5165\u7ed3\u5408\u5904\u7406\u7ed9\u5230\u4e0b\u4e00\u4e2a\u73af\u8282, \u6700\u540e\u8f93\u51fa\u5206\u7c7b         - \u7531\u4e8e\u8bcd\u8bed\u91cf\u7684\u589e\u52a0\u4f1a\u5bfc\u81f4\u6bcf\u4e00\u5c42\u7684\u6fc0\u6d3b\u51fd\u6570\u4e00\u76f4\u7d2f\u79ef\u8d77\u6765, \u5bfc\u81f4\u5bfc\u6570\u4f1a\u54110\u9760\u62e2, \u51fa\u73b0vanishing gradients problem     - Vanilla RNNs: Many-to-many       collapsed:: true         - Every input has a label: Language modelling -&gt; predicting the next word         - The LM loss is predicted from the cross-entropy losses for each predicted word:         - ![image.png](../assets/image_1675689932905_0.png)         - \u4e0b\u4e00\u4e2a\u8f93\u5165\u7528\u5f53\u524d\u8f93\u51fa\u7684\u7ed3\u679c\u6765feed into nn, \u6765\u8ba1\u7b97cross entropy\u4f5c\u4e3aloss, \u8fd9\u4e48\u505a\u53ef\u4ee5\u8ba9RNN\u6a21\u578b\u5b66\u4e60\u6839\u636e\u5386\u53f2\u4fe1\u606f\u548c\u5f53\u524d\u8f93\u5165\u6765\u83b7\u5f97\u4e0b\u4e00\u4e2a\u8bcd\u8bed\u7684\u6982\u7387, \u4ece\u800c\u4e0d\u65ad\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u8bed         - [[Teacher Forcing]]           collapsed:: true             - ![image.png](../assets/image_1675690408483_0.png)             - \u6ca1\u6709teacher forcing\u4e0b\u4e00\u4e2a\u8f93\u5165\u5c06\u4f1a\u662f\u9884\u6d4b\u7684chased, \u4f46\u662fteacher forcing\u4f1a\u5f3a\u5236\u7ed9\u5b9a\u6b63\u786e\u7684\u8f93\u5165 sat\u6765guide nn\u5b66\u4e60\u5230\u6b63\u786e\u7684\u6982\u7387\u5206\u5e03, \u52a0\u901f\u5b66\u4e60\u8fc7\u7a0b             - ![image.png](../assets/image_1675732079125_0.png)             - During training, we use teacher forcing with a *ratio*. I.e. we well teacher force               ratio% of time. The ratio can be 100% (i.e. full teacher forcing), 50%, or you can even anneal it during training.             - ![image.png](../assets/image_1675732174448_0.png)             - Teacher forcing creates a phenomenon called [[Exposure Bias]].             - Exposure Bias is when the data distributions the model is conditioned on vary               collapsed:: true               between training and inference                 - In this case, if we set teacher forcing to 100%, then the model is never                   conditioned on its own predictions                 - So during inference, when the model uses the previously generated                   words as context, it is different than what it has seen in training             - It\u2019s unclear how much of an issue exposure bias actually is. There are multiple               papers on either side of the argument.         - Weight tying, reducing the no. parameters           collapsed:: true             - \u4e3a\u4e86\u8282\u7ea6\u53c2\u6570, \u53ef\u4ee5\u7528\u76f8\u540c\u7684embedding weights, \u4e00\u5f00\u59cb\u6211\u4eec\u628aone-hot\u53d8\u6210\u4e86embedding, \u6700\u540e\u7684\u9884\u6d4b\u7684embedding\u4e5f\u7528\u8fd9\u4e2aweights \u6765\u8f93\u51fa\u8bcd\u8bed\u7684\u6982\u7387     - Bi-directional RNNs       collapsed:: true         - \u5982\u4f55\u5b9e\u73b0\u8bed\u6cd5\u7ea0\u6b63? \u5982\u4f55\u5229\u7528\u524d\u540e\u6587\u6240\u6709\u4fe1\u606f\u6765\u505a\u5206\u7c7b\u4efb\u52a1? \u53cc\u5411\u7684RNN\u53ef\u4ee5         - ![image.png](../assets/image_1675690762021_0.png)         - For classification: \u62fc\u63a5\u8d77\u6b63\u5411\u548c\u53cd\u5411\u7684\u8f93\u51fa\u6765\u5e2e\u52a9\u5206\u7c7b           collapsed:: true             - ![image.png](../assets/image_1675690789941_0.png)         - Multi-layered RNNs: \u591a\u5c42RNN, \u5f97\u5230\u66f4\u591a\u6df1\u5c42\u4fe1\u606f           collapsed:: true             - ![image.png](../assets/image_1675690862974_0.png)         - Bidirectional multi-layered RNNs: \u591a\u5c42\u52a0\u53cc\u5411           collapsed:: true             - ![image.png](../assets/image_1675690892555_0.png)     - LSTM: Long Short Term Memory       collapsed:: true         - \u5c06memory\u5206\u6210\u4e86long term memory\u548ccurrent working memory         - ![image.png](../assets/image_1675691590143_0.png)         - RNN:           collapsed:: true             - \u2022 Recurrent neurons receive past recurrent outputs and current input as inputs.               \u2022 Processed through a tanh() activation function               \u2022 Current recurrent output passed to next higher layer and next               time step.         - ![image.png](../assets/image_1675691650495_0.png)           collapsed:: true             - Constant Error Carousel               \u2022 Key of LSTM: a remembered cell state               \u2022 Ct is the linear history carried by the constant error carousel.               \u2022 Carries information through and only effected by a gate               \u2022 Addition of history (gated).         - Gate           collapsed:: true             - ![image.png](../assets/image_1675691751785_0.png)         - Forget gate           collapsed:: true             - ![image.png](../assets/image_1675691769983_0.png)             - \u2022 The first gate determines whether to carry over the history or forget it               \u2022 Called \u201cforget\u201d gate.               \u2022 Actually, determine how much history to carry over.               \u2022 The memory C and hidden state h are distinguished.         - Input Gate           collapsed:: true             - ![image.png](../assets/image_1675691787804_0.png)             - The second gate has two parts               \u2022 A tanh unit determines if there is something new or               interesting in the input.               \u2022 A gate decides if it is worth remembering.         - Memory Cell Update           collapsed:: true             - ![image.png](../assets/image_1675691804193_0.png)             - Add the output of input gate to the current memory cell               \u2022 After the forget gate.               \u2022 \u2295: Element-wise addition.               \u2022 Perform the forgetting and the state update         - Output and Output Gate           collapsed:: true             - ![image.png](../assets/image_1675691818113_0.png)             - The output of the memory cell               \u2022 Similar to input gate.               \u2022 A tanh unit over the memory to output in range [-1, 1].               \u2022 A sigmoid unit [0,1] decide the filtering.               \u2022 Note the memory is carried through without tanh.         - the \u201cPeephole\u201d Connection           collapsed:: true             - ![image.png](../assets/image_1675691862408_0.png)             - Let the memory cell directly influence the gates!         - ![image.png](../assets/image_1675691879433_0.png)         - application and \u4f18\u52bf           collapsed:: true             - \u2022 Does not suffer from Vanishing Gradient problem.               collapsed:: true                 - 1. Additive formula means we **don\u2019t have repeated multiplication** of the same matrix (we have a derivative that\u2019s more \u2018well behaved\u2019)                   2. The forget gate means that our model can learn **when to let the gradients vanish, and when to preserve them**. This gate can take different values at different time steps.             - \u2022 Very powerful, especially in deeper networks.               \u2022 Very useful when you have a lot of data.         - ![image.png](../assets/image_1675692122428_0.png)         -     - Week5 Statistical Machine Translation, Neural MT, Encoder-decoder, BiRNN, Attention, Metrics, Inference, Data Augmentation       collapsed:: true         - Lecture 4 Machine Translation - Statistical machine translation \u6839\u636e\u4f20\u7edf\u7edf\u8ba1\u5b66\u5f97\u5230\u7684\u673a\u5668\u7ffb\u8bd1   collapsed:: true     - pipeline of several sub-models       collapsed:: true         - alignment model: \u7528\u4e8e\u5bf9\u9f50\u6587\u672c is responsible for extracting the phrase pairs         - translation model p(s|t): \u7528\u4e8e\u7ffb\u8bd1\u5230\u5bf9\u5e94\u7684\u8bed\u8a00\u6587\u672c is essentially a lookup table. We won\u2019t dive into it, but you can imagine that statistics over large pairs of parallel texts can help identify parallel phrases         - language model p(t): \u7528\u4e8e\u68c0\u6d4b\u5bf9\u5e94\u8bed\u8a00\u6587\u672c\u662f\u5426\u7b26\u5408\u79d1\u5b66, contains the probability of target language phrases, can be trained on monolingual data and gives us probability of a phrase occurring in that language.     - **Parallel corpus** means we have aligned sentences: i.e. an english sentence and its corresponding french sentence     - we want to model p(t|s), target given source       collapsed:: true         - ![image.png](../assets/image_1675703577995_0.png)     - translation is some combination of language model and translate model, \u6709\u8bed\u8a00\u6a21\u578b\u8fd8\u6709\u7ffb\u8bd1\u5373\u53d8\u6362\u7684\u6a21\u578b     - language model p(t): can be trained on monolingual data and gives us probability of a phrase occurring in that language.     - translation model p(s|t): How likely is the source phrase going to occur given a candidate translation t. We will have multiple candidate translations (example on next slide) \u7ed9\u5b9a\u4e00\u4e2a\u5907\u9009\u7684\u76ee\u6807\u8bed\u8a00\u53e5\u5b50, \u6e90\u8bed\u8a00\u53e5\u5b50\u51fa\u73b0\u7684\u53ef\u80fd\u6027\u6709\u591a\u5927     - ![image.png](../assets/image_1675703720492_0.png)     - language \u548c translation\u7ec4\u5408\u7684\u539f\u56e0\u5728\u4e8e, \u5f97\u5230translation\u7684\u7ed3\u679c\u4ee5\u540e, \u653e\u5230\u8bed\u8a00\u6a21\u578b\u4e2d, \u8bc4\u4f30\u8fd9\u53e5\u8bdd\u5728\u8fd9\u4e2a\u8bed\u8a00\u4e4b\u4e2d\u79d1\u4e0d\u79d1\u5b66, \u51fa\u73b0\u7684\u6982\u7387\u9ad8\u4e0d\u9ad8, \u6765\u9a8c\u8bc1\u8fd9\u53e5\u8bdd\u8d28\u91cf\u597d\u4e0d\u597d Language model p(t) then says: for each of the candidate targets, how likely is that phrase to occur in the target language. \u4e0b\u56fe\u4e2d\u7ea2\u8272\u90e8\u5206\u5c31\u662f\u9a8c\u8bc1target language\u4e2d\u8fd9\u53e5\u8bdd\u51fa\u73b0\u7684\u6982\u7387       collapsed:: true         - ![image.png](../assets/image_1675729336588_0.png)     - downsides:       collapsed:: true         - sentence alignment Long sentences may be broken up, short sentences may be merged. There are even some languages that use writing systems without clear indication of a sentence end (for example, Thai).         - word alignment\u95ee\u9898, \u5e76\u4e0d\u4e00\u5b9a\u4e00\u4e00\u5bf9\u5e94, \u5e76\u4e14\u8fd8\u4f1a\u6709\u4e00\u4e9b\u6ca1\u6709\u5bf9\u5e94\u610f\u601d\u7684\u8bcd\u8bed         - Statistical anomalies: Real-world training sets may override translations of, say, proper nouns. \u4f8b\u5982\u8bad\u7ec3\u96c6\u91cc\u9762\u51fa\u73b0\u4e86\u592a\u591a took train to paris, \u7ffb\u8bd1\u8d77\u6765\u5c31\u7b97\u539f\u6587\u662ftrain to berlin \u4e5f\u4f1a\u53d8\u6210paris         - Idioms: Only in specific contexts do we want idioms to be translated. For           example, using in some bilingual corpus (in the domain of Parliment), \"hear\" may almost invariably be translated to \"Bravo!\" since in Parliament \"Hear, Hear!\" becomes \"Bravo!\u201d. \u6211\u4eec\u4e0d\u5e0c\u671b\u4e60\u8bed\u88ab\u7ffb\u8bd1, \u5e0c\u671b\u4fdd\u7559\u539f\u672c - Neural Machine Translation     - Encoder  Decoder       collapsed:: true         - solve sequence-to-sequence tasks         - encoder represents the source as a **latent encoding**, \u7f16\u7801\u5668\u7528\u4e8e\u5c06\u6e90\u8bed\u8a00\u7f16\u7801\u6210\u67d0\u79cd\u6f5c\u7a7a\u95f4\u7f16\u7801, \u7528\u6765\u8868\u793a\u8f93\u5165\u7684\u8bed\u8a00\u5e8f\u5217, \u7528\u4e8e\u7ed9decoder\u4f7f\u7528         - decoder generates a **target** from the latent encoding, \u53ef\u4ee5\u628adecoder\u770b\u4f5c\u662flanguage model, \u7528\u4e8e\u5c06latent encoding\u8f6c\u6362\u4e3a\u8bed\u8a00         - \u4e09\u79cd\u65b9\u5f0f\u6765\u5b9e\u73b0\u7f16\u89e3\u7801\u5668           collapsed:: true             - RNNs             - Attention             - Transformers         - \u901a\u5e38encoder\u6700\u540e\u4ea7\u751f\u7684hidden state \u8868\u793a\u6574\u4e2a\u5e8f\u5217, \u53cc\u5411\u7684\u7f51\u7edc\u53ef\u80fd\u4f1a\u6709\u4e24\u4e2ah concat\u8d77\u6765, \u8868\u793a\u6574\u4e2a\u5e8f\u5217\u7684latent encoding     - BiDirectional RNN ([[BiRNN]]) \u53cc\u5411\u5faa\u73af\u795e\u7ecf\u7f51\u7edc       collapsed:: true         - ![image.png](../assets/image_1675731217398_0.png)         - \u53cc\u5411\u7684\u7ed3\u6784\u9002\u5408\u4e8eencoding \u7ffb\u8bd1\u6e90, \u56e0\u4e3a\u4ed6\u5e76\u6ca1\u6709\u5728\u9884\u6d4b, \u800c\u662f\u5728\u7f16\u7801, \u9700\u8981\u5f97\u5230\u6574\u4e2a\u53e5\u5b50\u7684\u610f\u601d, RNN\u7684init\u90fd\u662f\u968f\u673a\u6216\u80050\u521d\u59cb\u5316\u7684\u4f5c\u4e3ahidden history, \u8fd9\u79cd\u7ed3\u6784\u53ef\u4ee5\u5c06\u5f00\u59cb\u7684\u4fe1\u606f\u4e0e\u5c06\u6765\u7684\u4fe1\u606f\u7ed3\u5408\u8d77\u6765, \u83b7\u5f97\u6574\u4e2a\u8bed\u5883\u7684\u4fe1\u606f, \u5b9e\u73b0\u65b9\u5f0f\u662f\u6bcf\u4e2a\u65f6\u95f4\u8282\u70b9\u7684\u8f93\u51fa\u90fd\u53d8\u6210\u4e86\u4e24\u4e2a\u53cd\u5411\u7684RNN\u7684h\u7684concat\u7ed3\u679c [h_i ; h\u2019_0]         - ![image.png](../assets/image_1675731674697_0.png)         - ![image.png](../assets/image_1675731700252_0.png)         - \u901a\u8fc7\u53cc\u5411\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684\u53cc\u5411\u8f93\u51fahidden state \u53ef\u4ee5\u5f97\u5230\u5bf9\u4e8e\u6e90sequence \u7684encoding, c, \u53ef\u4ee5\u662f\u5355\u5355hi, \u4e5f\u53ef\u4ee5\u662f\u4e24\u4e2a\u8f93\u51fa\u7684concat, \u4e5f\u53ef\u4ee5\u662f\u53d6\u4e00\u4e9b\u5e73\u5747; \u8fd9\u4e2ac\u4f1a\u88ab\u7528\u4f5cinput\u653e\u5230decder\u7ed3\u6784\u4e2d, \u7528\u6765\u9884\u6d4b\u751f\u6210\u76ee\u6807\u8bed\u8a00\u7684\u8bcd\u8bed\u5e8f\u5217         - Decoder\u53ef\u4ee5\u5c31\u662f\u4e00\u4e2a\u6211\u4eec\u8bad\u7ec3\u7684typical language model, \u6bd4\u5982\u4e00\u4e2aRNN, \u53ea\u4e0d\u8fc7\u5b83\u6700\u5f00\u59cb\u7684\u8f93\u5165\u7684hidden state\u5e76\u4e0d\u662f0, \u800c\u662fencoding         - \u6ce8\u610f\u56e0\u4e3a\u6211\u4eec\u8fd9\u91cc\u4f7f\u7528\u4e86concat\u7684\u65b9\u6cd5, \u539f\u672c\u7684embedding \u7684 dim\u662fd, \u73b0\u5728\u6211\u4eec\u5c31\u75282d\u4e86         - \u6211\u4eec\u4f1a\u7528\u5230 [[Teacher Forcing]] \u6765\u5728\u8bad\u7ec3\u9636\u6bb5\u51cf\u5c11\u9519\u8bef\u7d2f\u79ef, \u5e2e\u52a9\u8bad\u7ec3, \u4f46\u662finference\u7684\u8fc7\u7a0b\u662f [[Auto-regression]], \u81ea\u56de\u5f52\u7684, \u8fd9\u4e2a\u65f6\u95f4\u70b9\u7684\u9884\u6d4b\u4f1a\u6210\u4e3a\u4e0b\u4e00\u4e2a\u65f6\u95f4\u70b9\u7684\u8f93\u5165         - ![image.png](../assets/image_1675732414097_0.png)         - Problems           collapsed:: true             - All sequences are being represented by a d-dimensional vector. For longer sequence lengths, the ability to retain all the source information in this vector diminishes. \u4e0d\u7ba1\u591a\u957f\u7684sequence \u90fd\u662f\u8fd9\u6837\u5b50\u4e00\u4e2a\u957f\u5ea6\u7684embedding vector, \u592a\u957f\u7684sequence \u6839\u672c\u5c31\u6ca1\u6709source information\u4e86             - \u5bf9\u4e8edecoder \u6765\u8bf4\u5c31\u662f\u76f8\u5f53\u4e8e, \u4e00\u5f00\u59cb\u7ed9\u7684context vector c\u6ca1\u4e86 diminish\u4e86             - vanishing gradients and the lack of ability to \u2018remember\u2019 things from earlier parts of the sequence         - \u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898, \u6211\u4eec\u5c31\u8981\u63d0\u4f9b\u6240\u6709\u7684hidden state h\u4eec, \u8fd9\u6837\u5b50\u7684\u8bdd\u5c31\u4e0d\u4f1a\u53ea\u4f9d\u636e\u4e8e\u4e00\u4e2a\u6700\u7ec8\u7684\u8f93\u51fac (\u7531\u6700\u7ec8\u7684\u4e24\u4e2ah concat)\u4e86,  (\u5f15\u51fa[[attention]]\u6ce8\u610f\u529b\u673a\u5236) during decoding, the current decoding timestep could look at all the source words, and find out which source words are most important to its current stage of decoding. \u5728\u89e3\u7801\u9636\u6bb5, \u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6240\u6709\u7684\u6e90\u8bcd\u8bed, \u8fd8\u80fd\u591f\u627e\u5230\u54ea\u51e0\u4e2a\u662f\u5bf9\u4e8e\u8fd9\u4e2a\u9636\u6bb5\u6700\u4e3a\u91cd\u8981\u7684, \u90a3\u5c31\u89e3\u51b3\u4e86\u4fe1\u606f\u7f3a\u5931\u7684\u95ee\u9898, \u800c\u627e\u5230\u91cd\u8981\u7684\u8bcd\u8bed, \u6216\u8005\u8bf4weight\u8bcd\u8bed\u7684hidden state \u7684\u8fc7\u7a0b, \u5c31\u88ab\u53eb\u505aattention, \u56e0\u4e3a\u6211\u4eec\u5728pay attention to some important words than the other words         -     - [[Attention]] \u6ce8\u610f\u529b\u673a\u5236       id:: 63e1a744-5b0f-40ab-9f5e-4e53346baf10         - a dynamic weighted average           id:: 63e1a74e-6dda-46ab-a2ca-9cdcf5de628b         - In the immediate context, it allows us to dynamically look at individual tokens in           id:: 63e1a77c-f1f4-434c-9dc0-a2934287263a           the input and decide how much weighting a token should have with respect to the current timestep of decoding.         - Types of attention           id:: 63e1a798-6d0f-4b1c-9250-690953da3ffb             - additive/MLP               id:: 63e1a7ab-48b4-4455-a781-87bc715ebbc0             - Multiplicative               id:: 63e1a7b7-efb3-4ec7-a7fb-67050aacb2eb             - Self-attention               id:: 63e1a7be-1405-440e-ab5e-ddfd71719d57         - [[MLP Attention]]           id:: 63e1a7c2-05bb-4762-b60f-e7ca15370cd7             - $c_t\\ =\\ \\Sigma_{i=1}^{I}\\alpha_i h_i$               collapsed:: true                 - c_t is the context vector for the t\u2019th decoding timestep                 - We then loop over all the hidden states i, and weight it by a scalar value                   alpha                 - So if alpha is 0, then the i\u2019th hidden state is 0                 - If alpha is 1, then we retain the full information of that hidden state - We then sum together our weighted hidden states to obtain a                   contextualised representation for the t\u2019th decoding step                 - Now the question is... how do we obtain alpha?             - ![image.png](../assets/image_1675733243112_0.png)               collapsed:: true                 - 1. What we\u2019re trying to do is decode the y_t word                   2. And we have access to bidirectional encodings of each source word                   3. Think of the attention module as a black box for a second. We\u2019ll look at how it works in the next slide                   4. So, before we decode y_t, we\u2019re going to feed **all our encoder hidden states** AND the **decoder hidden state (s_t-1)** to our attention module                   5. The module will output a **context vector, c_t**.                   6. **c_t is a dynamic and contextualised representation**. It uses the decoder hidden state information to try and figure out which source words are most important when for decoding y_t                   7. We send c_t to the t\u2019th RNN step, alongside the previously generated token y_t-1.                   8. One final change that the methodology introduced (not strictly related to attention itself), is that the output projection layer now also takes in c_t and the word embedding for y_t-1 (alongside s_t) to predict the t\u2019th word.             - ![IMG_DD2AAD24FA0B-1.jpeg](../assets/IMG_DD2AAD24FA0B-1_1675733471341_0.jpeg)               collapsed:: true                 - \u56e0\u4e3a\u8fd9\u4e2a\u662fadditive attention, energy score \u662f\u7531\u52a0\u6cd5\u5f97\u5230\u7684, \u7531\u6211\u4eec\u9700\u8981attention\u7684\u4e24\u4e2as\u548ch\u4e0e\u5404\u81ealearnable weights W\u548cU\u5411\u4e58\u540e\u76f8\u52a0\u518d\u6fc0\u6d3b\u5f97\u5230\u7684                 - \u6211\u4eec\u5148\u770ba, \u5373energy \u51fd\u6570 alignment function, \u7528\u6765\u83b7\u5f97attention \u5206\u6570\u7684\u51fd\u6570, \u8fd9\u4e2a\u51fd\u6570\u8f93\u5165\u4e86\u6211\u4eec\u6b63\u5728decode\u7684state s (1 x d) \u548c\u6211\u4eecencoder\u4e2d\u7684\u6240\u6709h concat\u8d77\u6765\u7684\u4e00\u4e2a(i x 2d)\u5411\u91cf, \u8fd9\u91cci \u662f\u8bcd\u8bed\u6570, \u4e5f\u5c31\u662fsequence \u957f\u5ea6, 2d\u662f\u56e0\u4e3a\u53cc\u5411RNN\u7684\u4e24\u4e2a\u7ed3\u679c\u76f8\u63a5.                 - a\u51fd\u6570\u62ec\u53f7\u5185\u90e8, s\u8fdb\u884c\u4e86repeating\u64cd\u4f5c, \u4e3a\u4e86\u548cU\u5bf9\u9f50, \u597d\u4e00\u4e2a\u4e2aattention, \u56e0\u4e3a\u6211\u4eec\u7528\u4e86\u77e9\u9635\u52a0\u901f, \u6240\u4ee5\u8981\u8003\u8651\u5230\u8fd9\u4e2a\u5bf9\u9f50, W\u548cU\u90fd\u662f\u53ef\u5b66\u4e60\u53c2\u6570; \u62ec\u53f7\u5916\u7684v\u4f5c\u7528\u662f\u628atanh\u8f93\u51fa\u7684(i x d)\u5411\u91cf\u8f6c\u6362\u6210(i x 1), \u4e5f\u5c31\u662f\u53d8\u6210i\u4e2ascalar \u4f5c\u4e3a\u6bcf\u4e2ahidden state h_i\u7684\u6ce8\u610f\u529b\u5206\u6570,                 - \u7b97\u51fa\u6bcf\u4e2ah\u7684\u6ce8\u610f\u529b\u5206\u6570\u4ee5\u540e, softmax\u5c31\u80fd\u5f97\u5230weight                 - \u6700\u540e\u7528weight \u6765\u8fdb\u884c\u52a0\u6743\u5e73\u5747\u5c31\u5f97\u5230\u4e86\u6211\u4eec\u9700\u8981\u7684context c                 - 1. Tell them: alpha is a scalar value. h_i is 2d                   2. Alpha represents how important the i\u2019th source word is to the current decoding step.                   3. To obtain this, we need to calculate the energy scores for each word (e_i).                   4. Energy scores are calculated by using an alignment function: a.                   5. Once we have energy scores, we concatenate them together. Then we apply a softmax. The softmax is the normalized relevance of each source word with respect to the current decoding step.                   6. Then we perform the alpha*h_i multiplication: This is a keypoint of attention. It applies a \u201cmask\u201d to each of our hidden states. Low energy values tend to 0 which means that we do not need that word\u2019s information to decode the next word                   7. Think of the alignment function as a 2 layered MLP. The first layer combines our decoder hidden state with all our encoder hidden states. The second layer (v) uses this contextualised representation to predict energy scores: i.e. unnormalized \u201cimportance\u201d of each of the source words.                 -             - \u7ec6\u8282\u5b9e\u73b0\u548c\u7ef4\u5ea6\u4fe1\u606f               collapsed:: true                 - ![image.png](../assets/image_1675959759448_0.png)                 - \u5de6\u8fb9\u7684\u6a59\u8272vectors\u662fBiDRNN\u7684\u6bcf\u4e2a\u65f6\u523b(\u5373\u6bcf\u4e2a\u8f93\u5165\u8bcd\u8bed)\u7684hidden state, \u7531\u4e8e\u662f\u53cc\u5411\u7684, \u6240\u4ee5\u62fc\u63a5\u8d77\u6765\u540e\u662f2D, \u4f46\u662f\u6211\u4eec\u7684decoder\u60f3\u8981\u4e00\u4e2aD\u7684, \u6240\u4ee5\u4f1a\u901a\u8fc7\u4e00\u4e2aprojection \u6295\u5c04\u5230D, \u4f5c\u4e3aDecoder\u7684hidden \u8f93\u5165                 - ![image.png](../assets/image_1675960019882_0.png)                 - decoder rnn\u7f51\u7edc\u4e2d\u6bcf\u4e2a\u65f6\u95f4\u8282\u70b9\u7684hidden state s\u90fd\u4f1a\u4e0e\u6240\u6709\u7684encoder \u7684hidden state\u4eec\u8fdb\u884cattention, \u4e3e\u4f8b\u6765\u8bf4\u5c31\u662fs0\u548c\u6bcf\u4e2ahi\u7b97\u52a0\u6027\u6ce8\u610f\u529b\u5206\u6570, \u6839\u636e\u8fd9\u4e2a\u5206\u6570a, \u6765\u51b3\u5b9a\u6bcf\u4e2ahi\u8981\u53d6\u591a\u5c11, \u52a0\u6743\u5f97\u5230\u7684\u5c31\u662fcontext vector ct, \u6bcf\u4e2a\u6ce8\u610f\u529b\u5206\u6570\u90fd\u53ef\u4ee5\u88ab\u89e3\u91ca\u4e3a\u8fd9\u4e2a\u8bcd\u8bed\u5bf9\u4e8e\u73b0\u5728\u7684decoding step \u7684\u91cd\u8981\u6027                 - ![image.png](../assets/image_1675960257567_0.png)                 - \u7ef4\u5ea6\u4fe1\u606f\u5982\u4e0b\u56fe\u6240\u793a, \u6ce8\u610f\u8fd9\u91cc\u8ba1\u7b97\u7684\u662f\u5bf9\u4e8e\u67d0\u4e00\u4e2ah\u7684\u6ce8\u610f\u529b\u5206\u6570, \u6240\u4ee5\u540e\u9762\u90fd\u662fD x 1                 - ![image.png](../assets/image_1675960384717_0.png)                 - ![image.png](../assets/image_1675961588066_0.png)                 - ![image.png](../assets/image_1675961600210_0.png)                 - \u4f7f\u7528\u4e86attention\u7684RNN decoder \u5c31\u4f1a\u6709\u4e09\u4e2ainput, c, s \u548ci, \u5206\u522b\u662fcontext from attention, hidden state from previous output, input             -         - Lecture 4.1 Evaluation metrics of MT/NLG systems           collapsed:: true - NLG Evluation   collapsed:: true     - Human evaluation:  \u597d, \u4f46\u662f\u4e0d\u73b0\u5b9e, \u5f88\u8d35     - Automatic evaluation       collapsed:: true         - \u901a\u5e38\u57fa\u4e8e\u7edf\u8ba1\u8ba1\u6570 n-grams\u7684; \u4e5f\u5c31\u662fn\u4e2a\u8bcd\u7ec4\u6210\u7684\u5bf9, \u6709\u591a\u5c11\u6b21\u51fa\u73b0\u5728reference \u4e2d           collapsed:: true             - BLEU, Chr-F, TER, METEOR, ROUGE             - **BertScore** is non n-gram count based, model based - BLEU: reports a modified precision metric for each level of n-gram \u6ca1\u5173\u6ce8recall   collapsed:: true     - MP (Modified Precision)\u5206\u6570\u6765\u6e90\u4e8e\u6bcf\u4e2aunique n-gram\u5728references \u4e2d\u51fa\u73b0\u7684\u6700\u5927\u6b21\u6570\u7684\u603b\u548c, \u4f8b\u5982the \u5728r1\u51fa\u73b0\u4e09\u6b21, r2\u51fa\u73b0\u4e00\u6b21, \u90a3\u5c31\u53ea\u7b973\u6b21, \u7136\u540e\u5176\u4ed6\u7684n-gram\u8bcd\u8bed\u4e5f\u5f97\u8fd9\u4e48\u7b97     - ![image.png](../assets/image_1675963919749_0.png)     - ![image.png](../assets/image_1675963941678_0.png)     - \u4f8b\u5982\u4e0a\u56fe, \u6bcf\u4e2a\u8bcd\u8bed\u662f\u4e00\u4e2a1-gram, r1\u4e2d\u5df2\u7ecf\u90fd\u51fa\u73b0\u8fc7\u4e861\u6b21, r2\u4e2d\u7684\u5c31\u4e0d\u7b97\u4e86     - ![image.png](../assets/image_1675963978049_0.png)     - ![image.png](../assets/image_1675964008502_0.png)     - Definition of BLEU:       collapsed:: true         - Is a precision based metric over the product of n-gram matches         - The matches are scaled by the brevity penalty which penalises shorter           translations.         - There are a couple of interpretations about why we use a BP. They\u2019re           mostly about encouraging the Hyps to be of a similar length to a reference (see BP equation). Feel free to research more about it in your own time. An intuitive reason is for its existence is to account for the lack of recall term in the metric.     - Practically, there are some differing definitions and implementations of BLEU. When you want to report this score, it is good practise to use a standardized library (e.g. SacreBLEU) - Chr-F: Character n-gram F\u00df score   collapsed:: true     - Balances character precision and character recall - TER: Translation Error Rate   collapsed:: true     - minimum number of edits required to change a hypothesis into one of the references - ROUGE-n: Measures the F-score of n-gram split references and system outputs   collapsed:: true     - ROUGE balances both precision and recall via the F-Score.     - Though originally a translation metric, ROUGE is more common in       captioning/summarization literature than translation. Obviously it can be used for       translation though.     - ROUGE-n measures the F-score of n-gram split references and system outputs     - ![image.png](../assets/image_1675964411735_0.png) - METEOR: Unigram precision and recall with R weighted 9* higher than P; Modern way   collapsed:: true     - ![image.png](../assets/image_1675964490240_0.png) - Shortcomings for N-gram methods   collapsed:: true     - ![image.png](../assets/image_1675964576575_0.png) - BertScore: computes pairwise cosine similarity for each token in the candidate with each token in the reference sentence   collapsed:: true     - \u4e0d\u518d\u662fn-gram\u4e86     - Biggest drawback is now not the n-gram matching. Rather, the scores can vary if evaluated against different BERT models - Inference: \u4e0d\u77e5\u9053\u771f\u5b9elabel\u600e\u4e48\u529e   collapsed:: true     - Greedy decoding       collapsed:: true         - ![image.png](../assets/image_1675964842183_0.png)         - Chosen word might be best at current timestep. But as we decode the rest of the sentence, it might be worse than we thought. If we were able to see what the future candidates might be, we might be able to predict a better word for the current time step     - Beam search       collapsed:: true         - ![image.png](../assets/image_1675964858062_0.png)         - In practise, k is normally between 5-10. For the example we\u2019re about to work through, k=2         - Note that we will end up with k hypothesis at the end of decoding. Decoding finishes when we hit an EOS token         - Example run through:         - t=0: arrived and the are the 2 most likely words         - t=1: for each of these words, decode the next k. So we have [start           arrived the, start arrived witch, start the green, start the witch]         - Then we prune all but the top-k: [start the green, start the witch]         - t=2: Repeat. Now we have [start the green mage, start the green witch,           start the witch arrived, start the witch who]         - After pruning: [start the green witch, start the green who]     - Temperature sampling       collapsed:: true         - ![image.png](../assets/image_1675964874846_0.png)         - Temperature sampling lets us inject non-determinism into our decoder.           collapsed:: true             - Perhaps not ideal for translation, but can be useful for language               modelling         - Gif is the post softmax value of each of the 10 classes.         - Higher temperature leads to smoother softmax operation.         - Thus more diverse (but sometimes less coherent) outputs. - Data Augmentation   collapsed:: true     - Backtranslation     - Synonym replacement     - ![image.png](../assets/image_1675964955494_0.png) - Batching, Padding and Sequence Length   collapsed:: true     - Group similar length sentences together in a batch, \u628a\u957f\u5ea6\u76f8\u4f3c\u7684\u653e\u8fdb\u4e00\u4e2abatch\u91cc\u9762, \u8fd9\u6837\u5b50padding\u7684\u5c31\u6bd4\u8f83\u5c11, \u4e5f\u6bd4\u8f83\u7c7b\u4f3c, more efficient     - Train model on simpler/smaller sequence lengths first: \u5148\u8bad\u7ec3\u90a3\u4e9b\u957f\u5ea6\u77ed\u7684\u53e5\u5b50       collapsed:: true         - Models have been shown to better model more complicated sequences when they\u2019ve been exposed to sequences in gradually increasing complexity; \u56e0\u4e3a\u5b9e\u9a8c\u8bc1\u660e\u8981\u5148\u77ed\u540e\u957f         - Tutorial           collapsed:: true - TODO \u4e0a\u8bfe\u7684\u65f6\u5019\u8ba9\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e0bencoder decoder, \u5c31\u662f\u4e00\u4e9bweight\u7684\u5b66\u4e60, \u8981\u5339\u914d\u597dsequence\u7684\u7ef4\u5ea6, \u653e\u5165nn\u7684rnn\u91cc\u9762 - TODO \u770b\u4e86\u4e00\u773cRNN\u7684\u5b9e\u73b0\u89c6\u9891:   collapsed:: true     - \u8f93\u5165\u7ef4\u5ea6\u4f1a\u662f\u4ee5\u65f6\u95f4\u4e3a\u5bfc\u5411 (T, N, D) \u4e5f\u5c31\u662f\u5904\u7406\u7684\u65f6\u5019\u5c31\u548cRNN\u56fe\u91cc\u4e00\u6837, \u662f\u4e00\u4e2a\u4e2a\u65f6\u95f4\u8282\u70b9\u987a\u5e8f\u5904\u7406\u7684, \u5bf9\u6bcf\u4e2a\u65f6\u95f4\u8282\u70b9T, \u53d6N\u4e2asample, \u5bf9D\u8fdb\u884c\u8fd0\u7b97\u5f97\u5230N\u4e2ah, \u653e\u5230\u4e0b\u4e00\u4e2a\u65f6\u95f4\u70b9\u4f5c\u4e3a\u8f93\u5165\u4e4b\u4e00, \u7ee7\u7eed\u8fd0\u7b97     - \u5e93\u91cc\u7684rnn\u8fd8\u4f1a\u7ed9\u5230\u51e0\u5c42\u7684\u53c2\u6570, \u6240\u4ee5\u4f1a\u6709\u4e00\u4e2a\u51e0\u5c42\u5728\u7b2c\u4e00\u7ef4\u5ea6 - encoder RNN \u7684\u4e00\u4e9b\u4ee3\u7801\u7ec6\u8282   collapsed:: true     -</code>python       # Encoder, embedding \u628aone-hot\u7f16\u7801\u6210\u6211\u4eec\u8981\u7684embedding dim       self.embedding = nn.Embedding(src_vocab_size, embedding_dim)       # \u7528\u4e86\u81ea\u5e26\u7684rnn, \u7ed9\u5b9a\u4e86\u8bcd\u8bed\u7684embedding dim\u548chidden state\u7684dim       self.forwards_rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)       self.backwards_rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)       self.projection = nn.Linear(hidden_dim * 2, hidden_dim)</p> <pre><code>  # shape(inputs) = [B, I] (e.g. B=3, I=6) 3 sentences of length 6\n\n  # &gt; embed the inputs\n  embedded = self.embedding(inputs)\n  # shape(embedded) = [B, I, E] (e.g. B=3, I=6, E=5) 3 sentences of length 6, each word represented by a 5-dimensional vector\n\n  # &gt; create a tensor of zeros to initialize the hidden state of the RNN (N=1, B, D), where N is the number of layers\n  zeros = torch.zeros(1, inputs.shape[0], self.forwards_rnn.hidden_size)\n\n  # &gt; run the forwards RNN, \u7ed9\u5982embedded \u8bcd\u8bed\u5e8f\u5217\u548c\u521d\u59cb\u7684hidden state\n  _, final_forwards = self.forwards_rnn(embedded, zeros)\n  # shape(final_forwards) = [1, B, D], 1 is the number of layers\n  ```\n- Week6 Transformers\n  collapsed:: true\n    - Transformers Architecture\n</code></pre> <ul> <li></li> <li> </li> <li>Encoder   collapsed:: true<ul> <li></li> </ul> </li> <li>Decoder   collapsed:: true<ul> <li></li> </ul> </li> <li>Encoder-Decoder   collapsed:: true<ul> <li></li> </ul> </li> <li>Vision Transformers   collapsed:: true<ul> <li></li> </ul> </li> <li>Multimodal Transformers   collapsed:: true<ul> <li></li> </ul> </li> <li>         - Encoder           collapsed:: true</li> <li></li> <li>Each encoder layer consists of 2 sublayers. The first sublayer contains a multi-   head self-attention module, and the second contains a position-wise feedforward   network.</li> <li>the output of the \u201cResidual &amp; Norm\u201d after the Position-wise   feedforward will form the input to the Multi-head self-attention in the next encoder layer</li> <li>   collapsed:: true<ul> <li>The input will be an encoding of each of our words. So here, we have 3 words represented with 4 dimensions. More generally, you would have S words in the input sequence, each represented with D dimensionality.</li> <li>The MHA module processes the input and outputs another set of S x D encodings.</li> <li>These encodings get sent to the Residual &amp; Norm, which outputs another set of S x D encodings.<ul> <li>Self-attention (scaled dot product attention)   collapsed:: true</li> </ul> </li> </ul> </li> <li></li> <li>\u6bcf\u4e2a\u8bcd\u90fd\u548c\u5176\u4ed6\u8bcdattention, \u627e\u5230\u5bf9\u5e94attention\u5206\u6570, \u6839\u636e\u8fd9\u4e2a\u5206\u6570\u6765weight\u6bcf\u4e2a\u8bcd\u8bed\u5bf9\u5e94\u7684V\u7684\u503c, \u6c42\u52a0\u6743\u5e73\u5747\u5f97\u5230\u8fd9\u4e2a\u8bcd\u6700\u7ec8\u7684value</li> <li>Query: \u8868\u793a\u67e5\u8be2, self-attention\u4e2d\u8868\u793a\u8981\u67e5\u8be2\u4e0e\u6211\u76f8\u5173\u7684\u4e1c\u897f</li> <li>Key: \u8868\u793a\u952e\u503c, \u4e0equery\u5339\u914d\u67e5\u770b\u4e24\u8005\u7684\u76f8\u4f3c\u5ea6, \u6307\u5411\u7684\u662fvalue</li> <li>Value: \u8868\u793a\u503c, \u53ef\u4ee5\u770b\u4f5c\u662f\u4ee3\u8868\u8fd9\u4e2a\u4e1c\u897f\u7684\u6f5c\u5728feature \u8868\u8fbe, \u7528\u4e8e\u8868\u793a\u8fd9\u4e2a\u4e1c\u897f</li> <li></li> <li></li> <li>\u4e00\u4e2aself-attention head\u5c31\u7c7b\u4f3c\u4e8e\u8fd9\u4e48\u4e09\u4e2aW, \u7528\u6765\u628a\u8bcd\u8bed\u7684embedding\u53d8\u6362\u6210QKV\u7684\u8868\u793a, \u8fd9\u4e2a\u53d8\u6362\u662f\u5b66\u4e60\u6765\u7684</li> <li></li> <li></li> <li></li> <li>\u4e0a\u56fe\u5c31\u6f14\u793a\u4e86\u4e00\u4e2a\u8bcd\u505aself-attention \u7684\u8fc7\u7a0b, sigma\u662fsoftmax\u5b8c\u4e86\u4ee5\u540e\u7684\u6ce8\u610f\u529b\u6743\u91cd, \u8fd9\u4e2a\u6743\u91cd\u4e58\u4e0a\u6bcf\u4e2a\u8bcd\u5bf9\u5e94\u7684v, \u4f1a\u5f97\u5230S \u8bcd\u8bed\u4e2a\u6570\u4e2aweighted v, \u52a0\u8d77\u6765\u5c31\u662f\u4ed6\u7684\u65b0v, \u5bf9\u5e94\u4e8e\u505aself-attention\u7684\u8fd9\u4e2a\u8bcd\u8bed\u7167\u987e\u4e86\u4e0a\u4e0b\u6587\u610f\u4e49\u7684\u65b0\u5411\u91cf\u8868\u793avalue</li> <li></li> <li>\u8fd9\u91cc\u7684d_h\u662fKQV\u7684hidden dimension, \u8fd9\u91cc\u5bf9\u4e58\u6cd5\u6ce8\u610f\u529b\u5206\u6570\u9664\u4ee5\u4e86\u4e00\u4e2a\u6839\u53f7\u4e0b\u7684d_h\u7684\u76ee\u7684\u662f\u8ba9softmax\u5f97\u5230\u66f4\u5e73\u6ed1\u7684weight, \u56e0\u4e3a\u5982\u679c\u503c\u592a\u5927\u7684\u8bdd, \u7ecf\u8fc7exp\u4f1a\u5dee\u522b\u5f88\u5927, \u8fd0\u7b97\u4e0b\u6765norm\u5230\u6982\u7387\u5c31\u4f1a\u88ab\u90a3\u4e2a\u7a0d\u5fae\u5927\u4e00\u70b9\u7684\u503cdominate, \u4ece\u800c\u53d8\u6210\u4e86\u7c7b\u4f3c\u4e8eone-hot\u7684\u8868\u793a, \u800c\u7f29\u653e\u7684\u8bdd\u53ef\u4ee5\u8ba9\u4ed6\u4eec\u7684\u5206\u5e03\u66f4\u5e73\u6ed1, \u4f8b\u5982(0.2, 0.3, 0.5) \u800c\u975e(0.01, 0.01, 0.98); \u6240\u4ee5\u8981\u770b\u60c5\u51b5\u9700\u8981\u600e\u4e48\u6837\u7684\u5f62\u5f0f</li> <li>The intuition is that we divide through by sqrt(dq)\u200b in order to control how large the dot product between queries (Q) and keys (K) can get.</li> <li>Why do we do this? The idea is when we come to softmax (or apply any non linear function) having a down-scaled version of our attention weights allows the distribution to become smoother and less \"peaky\". This is useful as it means distribution is spread more smoothly across the words.</li> <li>But why the square root? The square root is just a popular choice in the literature as it has been shown to work quite well. That's not to say other factors wouldn't work too but taking the square root is just the most popular choice.</li> <li></li> <li>\u6700\u540e\u7684\u89e3\u96c7\u4e5f\u662fS x d_h</li> <li>sigma\u662frow-wise\u7684softmax, \u5bf9\u6bcf\u4e00\u884c\u8fdb\u884c         - Multi-head attention           collapsed:: true</li> <li>Why? Intuitively multiple attention heads allows for attending to parts of the sequence differently (e.g. some heads are responsible for longer-term dependencies, others for shorter-term dependencies)</li> <li></li> <li></li> <li> </li> <li></li> <li>The purpose of gamma and beta is to allow the neural network to learn the optimal scale and shift for each feature after normalization.</li> <li>By applying these learned parameters to the normalized feature vectors, the neural network can adjust the range and mean of each feature to better fit the task at hand.</li> <li>         - Residual           collapsed:: true</li> <li>Help mitigate the vanishing gradient problem (tiny weight changes)</li> <li></li> <li>By adding the previous layer's output directly to the current layer's output, a residual connection allows the current layer to focus on learning the difference between the two outputs, rather than learning an entirely new transformation.         - Position Wise Feedforward Network           collapsed:: true</li> <li>Is an MLP</li> <li>Position-wise \u7684\u610f\u601d\u662f\u540c\u6837\u7684weight\u4f1a\u7528\u4e8e\u6bcf\u4e00\u4e2a\u53e5\u5b50\u4e2d\u7684\u8bcd\u8bed, \u4e0d\u4f1a\u7528\u4e0d\u4e00\u6837\u7684</li> <li></li> <li>\u7528\u6765\u589e\u52a0\u4e00\u4e9bnon-linearity         - Positional Encodings</li> <li>Transformers are position invariant by default, \u5bf9\u4e8e\u4e0d\u540c\u8bcd\u8bed\u7684\u987a\u5e8f\u6ca1\u6709\u611f\u77e5, not inherently sequential, \u9700\u8981positional encoding \u6765inject position information into embeddings</li> <li>\u7406\u89e3\u4e0a\u5c31\u662f, a, b, c, d, e\u505aattention\u7684\u65f6\u5019, \u4e0d\u7ba1\u4f4d\u7f6e\u600e\u4e48\u53d8, \u600e\u4e48\u6392\u5217, attention\u5206\u6570\u90fd\u662f\u5206\u522b\u7b97\u5f97, \u6700\u540e\u7684\u7ed3\u679c\u90fd\u662fweighted avg, \u5e76\u6ca1\u6709\u4f4d\u7f6e\u4e0a\u7684\u5f71\u54cd\u5b58\u5728</li> <li>\u6211\u4eec\u7684\u505a\u6cd5\u5c31\u662f\u5728\u8bcd\u8bed\u7684embedding\u57fa\u7840\u4e0a\u52a0\u4e0a\u5bf9\u4e8e\u5b83\u4f4d\u7f6e\u4fe1\u606f\u7684encoding</li> <li></li> <li>Sinusoids<ul> <li></li> <li>PE is a function of 2 inputs: position of a word, and the model dimensionality</li> <li>Notice what is happening with these arguments. The 2i and 2i+1 imply that we\u2019re   collapsed:: true   going to be looping over the indexes in range of d.<ul> <li>E.g. Consider we had a vector of 512 dimensions. We would be looping   over it. At an even index (e.g. 0), we\u2019d apply the sin variant of PE. At an   odd index (e.g. 1), we would apply the cosine variant.</li> </ul> </li> <li>Now observe that the only difference between the functions is whether we use a   sin or a cosine</li> <li>Apart from that, the functions divide the position argument by 10000^(2i/d)   collapsed:: true<ul> <li>Note that when we\u2019re early on in looping over d, i will be small. The resulting exponent would be therefore be small. This makes the denominator small. Thus the overall value of pos/10000^(2i/d) would be larger than when we\u2019re later in our loop over d</li> <li>You can play around with the implications of how this affects the output sin and cosine in your own time (just thought it was worth mentioning)</li> </ul> </li> <li>   collapsed:: true<ul> <li>In implementation, we have a PE matrix in [maxT, d]</li> <li>maxT is the maximum sequence length we ever want to support (e.g. 5000)</li> <li>d is our \u2018model dimensionality\u2019</li> </ul> </li> <li>   collapsed:: true<ul> <li>Consider position 0 (i.e. the first word in the sequence)   collapsed:: true<ul> <li>Positional encoding function means no manipulation is done to the   vector</li> </ul> </li> <li>Consider position 1   collapsed:: true<ul> <li>Positional encoding function shows that indexes up to 10 have values close to 1 added to them</li> </ul> </li> <li>Consider position 22   collapsed:: true<ul> <li>Positional encoding function shows that the first few indexes are affected   heavily by positional encodings. Some indexes have values close to -1 added to them, followed shortly by a value +1 added to them</li> </ul> </li> </ul> </li> <li></li> <li>i\u8d8a\u5927\u7684\u65f6\u5019, 2i/d\u8d8a\u5927, \u5bf9\u5e94\u4e0a\u9762\u66f2\u7ebf\u7684x\u4e5f\u6108\u5927, \u4f1a\u8ba9y\u503c\u8d8a\u63a5\u8fd1\u4e8e0, positional encoding\u5f0f\u5b50\u5c31\u53d8\u6210\u4e86sin, cos(0), \u4e5f\u5c31\u662f\u56fe\u4e2d\u7684\u767d\u7ebf\u548c\u9ed1\u7ebf, \u5bf9\u5e941 \u548c0.</li> <li>\u800c\u5728i\u5c0f\u7684\u65f6\u5019, y\u503c\u662f\u5e73\u6ed1\u4e0b\u964d\u7684, \u5bf9\u5e94cos sin(ax)\u4e2d\u7684a\u662f\u8d8a\u6765\u8d8a\u5c0f\u7684 (0-&gt;1), sin\u548ccos\u7684\u9891\u7387\u662f\u8d8a\u53d8\u8d8a\u5927\u7684</li> </ul> </li> <li>Learned (in BERT)         - Test time (inference)           collapsed:: true</li> <li></li> <li>we perform auto-regressive generation</li> <li> <ol> <li>Our source sentence gets encoded via our encoder</li> </ol> </li> <li> <ol> <li>We feed an SOS token to our decoder. It then predicts the first word (i.e. I)</li> </ol> </li> <li> <ol> <li>We append the prediction to our SOS token, and then use this to predict the   next word (i.e. am)</li> </ol> </li> <li> <ol> <li>And so forth. At some point our decoder will predict an EOS token (not shown   here), and we\u2019ll know that this is the end of the generation.</li> </ol> </li> <li>\u6211\u4eec\u8fdb\u884c\u7684\u662f\u4e00\u4e2aiterative \u7684 prediction \u8fc7\u7a0b, \u6bcf\u6b21\u7684\u9884\u6d4b, \u4f1a\u4f5c\u4e3a\u4e0b\u4e00\u4e2adecoder\u7684\u8f93\u5165\u4e4b\u4e00, \u5e2e\u52a9\u4ed6\u7ee7\u7eed\u9884\u6d4b, \u76f4\u5230EOS \u6216\u8005\u957f\u5ea6\u8fbe\u6807\u4e86         - Train time           collapsed:: true</li> <li></li> <li>However, during training, we won\u2019t get the model to generate auto-regressively.</li> <li>We\u2019re actually going to to feed the whole target sequence as input to the   decoder and decode the sentence in one go \u4f7f\u7528\u5b8c\u6574\u7684\u53e5\u5b50, \u4e00\u6b21\u6027\u628a\u6240\u6709\u4e1c\u897f\u5168\u7ed9\u9884\u6d4b\u5b8c</li> <li>This means we can run the decoder stack once. \u53ea\u9700\u8981\u8fd0\u884c\u4e00\u6b21\u5566</li> <li>As opposed to running it for as many tokens as we have in the target   sequence (which is what we do when performing inference)</li> <li>If we feed in the whole sequence to the decoder, we need a way to tell the decoder not to look at future tokens when computing attention for one of the tokens. \u4f46\u662f\u6211\u4eec\u7ed9\u8fdb\u53bb\u7684\u6bcf\u4e00\u4e2a\u8bcd, \u9700\u8981\u53ea\u77e5\u9053\u5b83\u81ea\u5df1\u548c\u524d\u9762\u7684\u4e1c\u897f, \u867d\u7136\u7ed9\u7684\u662f\u5b8c\u6574\u7684\u5e8f\u5217\u53ef\u4ee5\u4e00\u6b21\u6027\u7b97\u5b8c(\u56e0\u4e3ateacher enforcing, \u6bcf\u4e00\u4e2adecoder block\u5185\u90e8\u76f4\u63a5\u53ef\u4ee5\u5e76\u884c, \u56e0\u4e3a\u6211\u4eec\u505a\u7684\u662fattention, \u5e76\u6ca1\u6709\u5185\u90e8\u65f6\u5e8f\u5173\u7cfb), \u4f46\u662f\u5bf9\u6bcf\u4e2a\u8bcd\u9884\u6d4b\u8ba1\u7b97loss\u7684\u65f6\u5019\u8fd8\u662f\u9700\u8981mask\u6389\u5f53\u524d\u8bcd\u8bed\u4e4b\u540e\u7684\u6240\u6709\u8bcd, \u56e0\u4e3a\u8981\u4eff\u7167\u6d4b\u8bd5\u65f6\u5e76\u4e0d\u77e5\u9053\u540e\u7eed\u4e1c\u897f\u7684\u6a21\u5f0f   collapsed:: true<ul> <li>E.g. when trying to compute the attention-based representation for \u201cam\u201d, we should not be allowed to look at \u201ca\u201d or \u201cstudent\u201d, since these tokens are in the future</li> <li>We enforce this unidirectionality with masking<ul> <li>Decoder   collapsed:: true</li> </ul> </li> </ul> </li> <li>Difference between Encoder and decoder   collapsed:: true<ul> <li>different in train and test time   collapsed:: true<ul> <li>masked multi-head self-attention</li> <li>100% teacher enforcing</li> <li>auto-regression during test (until a max length, or EOS)</li> </ul> </li> <li>cross attention</li> </ul> </li> <li>Architecture   collapsed:: true<ul> <li></li> <li></li> </ul> </li> <li>Masked Multi-head Self-attention   collapsed:: true<ul> <li>\u7531\u4e8eattention\u662f\u5bf9\u6574\u4e2a\u5e8f\u5217\u800c\u8a00\u7684, \u56e0\u6b64\u53ef\u4ee5\u88ab\u770b\u4f5c\u662fbi-directional\u7684; recall\u4e4b\u524d\u7528RNN\u505a\u7684encoder-decoder, decoder\u90e8\u5206\u662f\u7528\u7684\u5355\u5411\u7684RNN, \u6211\u4eec\u5728\u8fd9\u91cc\u89e3\u7801\u5668\u4e5f\u5e94\u5f53\u8ba9\u6bcf\u4e2a\u65f6\u95f4\u8282\u70b9\u7684\u8bcd\u8bed\u53ea\u80fd\u770b\u5230\u73b0\u5728\u548c\u5386\u53f2, \u800c\u4e0d\u80fd\u5077\u770b\u5230\u672a\u6765</li> <li>Target sequence\u5728training\u9636\u6bb5\u662f\u77e5\u9053\u7684, \u957f\u5ea6\u4e3aT, \u90a3\u4e48\u6211\u4e48\u6240\u4f7f\u7528\u7684mask\u4e5f\u5e94\u5f53\u662fT\u957f\u5ea6\u7684, \u5bf9\u4e8e\u6bcf\u4e2a\u8bcdT, \u770b\u5411\u6574\u4e2asequence T, \u56e0\u6b64\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u4e5f\u662fT. \u5373T x T</li> <li></li> <li>upper triangular \u8bbe\u4e3a-inf, indicating future tokens not be accessed</li> </ul> </li> <li>Cross Attention   collapsed:: true<ul> <li>\u5c31\u50cf\u4e4b\u524d\u7684RNN based \u7684ende\u4e00\u6837, \u9700\u8981\u6839\u636eencoder\u7684\u4fe1\u606f, \u6765\u751f\u6210\u540e\u7eed\u7684\u4fe1\u606f. \u6bcfdecode \u4e00\u4e2atoken, \u6211\u4eec\u90fd\u5f97\u8981\u77e5\u9053\u6211\u4eec\u8981\u53bb\u770b\u54ea\u51e0\u4e2aencoded words, \u8fd9\u4e2a\u5c31\u9700\u8981\u7528cross attention\u6765\u5b9e\u73b0. Q\u53d6\u7684\u662fdecoder\u91cc\u9762\u7684Q, \u4f46\u662fK \u548cV \u4f7f\u7528\u7684\u662fencoder\u6700\u540e\u4e00\u5c42\u8f93\u51fa\u7684K V. As we decode a token, we look at all encoded tokens to find out which are more important for the current decoding step</li> <li>Last Encoder layer\u4e2d\u7684K V \u4f1a\u88ab\u6240\u6709decoder layers\u4f7f\u7528</li> <li> </li> </ul> </li> <li></li> <li>Weight tying: Embedding matrix and output matrix are shared   collapsed:: true<ul> <li>word \u5230embedding\u7684matrix\u548cmodel dim\u5230output word\u7684matrix \u662fshared</li> </ul> </li> <li>Learning rate annealing         - Questions           collapsed:: true</li> <li>What are the differences between training and testing in a transformer</li> <li>How does masked self-attention work?</li> <li>What are teacher forcing ratio do we use in Transformers?</li> <li>What is cross attention doing?</li> <li>What should the shape of the cross attention matrix be?</li> <li>What does a transformer class do every training loop?   collapsed:: true<ul> <li>Create a source and target mask</li> <li>run the encoder</li> <li>run the decoder</li> <li>output logits for token prediction</li> <li>Week7 Tokenise, contextual word repre, pertaining models, BERT   collapsed:: true<ul> <li>Module 6.1: Pre-training models</li> </ul> </li> </ul> </li> <li>Byte Pair Encoding: Method of tokenisation<ul> <li>Intuition:   collapsed:: true<ul> <li>\u5982\u679c\u6211\u4eec\u53ea\u5904\u7406\u5b8c\u6574\u7684\u5e38\u7528\u8bcd, \u6211\u4eec\u4f1a\u96be\u4ee5\u5904\u7406\u90a3\u4e9b\u8bcd\u8bed\u7684\u53d8\u5f62, \u4f8b\u5982\u524d\u7f00\u540e\u7f00, \u5408\u6210\u8bcd, \u4ee5\u53ca\u4e00\u4e9b\u9519\u8bef\u62fc\u5199\u95ee\u9898</li> </ul> </li> <li>Solution: Subword units   collapsed:: true<ul> <li>break long and complex words down into smaller parts, so that each part occurs frequently enough in the training data to learn a good embedding for it.</li> <li>\u6bcf\u4e2a\u8bcd\u8bed\u90fd\u7531\u51e0\u4e2a\u90e8\u5206\u7ec4\u6210, \u6bd4\u5982\u4e00\u4e9b\u5e38\u89c1\u7684\u8bcd\u6839\u8bcd\u7f00\u53d8\u5f62\u7b49\u7b49, \u628a\u4ed6\u4eec\u5206\u5f00\u4ee5\u540e, \u53ef\u4ee5\u8ba9\u8bed\u8a00\u6a21\u578b\u7406\u89e3\u4e0d\u540c\u90e8\u5206\u7684\u542b\u4e49</li> <li></li> </ul> </li> <li>Method: Byte Pair Encoding   collapsed:: true<ul> <li>BPE is successfully used by GPT, GPT-2, GPT-3, RoBERTa, BART, and DeBERTa.   collapsed:: true<ul> <li>Byte-Pair Encoding tokenization - Hugging Face Course</li> </ul> </li> <li>\u6211\u4eec\u5f53\u7136\u4e0d\u4f1a\u624b\u52a8\u505a\u8fd9\u4ef6\u4e8b\u60c5, \u7528BPE\u8fd9\u4e2a\u7b97\u6cd5\u6765\u8fdb\u884c\u8bcd\u8bedsubpart \u5206\u89e3</li> <li>Training<ul> <li></li> <li></li> <li></li> <li></li> </ul> </li> <li>\u4e0a\u8ff0\u4f8b\u5b50\u4e2d\u7528\u5230\u4e86underscore _, \u7528\u5904\u662f   collapsed:: true<ul> <li> <ol> <li>It distinguishes the suffixes.</li> </ol> </li> <li>It tells us where to insert spaces when putting words back together.</li> </ul> </li> <li>\u4e0a\u9762\u662ftraining algorithm, \u5982\u679c\u9047\u5230\u65b0\u7684\u8bcd\u8bed, \u5c31\u53ef\u4ee5\u6839\u636e\u8bad\u7ec3\u5f97\u5230\u7684\u7ec4\u5408\u65b9\u6cd5, \u6309merge dict\u7684\u987a\u5e8f, \u6765\u5bf9\u5b57\u6bcd\u4eec\u8fdb\u884c\u7ec4\u5408</li> <li>Inference   collapsed:: true<ul> <li></li> <li></li> <li></li> <li>\u6700\u5dee\u7684\u60c5\u51b5\u5c31\u662f\u628a\u4e00\u4e2a\u8bcd\u5206\u89e3\u6210\u5355\u4e2a\u7684characters</li> </ul> </li> <li>dealing with Unicode characters, we may encounter characters that we haven\u2019t seen during training. Unicode\u65f6\u5019\u4f1a\u6709\u6ca1\u6709\u9047\u89c1\u8fc7\u7684, \u6240\u4ee5\u6211\u4eec\u628a\u5b57\u6bcdcharacter\u6269\u5c55\u5230\u4e86\u5b57\u8282byte based, vocabulary\u5c31\u53ef\u4ee5\u67092^8 256\u5927\u5c0f, \u4e5f\u5c31\u53ef\u4ee5\u51fa\u73b0\u5176\u4ed6\u672a\u89c1\u8fc7\u7684characters\u4e86</li> </ul> </li> <li>Method: Wordpieces   collapsed:: true<ul> <li>Used by BERT and other models based on BERT (e.g. DistilBERT).   collapsed:: true<ul> <li>WordPiece tokenization - Hugging Face Course</li> </ul> </li> <li>using corpus statistics to decide how to split words into subwords.\u4f7f\u7528\u7684\u4e5f\u662f\u8bed\u6599\u5e93\u6570\u636e\u6765\u51b3\u5b9a\u5982\u4f55\u7ec6\u5206\u8bcd\u8bed</li> <li></li> </ul> </li> </ul> </li> <li>Contextual word representations<ul> <li>\u4e00\u4e2a\u8bcd\u8bed\u53ef\u80fd\u6709\u591a\u4e2a\u610f\u601d, \u5728\u4e0d\u540c\u7684\u8bed\u5883\u6709\u4e0d\u540c\u7684\u8bed\u5883\u610f\u4e49, \u5982\u4f55\u6839\u636e\u8bed\u5883\u5b66\u4e60\u5230\u6bcf\u4e2a\u8bcd\u8bed\u5408\u9002\u7684embedding?</li> <li>RNN: We can train a recurrent neural network to predict the next word in the sequence, based on the previous words in the context.   collapsed:: true<ul> <li>Internally it will learn to encode any given context into a vector.</li> <li>RNN \u5e8f\u5217\u5316\u7684\u4e00\u4e2a\u4e2a\u5904\u7406\u8f93\u5165\u7684\u8bcd\u8bed, \u540e\u9762\u7684\u8bcd\u8bed\u53ef\u4ee5\u770b\u5230\u524d\u9762\u7684\u4e1c\u897f</li> </ul> </li> <li>ELMo: Embeddings from Language Models<ul> <li></li> <li>\u4ece\u524d\u5230\u540e\u548c\u4ece\u540e\u5230\u524d\u7684\u4fe1\u606f get</li> <li>Contextual word embeddings<ul> <li>When we need a vector for a word, combine the representations from both directions. \u7528\u53cc\u5411\u7684context \u4fe1\u606f\u4f5c\u4e3a\u8fd9\u4e2a\u8bcd\u8bed\u7684vector embedding</li> <li></li> <li>\u4f8b\u5982\u4e0a\u9762\u4e24\u4e2a\u65b9\u5411\u7684RNN, \u6700\u540e\u5f97\u5230\u7684\u5c31\u662fbrown\u7684\u6574\u53e5\u8bdd\u8003\u8651\u8fdb\u53bb\u7684embedding</li> <li></li> </ul> </li> <li>ELMo could be integrated into almost all neural NLP tasks with a simple concatenation to the embedding layer.<ul> <li>ELMo\u51e0\u4e4e\u53ef\u4ee5\u7528\u5728\u4efb\u4f55NLP\u4efb\u52a1\u4e2d\u7528\u6765\u52a0\u5f3aword embedding\u7684context\u610f\u4e49</li> <li></li> </ul> </li> </ul> </li> </ul> </li> <li>Encoder-decoder models   collapsed:: true<ul> <li></li> </ul> </li> <li>Pre-training encoders: BERT and relatives<ul> <li>BERT: Bidirectional Encoder Representations from Transformers</li> <li>Takes in a sequence of tokens, gives as output a vector for each token.</li> <li></li> <li>BERT\u662f\u4e2aencoder, \u62ff\u8fdb\u6765\u4e00\u5806\u8bcd\u8bed\u7684token, \u751f\u6210\u4ed6\u4eec\u7684embedding vector</li> <li></li> <li>Self-attention   collapsed:: true<ul> <li>The new representation of each word is calculated based on all the other words.</li> </ul> </li> <li>Multi-head self-attention   collapsed:: true<ul> <li>Each head learns to focus on different type of information.</li> </ul> </li> <li>\u4e0a\u56fe\u7684x1 x2\u7684embedding\u7531\u51e0\u4e2a\u90e8\u5206\u7ec4\u6210<ul> <li></li> <li>position embeddings \u544a\u8bc9transformer\u8bcd\u8bed\u4e4b\u95f4\u7684\u987a\u5e8f, segment embeddings\u544a\u8bc9transformer\u8fd9\u4e2a\u8bcd\u8bed\u5c5e\u4e8e\u54ea\u4e2a\u53e5\u5b50</li> <li>Token Embeddings \u662f\u968f\u7f51\u7edc\u4e00\u8d77\u8bad\u7ec3\u7684, \u7528\u7684\u662fnn.Embedding. \u5e76\u6ca1\u6709\u7528\u5176\u4ed6\u9884\u8bad\u7ec3\u7684embedding \u4f8b\u5982word2vec</li> <li>torch.nn\u5305\u4e0b\u7684Embedding\uff0c\u4f5c\u4e3a\u8bad\u7ec3\u7684\u4e00\u5c42\uff0c\u968f\u6a21\u578b\u8bad\u7ec3\u5f97\u5230\u9002\u5408\u7684\u8bcd\u5411\u91cf\u3002</li> </ul> </li> <li>Masked Language Modeling   collapsed:: true<ul> <li>\u6211\u4eec\u7684\u76ee\u6807\u662f\u8ba9\u6a21\u578b\u80fd\u591f\u7406\u89e3\u8fd9\u4e9b\u8bcd\u8bed\u5728\u53e5\u5b50\u4e2d\u7684\u610f\u601d, \u4ece\u800c\u53ef\u4ee5\u7ed9\u51fa\u4ed6\u4eec\u7684contextual vector\u5229\u7528\u4e8e\u66f4\u591a\u4e0b\u6e38\u4efb\u52a1\u4e2d, \u5176\u4e2d\u4e00\u79cd\u65b9\u6cd5\u5c31\u662fmask\u6389\u8f93\u5165\u4e2d\u7684\u4e00\u4e9b\u8bcd\u8bed, \u8ba9\u6a21\u578b\u731c\u6d4b\u4ed6\u4eec\u5e94\u8be5\u662f\u4ec0\u4e48, \u4ee5\u8fd9\u79cd\u65b9\u5f0f\u9f13\u52b1\u6a21\u578b\u5b66\u4e60general-purpose language understanding.</li> <li></li> <li></li> <li>\u6240\u8c13\u7684MLM\u5176\u5b9e\u662f\u4e00\u4e2a\u4e8eBERT\u4e4b\u4e0a\u7684\u795e\u7ecf\u7f51\u7edc, \u901a\u5e38\u662f\u7531MLP\u7ec4\u6210, in\u662fembedding dim, out\u662fvocabulary\u5927\u5c0f, BERT\u4f1a\u5bf9\u6709MASK\u7684\u4e00\u53e5\u8bdd(6\u4e2a\u8bcd, \u5305\u542b2\u4e2amask) encode\u5f97\u52306\u4e2aembedding, \u8fd9\u4e24\u4e2amask\u7684embedding\u4f1a\u7ed9\u5230MLM\u7f51\u7edc, \u7528\u6765map\u5230\u8bcd\u6c47\u8868\u4e2d, \u901a\u8fc7\u9f13\u52b1map\u5230\u771f\u6b63\u7684\u8bcd, \u6765\u9f13\u52b1BERT\u5b66\u4e60\u5230\u771f\u6b63\u7684\u8bed\u610f\u4fe1\u606f</li> </ul> </li> <li>Next sentence prediction   collapsed:: true</li> </ul> </li> <li>Putting pre-trained models to work   collapsed:: true<ul> <li>BERT-like models give us a representation vector for every input token. BERT\u4f1a\u7ed9\u8f93\u5165\u6587\u672c\u6bcf\u4e2a\u8bcd\u4e00\u4e2arepresentation, \u6211\u4eec\u53ef\u4ee5\u5229\u7528\u8fd9\u4e9bvectors\u6765\u8868\u793atokens\u6216\u8005\u53e5\u5b50</li> <li>Option 1:   Freeze BERT, use it to calculate informative representation vectors. Train another ML model that uses these vectors as input.</li> <li>Option 2 (more common these days):   Put a minimal neural architecture on top of BERT (e.g. a single output layer) Train the whole thing end-to-end (called fine-tuning).</li> <li>Sentence classification   collapsed:: true<ul> <li></li> </ul> </li> <li>Token labeling   collapsed:: true<ul> <li></li> </ul> </li> <li>Sentence pair classification   collapsed:: true<ul> <li></li> </ul> </li> <li>Question answering   collapsed:: true<ul> <li></li> </ul> </li> </ul> </li> <li> <p>Text classification with BERT in practice   collapsed:: true</p> <ul> <li>```python   checkpoint = 'bert-base-cased' # \u9009\u62e9\u6240\u9700\u8981\u7684\u6a21\u578b\u540d\u79f0, \u7528\u4e8e\u5728huggingface\u4e2d\u7d22\u5f15</li> </ul> <p># \u81ea\u52a8\u8f7d\u5165\u7528\u4e8e\u4e8c\u5206\u7c7b\u7684\u5bf9\u5e94checkpoint\u7684\u6a21\u578b   model = AutoModelForSequencialClassification.from_pretrined(checkpoint, num_labels=2)   model.to(device)</p> <p># \u52a0\u8f7d\u6570\u636e   datasets = load_dataset('glue', 'sst2')</p> <p># tokenisation, \u5bf9\u5e94model\u7684tokeniser   tokeniser = AutoTokenizer.from_pretrained(checkpoint)   tokenised = tokeniser('this is an example sentence', truncation=True)</p> <p>optimiser = AdamW(model.parameters(), lr=lr)</p> <p>num_epochs = 10</p> <p>for epoch in range(num_epochs):     for batch in train_loader:         outputs = model(batch)             loss = outputs.loss           loss.backward()           optimiser.step()           optimiser.zero_grad()</p> <p>```             - \u4e00\u4e9b\u4f8b\u5b50 \u770bppt             - Parameter-efficient fine-tuning               collapsed:: true - Models fine-tuned for one task are usually better at that particular task, compared to models trained to do many different tasks. - let\u2019s keep most of the parameters the same (frozen) and fine-tune only some of them to be task-specific. - Prompt tuning   collapsed:: true     - Include additional task-specific \u201ctokens\u201d in the input, then fine-tune only their embeddings for that particular task, while keeping the rest of the model frozen.     -  - Prefix tuning   collapsed:: true     -  - Control Prefixes   collapsed:: true     - Training different prefixes for each property/attribute you want the output to have. For example, the domain or desired length of the text. - Adapters   collapsed:: true     - Inserting specific trainable modules into different points of the transformer, while keeping the rest of the model frozen.     -  - BitFit   collapsed:: true     -  - Low-rank adaptation   collapsed:: true     -              - The keys to good models of language               collapsed:: true - 1. Transfer learning (model pre-training)   2. Very large models   3. Loads of data   4. Fast computation   5. A difficult learning task         - Module 6.2: pre-training encoder-decoder, decoder models, advanced prompting, human feedback           collapsed:: true             - Pre-training encoder-decoder models               collapsed:: true - Encoder-decoder:   Input is processed using an encoder, then output is generated using a decoder.   Particularly popular for machine translation. - can also be used for classification, by constructing a new output layer and connecting it to the last hidden state from the decoder. - Ideas of pre-training encoder-decoder models   collapsed:: true     - Can\u2019t really do Masked Language Modelling any more, there isn\u2019t a direct correspondence between input tokens and output tokens. \u8fd9\u91cc\u4e0d\u80fd\u518d\u76f4\u63a5\u628adecoder\u5bf9\u5e94\u4f4d\u7f6e\u8f93\u51fa\u7ed9\u5230MLM\u4e86, \u8fd9\u91cc\u548cBERT\u7684\u60c5\u51b5\u4e0d\u4e00\u6837, BERT\u662f\u7ed9\u7684\u4e00\u4e2a\u5bf9\u5e94mask\u4f4d\u7f6e\u7684embedding, \u4f46\u662fen-de\u7ed9\u7684\u662f\u4e00\u4e2a\u65b0\u751f\u6210\u7684\u4e1c\u897f, \u751f\u6210\u6a21\u578b, \u4e0d\u662f\u4e00\u5bf9\u4e00\u7684, \u6bd4\u5982\u6211\u4e5f\u53ef\u4ee5\u751f\u6210\u4e0d\u4e00\u6837\u957f\u5ea6\u7684, \u4e5f\u53ef\u4ee5\u53eb\u4ed6\u53ea\u751f\u6210mask\u6389\u7684\u4e1c\u897f, \u56e0\u6b64\u4f1a\u6709\u533a\u522b. \u6240\u4ee5\u7528\u7684\u65b0\u7684\u65b9\u5f0f\u662f\u7ed9decoder supervision, \u544a\u8bc9\u4ed6\u5e94\u8be5\u751f\u6210\u4ec0\u4e48, \u4f8b\u5982\u628a\u539f\u6587\u7ed9\u4ed6, \u4f5c\u4e3alabel, \u6216\u8005\u66f4\u96be\u4e00\u70b9, \u671f\u671b\u4ed6\u4ec5\u4ec5\u751f\u6210mask\u7684\u8bcd\u8bed     - Prefix language modeling       collapsed:: true         -      - Sentence permutation deshuffling       collapsed:: true         -      - BERT-style token masking       collapsed:: true         -  - Replace corrupted spans   collapsed:: true     - We can corrupt the original sentence in various different ways, then optimise the model to reconstruct the original sentences. Good for generation tasks. \u6253\u4e71\u539f\u53e5\u5b50, \u8ba9ende\u6765\u6062\u590d\u539f\u53e5\u5b50     -  - Instructional training   collapsed:: true     - Can be pre-trained trained on different supervised tasks, by stating in the input what task the model should perform. \u544a\u8bc9\u6a21\u578b\u4f60\u8981\u5e72\u4ec0\u4e48, \u6a21\u578b\u505a\u6211\u8bf4\u7684\u4e8b\u60c5     - T5 (Text-To-Text Transfer Transformer): trained using the span corruption unsupervised objective, along with a number of different supervised       tasks.     - Existing datasets can be converted into more conversational-sounding instructions using templates. \u53ef\u4ee5\u5c06\u539f\u6709\u7684\u6570\u636e\u96c6\u8fdb\u884c\u4e00\u4e9b\u66f4\u6539, \u628a\u4ed6\u4eec\u8f6c\u53d8\u6210\u81ea\u7136\u8bed\u8a00\u7684\u5f62\u5f0f       collapsed:: true         -      - \u8bad\u7ec3\u6a21\u5f0f: train with natural language instructions as inputs, and annotated answers as target outputs. \u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u6307\u793a\u4f5c\u4e3a\u8f93\u5165, \u6807\u8bb0\u8fc7\u7684\u7b54\u6848\u4f5c\u4e3a\u76ee\u6807\u8f93\u51fa             - Pre-training decoder models               collapsed:: true - Decoders: \u8bed\u8a00\u6a21\u578b, Are able to access context on the left. Language models, good for generating text. \u524d\u9762\u8f93\u51fa\u7684\u7ed3\u679c\u4f5c\u4e3a\u540e\u9762\u7684\u8f93\u5165, \u540e\u9762\u7684\u53ea\u80fd\u770b\u5230\u524d\u9762\u7684\u4e1c\u897f - \u8bad\u7ec3\u6a21\u5f0f: We can train on unlabeled text, optimizing \\(p_\\theta (w_t|w_{1:t-1})\\), Great for tasks where the output has the same vocabulary as the pre-training data. \u9002\u7528\u4e8e\u8f93\u51fa\u8bcd\u6c47\u548c\u4e0e\u8bad\u7ec3\u8bcd\u6c47\u76f8\u4f3c\u7684\u4efb\u52a1: dialogue systems, summarization, simplification, etc. - Learning methods: \u4e00\u4e9b\u6a21\u578b\u5b66\u4e60\u56de\u7b54\u95ee\u9898\u7684\u529e\u6cd5   collapsed:: true     - 1. Fine-tuning: Supervised training for particular input-output pairs.       Or we can put a new layer on top and fine-tune the model for a desired task.     - 2. Zero-shot: Give the model a natural language description of the task, have it generate the answer as a continuation. \u7ed9\u4e00\u4e2a\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u4efb\u52a1, \u4e0d\u7ed9example, \u76f4\u63a5\u8ba9\u6a21\u578b\u8f93\u51fa\u60f3\u8981\u7684\u7b54\u6848       3. One-shot: In addition to the description of the task, give one example of solving the task. No gradient updates are performed. \u4f1a\u7ed9\u5230\u4e00\u4e2aexample, \u4e0d\u518d\u8fdb\u884c\u68af\u5ea6\u66f4\u65b0       4. Few-shot: In addition to the task description, give a few examples of the task as input. No gradient updates are performed. \u7ed9\u5230\u66f4\u591a\u7684example, \u540c\u6837\u6ca1\u6709\u68af\u5ea6\u66f4\u65b0 - Fine-tuning decoder models   collapsed:: true     - Once pre-trained, we can fine-tune these models as classifiers, by putting a new output layer onto the last hidden layer.       The new layer should be randomly initialised and then optimized during training.       We can backpropagate gradients into the whole network.     - pre-trained model\u53ef\u4ee5\u5728\u6700\u540e\u7684\u9690\u85cf\u5c42\u540e\u9762\u52a0\u4e00\u4e2a\u65b0\u7684\u8f93\u51fa\u5c42, \u7528\u6765\u6839\u636edecoder\u4fe1\u606f\u6765\u5b8c\u6210\u7279\u5b9a\u7684\u4efb\u52a1, GPT\u5c31\u662f\u5148\u662f\u7528\u751f\u6210\u6765\u4e0e\u8bad\u7ec3, \u6700\u540efinetune\u6210\u4e86\u8fa8\u522b\u5668     -      - \u53ef\u4ee5\u5728sentence\u4e2dtoken\u5230\u4e00\u4e9b\u7279\u6b8a\u7684token\u4f8b\u5982, \u7528\u6765\u7ed9\u5206\u7c7b\u5668\u5206\u7c7b\u5b66\u4e60 -             - Advanced prompting               collapsed:: true - Chain-of-thought   collapsed:: true     - \u5728\u4e3e\u4f8b\u4e2d\u7ed9\u5230reasoning, \u53ef\u4ee5encourage model \u5728\u56de\u7b54\u7c7b\u4f3c\u95ee\u9898\u7684\u65f6\u5019\u505a\u7c7b\u4f3c\u7684reasoning, \u800c\u4e0d\u662f\u53ea\u751f\u6210\u4e00\u4e2a\u7b54\u6848     - \u751a\u81f3\u53ef\u4ee5\u6a21\u62dfcode, \u7ed9\u51fa\u9002\u5f53\u7684code, \u5e76\u4e14\u771f\u7684\u53ef\u4ee5\u88ab\u6267\u884c\u5f97\u5230\u7b54\u6848     - Zero-shot chain-of-thought       collapsed:: true         - \u53ef\u4ee5\u4e0d\u8bf4\u662fthe answer is, \u800c\u662flets think step by step, \u4e5f\u53ef\u4ee5\u63d0\u793a\u6a21\u578b\u7ed9\u51fachain of thought - Retrieval-based language models   collapsed:: true     -      - \u589e\u52a0\u4e86\u4ece\u6570\u636e\u5e93\u4e2d\u83b7\u53d6\u4fe1\u606f\u7684\u80fd\u529b - Limitations of instruction fine-tuning   collapsed:: true     - Language models are being trained with instruction fine-tuning, using manually created ground truth data that follows instructions. This data is expensive to collect. \u6307\u4ee4\u548c\u7b54\u6848\u7684\u8bad\u7ec3\u96c6\u5f88\u6602\u8d35, \u56e0\u4e3a\u662f\u4eba\u5de5\u7684     - \u25cf Problem 1: tasks like open-ended creative generation have no right answer.           Write me a story about a dog and her pet grasshopper. \u5f00\u653e\u6027\u95ee\u9898       \u25cf Problem 2: language modeling penalizes all token-level mistakes equally, but some errors are worse than others.\u6709\u7684\u9519\u8bef\u66f4\u4e25\u91cd, \u4f46\u662f\u60e9\u7f5a\u8d77\u6765\u662f\u4e00\u6837\u7684     -             - Learning from human feedback               collapsed:: true - Reinforcement learning from human feedback   collapsed:: true     - \u5f3a\u5316\u5b66\u4e60\u7684\u6982\u5ff5\u5462\u5c31\u662f\u8bf4\u6709\u4e00\u4e2a\u73af\u5883\u7ed9\u5230\u7684\u8bc4\u4ef7, reward, \u6bd4\u5982\u8bf4\u4e00\u4e2a\u7ed9\u6587\u672c\u521b\u5efa\u6982\u62ec\u7684\u4efb\u52a1, \u4eba\u53ef\u4ee5\u7ed9\u6a21\u578b\u7684\u8f93\u51fa\u6253\u5206\u6765\u544a\u8bc9\u6a21\u578b\u8fd9\u4e2a\u597d\u4e0d\u597d     -      - \u95ee\u9898\u4f9d\u7136\u5728\u4e8e\u4eba\u529b\u6210\u672c\u592a\u9ad8\u4e86, \u89e3\u51b3\u65b9\u6848\u53ef\u4ee5\u662f\u7528\u4e00\u4e2aLM\u6765\u9884\u6d4b\u4eba\u7c7b\u53ef\u80fd\u6253\u7684\u5206     - \u53e6\u4e00\u4e2a\u95ee\u9898\u662f\u4eba\u7c7b\u7684\u5224\u65ad\u662fnoisy\u4e14\u6709\u5931\u7126\u51c6\u7684, \u89e3\u51b3\u65b9\u6848\u662f\u4e0d\u53bb\u7ed9\u4e00\u4e2a\u5206, \u800c\u662f\u53bb\u5224\u65ad\u4e24\u4e2a\u4f8b\u5b50\u54ea\u4e2a\u66f4\u597d, \u8fd9\u6837\u5b50\u80fd\u51cf\u5c11noise     - Week8 POS, Constituency parsing, Dependency parsing       collapsed:: true         - Module 7.1 POS Tagging             - Tagset - \u25cf ADJ (adjective): old, beautiful, smarter, clean...   \u25cf ADV (adverb): slowly, there, quite, gently, ...   \u25cf INTJ (interjection): psst, ouch, hello, ow   \u25cf NOUN (noun): person, cat, mat, baby, desk, play   \u25cf PROPN (proper noun): UK, Jack, London   \u25cf VERB (verb): enter, clean, play   \u25cf PUNCT (punctuation): . , ()   \u25cf SYM(symbol):\\(,%,\u00a7,\u00a9, +,\u2212,\u00d7,\u00f7,=,&lt;,&gt;,:),\u2665   \u25cf X (other): ? (code switching) - \u25cf PREP (preposition): in, on, to, with   \u25cf AUX (auxiliary verb): can, shall, must   \u25cf CCONJ (coordinating conjunction): and, or   \u25cf DET (determiner): a, the, this, which, my, an   \u25cf NUM (numeral): one, 2, 300, one hundred   \u25cf PART (particle): off, up, \u2018s, not   \u25cf PRON (pronouns): he, myself, yourself, nobody   \u25cf SCONJ (subordinating conjunction): that, if, while             - Why need PoS Tagging - \u5bf9\u4e8eNER\u800c\u8a00, \u53ef\u4ee5\u5148\u8bc6\u522b\u51fanoun, \u518d\u8fdb\u884c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b, \u5373\u8bc6\u522b\u51fa\u4e00\u4e9b\u4e13\u6709\u540d\u8bcd - \u4e00\u4e9b\u9884\u5904\u7406\u53ef\u4ee5\u5728POS tagging\u7684\u57fa\u7840\u4e0a\u5b8c\u6210, \u4f8b\u5982\u60c5\u611f\u5206\u6790\u7684\u9884\u5904\u7406\u53ef\u4ee5\u63d0\u53d6\u51fa\u5f62\u5bb9\u8bcd - \u4e5f\u53ef\u4ee5\u7528\u4e8esyntactic and semantic parsing, \u53e5\u6cd5\u548c\u8bed\u610f\u5206\u6790 - For many applications such as spam detection, sentiment analysis at scale (e.g. millions of emails going through a server) you can focus on nouns,verbs and adjectives removing redundant tokens. \u63d0\u70bc\u51fa\u5783\u573e\u90ae\u4ef6\u4e2d\u7684\u540d\u6b21\u52a8\u8bcd\u548c\u5f62\u5bb9\u8bcd, \u6765\u589e\u52a0\u6548\u7387             - POS tagging libraries - Spacy,NLTK             - baseline method - Assign each word its most frequent POS tag - unknown words the tag NOUN - 90% acc             - Facing difficulties - Ambiguity - Unknown words             - Probabilistic POS tagging - Given a sequence of words W = w1, w2, w3, ..., wn - Estimate the sequence of POS tags T= t1, t2, t3, ...., tn - Compute \\(P(T|W)\\) - Generative approach (Bayes):     -      -      - \u57fa\u4e8e HMM, Hidden Markov Model,     - \u4e00\u4e2a\u662ftransition probability, tag\u4f5c\u4e3astate, state\u4e4b\u95f4\u6709transition, transition\u6709\u6982\u7387, \u8fd9\u5c31\u662fP(T), \u7531\u524d\u4e00\u4e2atag \u8ddf\u4e0b\u4e00\u4e2atag\u7684\u53ef\u80fd\u6027     - \u53e6\u4e00\u4e2a\u662fEmission probability, \u67d0\u4e2atag\u53ef\u80fd\u7684\u8bcd\u4e2d, \u662f\u8fd9\u4e2a\u8bcd\u7684\u53ef\u80fd\u6027, \u76f8\u5f53\u4e8etag\u53d1\u5c04\u5230\u8bcd. \u6ce8\u610f\u6211\u4eec\u8fd9\u91cc\u6c42\u7684\u662f\u8bcd\u5e8f\u5217\u5bf9\u5e94\u67d0\u4e2atag\u5e8f\u5217\u7684\u53ef\u80fd\u6027, \u56e0\u6b64\u8fd9\u91cc\u77e5\u9053\u7684\u53ea\u80fd\u662f\u8fd9\u4e2atag\u6709\u591a\u5927\u53ef\u80fd\u5bf9\u5e94\u8fd9\u4e2a\u8bcd     -      - \u56e0\u6b64\u4e5f\u5c31\u9700\u8981\u4e24\u4e2a\u8868, transition \u4ee5\u53caemission     - Example         -          -          -          - \u6ce8\u610f\u8fd9\u4e2aemission\u8868\u662f\u6bcf\u79cdtag\u5bf9\u5e94\u4e8e\u67d0\u4e2a\u8bcd\u7684\u53ef\u80fd\u6027, \u6a2a\u7740\u770b, \u7528\u6765\u7b97given t, \u67d0\u4e2a\u8bcd\u51fa\u73b0\u7684\u6982\u7387         - \u63a5\u4e0b\u6765\u8981\u771f\u6b63\u8ba1\u7b97\u6bcf\u4e2a\u8bcd\u53ef\u80fd\u7684\u8bcd\u6027\u4e86, \u4e5f\u5c31\u662f         -          - given John \u662fpronoun\u7684\u53ef\u80fd\u6027\u5c31\u662f, pronoun \u672c\u8eab\u5728start symbol\u540e\u51fa\u73b0\u7684\u6982\u7387p(t) \u4e58\u4e0a\u662fpronoun\u7684\u60c5\u51b5\u4e0bJohn\u51fa\u73b0\u7684\u6982\u7387p(w|t). \u4e5f\u8981\u9178\u695a\u5176\u4ed6\u7684\u8bcd\u6027\u7684\u53ef\u80fd\u6027.         - \u56e0\u4e3a\u8fd9\u4e2aViterbi \u7b97\u6cd5\u7684\u6700\u4f18\u524d\u63d0\u7279\u70b9, \u6211\u4eec\u53ea\u9700\u8981\u4ee5\u524d\u4e00\u4e2a\u8bcd\u7684\u6700\u4f18\u8bcd\u6027\u4e3a\u524d\u63d0\u5373\u53ef, \u4e0d\u9700\u8981\u627e\u5230\u6240\u6709\u7684\u53ef\u80fd\u6700\u5927\u5316\u6574\u53e5\u8bdd\u7684prob, \u56e0\u6b64wants\u53ea\u9700\u8981\u8003\u8651John\u662fpropn\u7684\u60c5\u51b5\u4f5c\u4e3a\u524d\u4e00\u4e2a\u65f6\u95f4\u70b9\u5373\u53ef         -          - \u5982\u679c\u51fa\u73b0\u4e86\u4e0b\u4e2a\u8bcd\u57fa\u4e8e\u4e0a\u8bcd\u6982\u7387\u5168\u90e8\u4e3a\u96f6, \u8bf4\u660e\u6709\u53ef\u80fd\u4e0a\u4e2a\u8bcd\u7684\u8bcd\u6027\u9009\u9519\u4e86, \u8fd9\u4e2a\u65f6\u5019\u5c31\u53ef\u4ee5\u8fdb\u884c\u91cd\u65b0\u9009\u62e9\u4e0a\u4e2a\u8bcd\u6027\u7684\u64cd\u4f5c, \u4f8b\u5982         -          - \u4e0d\u518d\u8ba4\u4e3arace\u4e3anoun, \u800c\u662fverb\u4e86     - HMM tagger         -          -          - \u6709\u5f88\u5f3a\u7684\u524d\u63d0\u5047\u8bbe, \u672a\u6765\u53ea\u57fa\u4e8ecurrent state, observation \u53ea\u57fa\u4e8e\u5f53\u524dstate         - HMM\u7684\u5176\u4e2d\u4e00\u4e2a\u4f5c\u7528\u5c31\u662f: Decoding/inference: task of determining the hidden state sequence corresponding to the sequence of observations             -      - Vertibi algorithm         - Dynamic programming         -          -          -          -          -          - This gives us the chain of states that generates the observations with the highest probability \u60e0\u7279\u6bd4\u7b97\u6cd5\u4f1a\u628a\u6240\u6709\u53ef\u80fd\u6027\u4e0d\u4e3a0\u7684path\u90fd\u8003\u8651\u4e00\u4e0b         - Viterbi\u2019s running time is O(SN2), where S is the length of the input and N is the number of states in the model     - Beam search as alternative decoding algorithm         - \u56e0\u4e3a\u6709\u7684tagset\u592a\u5927\u4e86, \u4e0d\u9002\u5408\u5168\u90e8\u7b97\u4e00\u904d, \u8fd9\u4e2a\u65f6\u5019\u53ef\u4ee5\u7528beam search, \u4ec5\u4ec5\u9009\u53d6k=2\u6bd4\u5982\u8bf4\u4e2atop most promising paths, \u53ea\u8003\u8651prob top2\u7684\u90a3\u4e24\u4e2a     - MEMM for POS tagging         - maximum entropy classifier (MEMM)         - sequence version of logistic regression classifier, a discriminative model to directly estimate posterior         -          -          - \u4e0d\u4ec5\u8003\u8651\u5230\u4e86\u4e0a\u4e00\u4e2a\u65f6\u95f4\u70b9\u7684tag\u4fe1\u606f, \u8fd8\u8003\u8651\u5230\u4e86\u8fd9\u4e2a\u65f6\u95f4\u70b9\u7684word\u4fe1\u606f         -          -          -      - approaches to POS tagging         -          - Module 7.2 Constituency parsing \u9009\u533a\u5206\u6790, \u53e5\u6cd5\u6210\u5206\u5206\u6790, \u628a\u53e5\u5b50\u5206\u6210\u4e00\u5757\u5757, \u751f\u6210\u53e5\u6cd5\u7ed3\u6784             - Goal: Generate the structure for the sentence \u751f\u6210\u53e5\u5b50\u7684\u7ed3\u6784             - Applications - Grammar checking - Semantic analysis     - Question answering     - Named entity recognition - Machine translation             - Challenges - One label per group of words (any length in principle) - Structural plus POS ambiguity:     - \u4e0d\u4ec5\u4f1a\u6709\u6bcf\u4e2a\u8bcd\u8bcd\u6027\u7684\u6a21\u7cca, \u8fd8\u6709\u7ed3\u6784\u4e0a\u7684\u6a21\u7cca\u4e0d\u6e05 -              - Constituency parsing \u53e5\u5b50\u6210\u5206\u5206\u6790 - A constituent is a sequence of words that behaves as a unit, generally a phrase, \u53e5\u5b50\u6210\u5206, \u5355\u5143, \u77ed\u8bed -              - The CKY algorithm - Grammar needs to be in Chomsky Normal Form (CNF), into two smaller sequences, \u53d8\u6210\u4e24\u90e8\u5206, \u800c\u4e0d\u662fCFG\u90a3\u6837\u5b50\u7684\u53ef\u4ee5\u662f\u4e00\u5806 - Rules are of the formX\u2192YZ or X\u2192w -  - \u8fd9\u79cdbinarisation makes CKY \u975e\u5e38\u7684\u9ad8\u6548: O(n3|G|): n is the length of the parsed string; |G| is the size of the CNF grammar G -  -  - \u6bcf\u4e2a\u8868\u4e2d\u5143\u7d20\u4ee3\u8868\u7740\u8fd9\u4e00\u884c\u4e2d\u8986\u76d6\u7684\u4ed6\u4ee5\u53ca\u4ed6\u5de6\u8fb9\u7684\u6240\u6709\u8bcd\u7684\u7ec4\u5408\u7684\u7ed3\u6784\u540d\u79f0, \u53ef\u4ee5\u7531\u5176\u5de6\u8fb9\u5df2\u6709\u7684\u7ed3\u6784\u548c\u4e0b\u9762\u7684\u7ed3\u6784\u7ed3\u5408, \u9700\u8981\u7b26\u5408\u4e24\u4e2a\u5b50\u7ed3\u6784\u52a0\u8d77\u6765\u7684\u5b57\u6570\u7b49\u4e8e\u8fd9\u4e2a\u7ed3\u6784\u7684\u603b\u5b57\u6570, \u6ef4\u5165\u7b2c\u4e8c\u884c\u7684VP\u662f3\u4e2a\u5b57, (eats a fish), \u9700\u8981\u4e24\u4e2a\u5b57\u7ed3\u6784\u5206\u522b\u662f1\u548c2\u4e2a\u5b57, \u90a3\u5c31\u53ef\u4ee5\u662feats\u548ca fish\u7684\u7ec4\u5408, \u5206\u522b\u5bf9\u5e94\u4e8e\u7b2c\u4e8c\u884c\u7684\u7b2c\u4e00\u4e2a(V, VP)\u548c\u7b2c\u4e09\u884c\u7684\u7b2c\u4e8c\u4e2a\u7684a fish(NP), \u7ec4\u5408\u6210\u4e86VP - \u53f3\u4e0a\u89d2\u7684\u5143\u7d20\u4f1a\u662fS, \u8868\u793a\u8fd9\u662f\u4e00\u4e2a\u53e5\u5b50 - CKY for parsing (CKY \u627e\u5230\u6240\u6709\u53ef\u80fd\u6027)     - \u4ee5\u4e0a\u7684\u4f8b\u5b50recognise\u4e86\u6bcf\u4e2a\u7ed3\u6784\u7684\u6210\u5206\u540d\u79f0, \u5982\u679c\u8981\u7528\u6765\u5206\u6790, \u8981\u8003\u8651\u5230\u4e0d\u540c\u7684\u7ec4\u5408\u53ef\u80fd\u6027, \u5e76\u4e14\u5173\u8054\u5230\u4e4b\u524d\u7684\u5143\u7d20     -      -      - \u4f46\u662f\u53ef\u4ee5\u4ece\u4e0a\u9762\u770b\u51fa\u6765, \u8fd9\u79cd\u65b9\u6cd5, \u4f1a\u628a\u6240\u6709\u7684\u60c5\u51b5\u5168\u90e8\u90fd\u7f57\u5217\u51fa\u6765, \u6bcf\u79cd\u53e5\u5b50\u53ef\u80fd\u7684\u7ec4\u6210\u65b9\u5f0f\u90fd\u4f1a\u6709, \u8fd9\u4e2a\u60c5\u51b5\u5c31\u76f8\u5f53\u68d8\u624b, \u56e0\u4e3a\u5982\u679c\u53e5\u5b50\u4e00\u957f, \u8bed\u6cd5\u4e00\u590d\u6742, \u5c31\u4f1a\u6709\u592a\u591a\u7c7b\u4f3c\u7684\u6811, \u6211\u4eec\u9700\u8981\u57fa\u4e8e\u7edf\u8ba1\u6765\u5206\u6790, \u627e\u5230\u6700\u6709\u53ef\u80fd\u7684\u53e5\u6cd5\u5206\u6790(\u4eec)             - Statistical parsing - learning (\u901a\u8fc7\u6982\u7387\u627e\u5230\u6700\u53ef\u80fd\u7684\u90a3\u4e2a) - Treebanks: \u201cLearn\u201d probabilistic grammars from labelled data e.g. Penn Treebank - What does it mean to \u201clearn\u201d a grammar? - \u5b66\u4e60grammar\u7684\u610f\u601d\u5c31\u662f, \u6839\u636e\u6570\u636e\u5e93\u8bad\u7ec3\u96c6\u5bf9\u6bcf\u79cdrule\u8fdb\u884c\u4e00\u4e2aprobability\u7684assign -  - Probabilistic/stochastic phrase structure grammar (PCFG)     - a context-free grammar PCFG = (T, N, S, R, q)     - \u25cf T is set of terminals       \u25cf N is set of nonterminals       \u25cf S is the start symbol (non-terminal)       \u25cf R is set of rules X\u2192 , where X is a nonterminal and is a sequence of terminals &amp; nonterminals       \u25cf q = P(R) gives the probability of each rule            - \u4f8b\u5982NP -&gt; \u5176\u4ed6\u7684 \u6240\u6709\u53ef\u80fd\u6027\u548c\u4e3a1       collapsed:: true         -      - \u4e0a\u56fe\u53ef\u89c1 VP\u6709\u4e24\u79cd\u5206\u6cd5, \u5206\u522b\u4e3a0.7 \u548c0.3\u7684, \u5c31\u53ef\u4ee5\u5199\u51fa\u4e24\u9897\u6811\u6765     -      - \u800c\u8fd9\u4e00\u6574\u9897\u6811, \u4e5f\u5c31\u662f\u8fd9\u53e5\u8bdd\u7684\u53e5\u6cd5\u5206\u6790\u7ed3\u6784\u662f\u8fd9\u6837\u5b50\u7684\u53ef\u80fd\u6027, \u5c31\u662f\u628a\u6bcf\u4e2a\u5206\u7684\u64cd\u4f5c\u7684\u53ef\u80fd\u6027\u7ed9\u4e58\u8d77\u6765The probability of each of the trees is obtained by multiplying the probabilities of each of the rules used in the derivation     -      -      - \u56e0\u4e3a\u662f\u6709\u4e00\u4e2aVP\u7684\u4e0d\u540c\u5206\u652f, \u6240\u4ee5\u5728\u53e5\u6cd5\u5206\u6790\u7684\u610f\u4e49\u4e0a\u8fd9\u53e5\u8bdd\u51fa\u73b0\u7684\u6982\u7387\u5c31\u662f\u4e24\u79cd\u53e5\u6cd5\u51fa\u73b0\u7684\u6982\u7387\u7684\u548c     - \u4f46\u662f \u4ee5\u4e0a\u7684naive\u7684\u65b9\u6cd5\u53c8\u6709\u65e0\u6cd5scale\u7684\u95ee\u9898: enumerating all options - The CKY algorithm for PCFG     - Application       collapsed:: true         - \u25cb Recognition: does this sentence belong to the language? \u6bcf\u4e2a\u8bed\u8a00\u6709\u81ea\u5df1\u7684           \u25cb Parsing: give me a possible derivation \u7ed9\u51fa\u53ef\u80fd\u7684\u53e5\u6cd5\u5206\u6790           \u25cb Disambiguation: give me the best derivation \u53bb\u9664\u6b67\u4e49     - Same dynamic programming algorithm, but now to find most likely parse tree     -      - \\(\\pi[i,j,X]\\)= **maximum** probability of a constituent with non-terminal X         spanning words i...j  inclusive; i,j\u6307\u7684\u662f\u4e00\u4e2a\u5b50\u5e8f\u5217     - \u4f8b\u5982\\)\\pi[2,5,NP]\\(\u610f\u601d\u5c31\u662fthe parse tree with the highest probability for words 2-5, whose head is NP     - \u76ee\u7684\u5c31\u662f\u901a\u8fc7CKY\u7684\u65b9\u5f0f\u627e\u5230\u6700\u53ef\u80fd\u7684\u90a3\u4e2aparsing P(t) for the whole sentence       collapsed:: true         - ![image.png](../assets/image_1678193720984_0.png)     - ![image.png](../assets/image_1677618871752_0.png)     - ![image.png](../assets/image_1677619043392_0.png)     - \u4ece\u4e0a\u9762\u8fd9\u4e24\u4e2a\u56fe\u53ef\u4ee5\u770b\u51fa, \u9012\u5f52\u7684\u5e94\u7528, \u4f7f\u5f97\u8fd9\u4e2a\u95ee\u9898\u53d8\u6210\u4e86\u4e0d\u65ad\u5bfb\u627esplit\u4ee5\u540e\u7684\u5206\u652f\u7684\u53ef\u80fd\u6027, \u8981\u9996\u5148\u8003\u8651\u5206\u88c2\u4ee5\u540e\u4e24\u4e2a\u5206\u652f\u53ef\u80fd\u7684\u53e5\u6cd5\u7ec4\u5408, \u4f8b\u5982\u5728\u8ba1\u7b97(3,8)\u4e3aVP\u7684\u53ef\u80fd\u6027\u7684\u65f6\u5019, \u8003\u8651VP\u53ef\u80fd\u5206\u6210\u4e24\u79cd\u7ec4\u5408, \u7136\u540e\u518d\u5728\u4e24\u79cd\u7ec4\u5408\u5185\u5bfb\u627e\u5206\u88c2\u70b9, \u9010\u6e10\u9012\u5f52\u5230base case. \u6982\u7387\u5176\u5b9e\u5c31\u662f\u5c42\u5c42\u76f8\u4e58\u7684\u7ed3\u679c, \u4f1a\u9012\u5f52\u56de\u6765. \u7136\u540e\u53d6\u53ef\u80fd\u6027\u6700\u5927\u7684\u90a3\u4e2a\u53e5\u6cd5\u7ec4\u5408, \u4ee5\u53ca\u8be5\u53e5\u6cd5\u7ec4\u5408\u7684split\u70b9\u4f5c\u4e3a\u8fd9\u4e2a(3,8)\u4e3aVP\u7684\u53ef\u80fd\u6027     - \u8fd9\u4e2a\u65b9\u6cd5\u5bf9\u6bcf\u79cd\u53ef\u80fd\u90fd\u8fdb\u884c\u4e86\u6982\u7387\u8fd0\u7b97, \u800c\u4e0d\u662f\u539f\u59cb\u7684CKY\u7b97\u6cd5\u90a3\u6837\u5b50\u628a\u6240\u6709\u7684\u53ef\u80fd\u6027\u90fd\u627e\u51fa\u6765, \u5217\u51fa\u6765. \u8fd9\u4e2aCKY for PCFG \u4f1a\u628a\u6700\u4f18\u89e3\u7ed9\u51fa\u6765     - ![image.png](../assets/image_1677619065356_0.png)     - Example         - ![image.png](../assets/image_1678194833743_0.png)         - \u4e0a\u9762\u7684\u4f8b\u5b50, S\u662f\u6700\u540e\u6211\u4eec\u8981\u6c42\u7684\u6574\u53e5\u8bdd\u7684\u53ef\u80fd\u6027. \u4e5f\u5c31\u662f\\)\\pi (1,8,S)\\(\u7684\u6700\u5927\u503c         - \u53ef\u4ee5\u88ab\u5206\u89e3\u4e3a, S\u7684split\u7684\u53ef\u80fd\u6027\u4e58\u6700\u5927\u53ef\u80fd\u6027\u7684\u5206\u88c2\u70b9\u4e24\u8fb9\u7684\u53ef\u80fd\u6027, \u4e5f\u5c31\u662f\u5206\u88c2\u70b9\u4e3a2...7\u4f7f\u5f97\u4e24\u8fb9\u53ef\u80fd\u6027\u4e58\u8d77\u6765\u6700\u5927\u7684\u90a3\u4e2a\u5206\u88c2\u70b9         - \u7531\u4e8eS\u53ea\u6709\u4e00\u79cd\u5206\u6cd5, \u6240\u4ee5\u662f\\)1 \\times \\pi (1,?,NP) \\times \\pi (1,?,VP)$         - \u4e0a\u9762\u7684\u95ee\u53f7, \u9700\u8981\u4e00\u4e2a\u4e2a\u8bd5, \u627e\u5230\u80fd\u8ba9\u8fd9\u4e2a\u6700\u5927\u7684\u90a3\u4e2a - Evaluating parsers     - \u25cf Parseval metrics: evaluate structure       collapsed:: true         - \u25cb How much of constituents in the hypothesis parse tree           look like the constituents in a gold-reference parse tree           \u25cb A constituent in hyp parse is labelled \u201ccorrect\u201d if there is a           constituent in the ref parse with the same yield and LHS symbol               \u25a0 Only rules from non-terminal to non-terminal           \u25cb Metrics are more fine-grained than full tree metrics, more           robust to localised differences in hyp and ref parse trees     -      - \u5de6\u8fb9\u56db\u4e2a\u662f\u6211\u4eec\u9884\u6d4b\u5230\u7684\u6709\u7684\u53e5\u6cd5\u7ed3\u6784, \u53f3\u8fb9\u662f\u771f\u5b9e\u7684, \u53ea\u8981\u6211\u4eec\u9884\u6d4b\u7684\u5728\u53f3\u8fb9\u6709, \u5c31precision\u6709\u4e86     -      - \u8fd9\u91cc\u4e3a\u4e86\u7b80\u4fbf, \u53ea\u8003\u8651\u4e86non terminal\u548c\u975e\u4e00\u5143\u7684(\u5c31\u662f\u81f3\u5c11\u6709\u4e24\u4e2a\u5206\u53c9\u7684\u90a3\u4e9b\u53e5\u6cd5\u7ed3\u6784) \u5de6\u8fb9\u5c31\u53ea\u6709\u4e00\u4e2aNP, \u53f3\u8fb9\u67096\u4e2a, \u6240\u4ee5recall\u53ea\u67091/6\u4e86 - Issues with PCFG     - Poor independence assumption         - CFG\u662fcontext free\u7684, \u6bcf\u4e2a\u8bcd\u8bed\u7684dependency\u53ea\u6709\u4ed6\u4eec\u81ea\u5df1, \u4e0d\u5b58\u5728\u76f8\u4e92\u5173\u8054, \u4ed6\u4eec\u53ea\u4f9d\u8d56\u4e8e\u4ed6\u4eec\u81ea\u5df1\u7684POS tag         - \u5728\u82f1\u8bed\u4e2d\uff0c\u4e3b\u8bed\u529f\u80fd\u7684NPs\u66f4\u6709\u53ef\u80fd\u88ab\u6d3e\u751f\u4e3a\u4e3b\u8bed\uff0891%\uff09\uff0c\u800c\u5bbe\u8bed\u529f\u80fd\u7684NPs\u66f4\u6709\u53ef\u80fd\u88ab\u6d3e\u751f\u4e3a\u975e\u4e3b\u8bed\uff0866%\uff09\u3002         -          - In this case, S \u2192 NP^S VP^S could indicate that the           first NP is the subject         -          - \u7ed9\u53e5\u6cd5\u7ed3\u6784\u6ce8\u91ca\u4e0a\u4e86\u4e0a\u4e00\u5c42\u7684\u4fe1\u606f, \u544a\u77e5\u5176\u4e3asubject\u7684\u53ef\u80fd\u6027, \u56e0\u4e3a\u4e0a\u4e00\u5c42\u662fS\u7684\u8bdd, NP\u66f4\u6709\u53ef\u80fd\u662fsubject, \u56e0\u6b64\u4e0b\u4e00\u5c42\u4f1a\u66f4\u53ef\u80fd\u662fPRP; \u800c\u53f3\u4fa7\u7684VP\u4e4b\u4e0b\u7684NP\u5219\u66f4\u53ef\u80fd\u4e3aobject, \u56e0\u6b64\u7ec6\u5206     - Lack of lexical conditioning         - \u7f3a\u5c11\u4e86\u8bcd\u6c47\u6761\u4ef6, \u6ca1\u6709\u8003\u8651\u5230\u7279\u4fd7\u8bcd\u6c47\u7684\u53e5\u6cd5\u4e8b\u5b9e, \u4f8b\u5982\u56fa\u5b9a\u7684\u642d\u914d, \u56e0\u4e3a\u6709\u4e9b\u8bcd\u6c47\u66f4\u5bb9\u6613\u7ec4\u5408\u5728\u4e00\u8d77         -          - The affinity between \u2018dump\u2019 and \u2018into\u2019 is greater than the affinity between \u2018masks\u2019 and \u2018into\u2019. Conversely , the affinity btw. \u2018kilos\u2019 and \u2018of\u2019 is greater than btw. \u2018catch\u2019 and \u2018of\u2019 - Probabilistic Lexicalised CFG     - Add annotations specifying the head of each rule, \u6307\u660e\u6bcf\u6761\u89c4\u5219\u7684head     -      -      - head\u662f\u81ea\u4e0b\u800c\u4e0a\u4f20\u9012\u7684, Head\u662f\u6838\u5fc3\u8bed\u8a00\u6982\u5ff5\uff0c\u662f\u6bcf\u6761\u89c4\u5219\u7684\u6838\u5fc3\u5b50\u6210\u5206, \u53ef\u4ee5\u4f7f\u7528rule\u6765\u5206\u8fa8\u4ed6\u4eec, \u80fd\u591f\u5927\u5927\u63d0\u5347\u51c6\u786e\u7387, \u4f8b\u5982         -              -         - Module 7.3 Dependency parsing             - \u793a\u4f8b -              - Connect words in sentence to indicate dependencies between them - much older linguistic theory. Build around notion of having heads and dependents.             - \u25cb Head (governor), also called argument: origin               \u25cb Dependent, also called modifier: destiny             - \u793a\u4f8b -  -  - \u53ef\u4ee5\u89c1\u5230, prefer\u88ab\u8ba4\u4e3a\u662f\u8fd9\u53e5\u8bdd\u7684root, \u6240\u6709\u5176\u4ed6\u7684\u8bcd\u90fd\u662f\u7531\u8fd9\u4e2ahead\u51fa\u53d1\u7684, \u662fdependent. \u56e0\u6b64prefer\u662froot. flight\u4e5f\u662f\u4e00\u4e2a\u5de8\u5927\u7684head, linked to\u5f88\u591a\u7684dependency             - Main advantages in dependency parsing - \u25cb Ability to deal with languages that are morphologically   rich and have a relatively free word order. E.g. Czech location adverbs may occur before or after object:   I caught the fish here vs I caught here the fish   \u4e0d\u518d\u53d7\u5230\u8bed\u5e8f\u7684\u5f71\u54cd - \u25cb Would have to represent two rules for each possible place of the adverb for constituency   \u5982\u679c\u662fconstituent parsing\u7684\u8bdd\u5fc5\u987b\u4e3a\u9009\u533a\u526f\u8bcd\u7684\u6bcf\u4e2a\u53ef\u80fd\u7684\u4f4d\u7f6e\u8868\u793a\u4e24\u6761\u89c4\u5219 - \u25cb Dependency approach: only one link; abstracts away from word order   \u4f46\u662fdependency \u7684\u8bdd\u5c31\u53ea\u9700\u8981\u4e00\u4e2alink\u5c31\u53ef\u4ee5\u4e86 - \u25cb Head-dependent relations provide approximation to semantic relationship between predicates and arguments - \u25cb Can be directly used to solve problems such as co-reference resolution, question answering, etc.   \u53ef\u4ee5\u88ab\u76f4\u63a5\u7528\u4e8e co-reference resolution, question answering     - For example, if the dependency parser identifies that \"John\" is the subject of the sentence \"He went to the store,\" then it can be inferred that \"he\" refers to John.     - This graph can then be used to identify the relevant information in a text corpus that can be used to answer the question.             - Dependency formalisms - general case - A dependency structure is a directed graph G = (V, A) - \u25cb Has a single ROOT node that has no incoming arcs   \u25cb Each vertex has exactly one incoming arc (except for ROOT)   \u25cb There\u2019s a unique path from ROOT to each vertex   \u25cb There are no cycles A\u2192B,B\u2192A - \u7279\u6027     - \u5f62\u6210\u4e86\u4e00\u68f5\u6811     - \u6bcf\u4e2a\u8bcd\u53ea\u6709\u4e00\u4e2a\u6307\u5411\u5b83\u7684head     - connected     - \u53ea\u6709\u4e00\u4e2aroot             - Dependency parsing - two approaches - Shift-reduce (transition-based)     - \u25cb Predict from left-to-right       \u25cb Fast (linear), but slightly less accurate       \u25cb MaltParser     - Greedy choice of attachment for each word in order, guided by ML classifier, Linear time parsing!     -      -      -      -      - right arc\u5c31\u662f\u628abuffer\u91cc\u9762\u7684\u4e24\u4e2a\u8bcd\u4e2d\u7684\u53f3\u8fb9\u90a3\u4e2a\u53d6\u51fa\u6765\u4f5c\u4e3adeterminant, \u6ca1\u53d6\u51fa\u6765\u7684\u90a3\u4e2a\u662fhead, \u4e0a\u9762\u7684\u4f8b\u5b50\u91cc\u9762ate\u5c31\u662fhead\u6307\u5411\u4e86fish     - Each action is predicted by a discriminative classifier over each move, \u4f7f\u7528\u4e86ML\u6765\u9884\u6d4b, feature\u662fstack \u9876\u90e8\u7684\u8bcd\u8bed\u548cPOS, buffer \u9876\u90e8\u7684\u8bcd\u548c\u4ed6\u7684POS     - \u6539\u8fdb\u65b9\u6848:         - Replace binary features by embeddings, Concatenate these embeddings - Spanning tree (graph-based, constraint satisfaction)     - \u25cb Calculate full tree at once       \u25cb Slightly more accurate, slower       \u25cb MSTParser             - Dependency parsing - evaluation - Accuracy, precision or recall     - Count identical dependencies: span (non-typed parsing) or span and type of dependency (typed parsing)     -              - Neural parsing - simple approach - Parsing as translation     - Linearise grammar from treebank: convert tree to bracketed representation, all in one line     - Pair sentences and their linearised trees     - \u628adependency\u7ed3\u6784\u53d8\u6210\u7ebf\u6027\u7684\u4e00\u884c\u8bdd, \u7136\u540epair\u8d77\u6765, \u76f4\u63a5\u8ba9\u6a21\u578b\u5b66     -  - Graph-basedmethods     - Week9 Revision       collapsed:: true         - Exam topics           collapsed:: true             - 2122               collapsed:: true - 1. Smoothing, CKY, HMM &amp; Viterbi, character-level language model &amp; token-level (\u8001ppt\u91cc) - 2. many-to-one or many-to-many RNN, metrics, machine translation, BPE, Transformers and (attention-based) RNNs - 3. self-attention, BERT, MLM, how NER is performed, Transformer parameters             - 2021               collapsed:: true - 1. word2vec, skip-gram, BPE, n-gram language model, CKY, parse trees - 2.  word sense disambiguation, pre-processing techniques, CNN for hate detection - 3. FFLM, number of learnable parameters, GRU, NMT, summarise hidden states of RNN, BLEU, loss of NMT - 4.   neural models based on the autoregressive, autoencoding, comparison, HMM, POS, Viterbi, how Transformers can learn long sequences             - 1920               collapsed:: true - 1. skip-gram using softmax and negative sampling,  n-gram language model, CKY - 2. NN for sentiment classification, CNN for sentiment analysis, - 3. markov assumption, BPTT, cross-entropy, attention - 4. word2vec skip-gram model (original formulation), n-gram language model, recurrent neural net language model, and transformer-based embeddings, CNN in text classification, HMM, Viterbi, transformer-based and attentive recurrent neural machine translation.             - 1819               collapsed:: true - 1. negative sampling and skip-gram and loss function, bigram language model for a corpus, pre-processing techniques, perplexity, mitigate issues with zero-count, HMM POS table - 2. CNN for sentiment analysis, classifier - 3. (RNN) language model, initialise an RNN with pre-trained word embeddings, self-attention, role of attention in NMT, loss function in RNN, metrics to evaluate RNN LM             - \u603b\u7ed3\u4e00\u4e0b:               collapsed:: true - \u8ba1\u7b97: CKY, HMM &amp; Viterbi, n-gram, parameters\u8ba1\u7b97 - \u96be\u70b9: CNN\u7684\u7ec6\u8282, filer size, padding, \u5982\u4f55\u5e94\u7528\u7684; \u5404\u4e2a\u7b97\u6cd5\u4f7f\u7528\u7684loss func, metrics, \u57fa\u7840\u7684pre-processing\u7684\u65b9\u6cd5\u4eec, disambiguate         - Ed           collapsed:: true             - Layer Norm serves 2 purposes in the Transformer:               collapsed:: true - 1) Rescaling inputs to improve generalization - 2) Inducing non-linearities into the model             -         - ---         - \u5168\u5c40\u7684\u7406\u89e3             - Classifier               collapsed:: true - Bag of words   collapsed:: true     - \u63d0\u70bc\u51fa\u8bad\u7ec3\u96c6\u4e2d\u6709\u591a\u5c11\u8bcd\u6c47, V\u7ef4\u5411\u91cf, \u6bcf\u53e5\u8bdd\u7528V\u7ef4\u5411\u91cf\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u8bcd\u6c47\u51fa\u73b0\u7684\u6b21\u6570\u4f5c\u4e3a\u5176vector\u6765\u8fdb\u884c\u540e\u7eed\u4efb\u52a1. \u4f1a\u51fa\u73b0\u5f88\u4e0d\u51c6\u786e\u7684\u60c5\u51b5, \u5176\u5047\u8bbe\u662f\u6240\u6709\u8bcd\u8bed\u90fd\u662f\u5355\u72ec\u7684feature, \u4f46\u662f\u5b9e\u9645\u4e0a\u8bcd\u8bed\u95f4\u6709\u5f3a\u5173\u8054, \u4f8b\u5982'not good'\u4e24\u4e2afeature\u95f4highly correlated, \u6b64\u65f6P (x1, x2|y) is not equal to P (x1|y) \u00d7 P (x2|y), \u6b64\u65f6\u4f1a\u6781\u5927\u5f71\u54cd\u6a21\u578b\u5224\u65ad. - Binary Naive Bayes   collapsed:: true     - \u4e00\u53e5\u8bdd\u4e2d(\u67d0\u4e2a\u6b63\u4f8b)\u51fa\u73b0\u4e86\u548c\u6ca1\u51fa\u73b0\u67d0\u4e2afeature, \u800c\u4e0d\u662f\u8fd9\u4e2afeature\u7684\u4e2a\u6570. \u6240\u4ee5\u8bad\u7ec3\u7684\u65f6\u5019\u5c31\u662fp('good'|+)\u8ba1\u7b97\u7684\u662f\u51fa\u73b0\u4e86\u6b63\u4f8b\u7684\u53e5\u5b50\u6570\u9664\u4ee5\u6b63\u4f8b\u53e5\u5b50\u6570; \u5982\u679cadd one smoothing\u7684\u8bdd, \u5206\u5b50\u52a0\u4e00, \u5206\u6bcd\u52a0\u4e0afeature\u79cd\u7c7b\u603b\u6570             - Language Model               collapsed:: true - language model\u53ef\u4ee5\u8ba4\u4e3a\u662fdecoder, \u7528\u4e8e\u751f\u6210\u6587\u672c, \u4e5f\u53ef\u4ee5\u7528\u4e8e\u9884\u6d4b\u67d0\u4e2a\u6587\u672c\u7684\u5b58\u5728\u53ef\u80fd\u6027 - CE\u7528\u6765\u8ba1\u7b97loss, PPL\u7528\u6765\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b, 1\u6700\u597d - n-gram\u4ec5\u8003\u8651\u4e86\u7edf\u8ba1\u6570\u636e, \u957fdependency\u6ca1\u529e\u6cd5\u8003\u8651\u5230 - RNN \u53ef\u4ee5\u7528\u4e8e\u4ece\u524d\u5f80\u540e\u751f\u6210\u6587\u672c - Bi-directional RNNs \u53ef\u4ee5\u7528\u4e8e\u7406\u89e3\u524d\u540e\u8bed\u4e49\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u6587\u672c, \u53ef\u4ee5\u7528\u4e8e\u8bed\u6cd5\u7ea0\u6b63 - LSTM \u7528cell state\u548cgates\u9009\u62e9\u6027\u4fdd\u7559gradient - 4-gram FF LM \u5728concat\u4e86\u4e09\u4e2aembedding\u4ee5\u540e\u8fdb\u884c\u4e86\u4e00\u4e2atanh\u6fc0\u6d3b, \u518d\u6295\u5230V\u8fdb\u884csoftmax, \u76ee\u7684\u662f\u63d0\u53d6\u5230\u6709\u7528\u7684\u5e0c\u671b\u88ab\u5173\u6ce8\u5230\u7684feature - Beam search, greedy decoding, Temperature sampling: \u4fdd\u7559k\u4e2a, \u4fdd\u75591\u4e2a, smooth\u4ee5\u540e\u7684softmax\u7684\u6982\u7387\u7528\u6765sample             - Encoder               collapsed:: true - BERT\u53ea\u6709encoder, \u56e0\u4e3a\u76ee\u7684\u662fencode texts to latent representation, \u6765\u8868\u793a\u67d0\u4e2a\u8bcd\u5e26\u6709context\u7684\u542b\u4e49\u6216\u8005\u6574\u53e5\u8bdd\u7684\u542b\u4e49, \u670d\u52a1\u4e8e\u4e0b\u6e38\u76ee\u6807 - \u6ce8\u91cd\u7f16\u7801\u63d0\u53d6\u4e0a\u4e0b\u6587\u7279\u5f81\u7528\u4e8e\u4e0b\u6e38, \u6216\u9884\u6d4b\u4e2d\u95f4\u8bcd: classification, MLM, NER, Q&amp;A; BERT, RoBERTa             - Decoder               collapsed:: true - GPT\u53ea\u6709decoder, \u662f\u4e2alanguage model, \u7528\u6765\u751f\u6210\u6587\u672c, \u800c\u4e0d\u662fencode \u6587\u672c. \u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5c31\u662f\u4f1a\u770b\u524d\u9762\u5df2\u7ecf\u6709\u7684\u4fe1\u606f, \u7136\u540e\u751f\u6210\u4e0b\u4e00\u4e2a\u5b57\u7b26, \u5b83\u4e5f\u6709\u7406\u89e3\u8bed\u5883\u7684\u80fd\u529b, \u53ea\u662f\u76f4\u63a5\u7528\u6765\u751f\u6210\u4e86. \u4ed6\u6ca1\u6709source and target\u7684\u6982\u5ff5 - \u6ce8\u91cd\u76f4\u63a5\u7528\u5df2\u6709\u8f93\u5165\u548c\u6a21\u578b\u751f\u6210\u9884\u6d4b\u540e\u9762\u7684\u8bcd: LM, text gen, dialogue systems, summarization, simplification, classification(\u6700\u540e\u4e00\u4e2a\u8f93\u51fa), Q&amp;A, NLI; GPT             - Encoder-Decoder               collapsed:: true - Transformer \u4e24\u4e2a\u90fd\u6709, \u9002\u7528\u4e8eseq2seq\u7684\u4efb\u52a1, \u6bd4\u5982\u8bf4 machine translation, encoder and decoder can have separate vocabularies and focus on different languages. - seq2seq\u573a\u666f, \u9700\u8981\u5bf9source\u8fdb\u884c\u7279\u6b8a\u7406\u89e3\u7684\u573a\u666f: translation, summarisation, Q&amp;A; Transformer, BART - \u4e5f\u53ef\u4ee5\u7528\u4e8eclassification, \u5728decoder\u7684\u6700\u540e\u8f93\u51fa\u540e\u9762\u63a5\u5206\u7c7b\u5c42 - \u5f53\u6211\u4eec\u589e\u52a0model dim\u7684\u65f6\u5019self-attention\u548cPWFF\u7684parameter\u7684\u589e\u957f\u90fd\u662fquadratic\u7684, \u56e0\u4e3aWqkv\u662fD-&gt;D/h, PWFF\u662fD-&gt;ff(4D)-&gt;D - the cross attention matrix shape of [T x S], T is my target sequence length and S is my input sequence length   collapsed:: true     - Q in T x D; K in S x D; V in S x D       QK^T = [T x D] x [D x S] = T x S &lt;---- This is the cross attention matrix that is being asked about       (QK^T)V = [T x S] x [S x D] = T x D &lt;---- This is the output of the cross attention formula             - CNN               collapsed:: true - perform well if the task involves key phrase recognition - Text classification, NER, Semantic role labelling             - RNN               collapsed:: true - d2l\u91cc\u9762\u8f93\u5165\u7684\u662fV\u5927\u5c0f\u4f5c\u4e3ainput, \u8f93\u51fa\u7684\u662fhidden state\u5927\u5c0f\u7684vector - \u6b63\u5e38\u8bad\u7ec3\u4e2d, \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4e8b\u5148\u8bad\u7ec3\u597d\u7684word embedding \u6765\u4f5c\u4e3a\u8f93\u5165 - \u53ef\u4ee5\u4f5c\u4e3a LM, \u53ef\u4ee5\u4f5c\u4e3aDecoder, \u53cc\u5411\u7684RNN\u9002\u5408\u4f5c\u4e3aEncoder - Sentiment analysis, text generation             - M7               collapsed:: true - POS: For many applications such as spam detection, sentiment analysis at scale (e.g. millions of emails going through a server) you can focus on nouns,verbs and adjectives removing redundant tokens. Another common application is Named Entity Recognition where we use POS tagging to only recognise nouns assuming named entities (people, organisations, places etc) are nouns in English.         - \u57fa\u7840           collapsed:: true             -              -              - For regression (predicting a score):               collapsed:: true - Output layer of size 1 - Linear activation for the output layer, so the value is not restricted - Use Mean Squared Error (MSE) as the loss             - For binary classification:               collapsed:: true - Output layer of size 1 - Use sigmoid to predict between two classes (0 and 1) - Use binary cross-entropy as the loss             - For multi-class classification (predicting one class out of many):               collapsed:: true - With k classes, have output layer of size k - Use softmax activation to get a probability distribution - Use categorical c             - For multi-label classification (possibly predicting many classes):               collapsed:: true - With k classes, have output of size k - Use sigmoid   activation, making each output is an independent binary classifier         - Word representations             - \u96be\u70b9 - Intuition: Represent words as vectors, so that similar ones have similar vectors - options:     - [[One-hot]] (1-of-V): V\u957f\u5ea6\u7684vector, \u6bcf\u4e2a\u8bcd\u5360\u7528\u4e00\u4e2abit\u76841, \u5176\u4ed6\u90fd\u662f0       collapsed:: true         - super sparse, all are orthogonal and equally distant     - [[WordNet]]: map words to broader concepts, \u732b-&gt;\u732b\u79d1\u54fa\u4e73       collapsed:: true         - rely on manually curated database         - miss rare and new meanings         - ambiguity mouse\u6709\u4e24\u4e2a\u542b\u4e49     - Distributed vectors: \u4e00\u5806\u5c5e\u6027\u7684\u7a0b\u5ea6\u6765\u5f62\u5bb9\u4e00\u4e2a\u8bcd\u8bed, furry, danger\u7684\u6253\u5206\u53ef\u4ee5\u533a\u5206\u732b\u548c\u8001\u864e       collapsed:: true         - elements representing properties shared btw words,         - \u4f7f\u7528cosine\u6765\u63cf\u8ff0\u76f8\u4f3c\u5ea6     - Count-based vectors: \u7528\u5176\u5468\u8fb9\u8bcd\u7684\u51fa\u73b0\u9891\u7387\u6765\u63cf\u8ff0\u8fd9\u4e2a\u8bcd       collapsed:: true         - vector\u8fd8\u662f\u5f88\u957f, \u4e14sparse, \u5f88\u591a\u90fd\u662f0         - \u8981\u89e3\u51b3\u90a3\u4e9b\u5230\u5904\u90fd\u51fa\u73b0\u7684\u8bcd\u7684\u95ee\u9898, \u4ed6\u4eec\u6ca1\u6709\u989d\u5916\u4fe1\u606f         - \u4f7f\u7528TF-IDF \u6765weight \u8fd9\u4e9bcontext\u8bcd Term Frequency-Inverse Document Frequency. d\u662ftarget word, w\u662fcontext word, D\u662fducument sentences         - TF: w\u548cd\u4e00\u8d77\u51fa\u73b0\u7684\u6b21\u6570/\u6240\u6709context word\u548cd\u4e00\u8d77\u51fa\u73b0\u6b21\u6570\u548c         - IDF: log(|D|/\u6709w\u7684\u53e5\u5b50\u4e2a\u6570)         - weight = TFIDF     - [[Word Embeddings]]         - embedding the words into a real-valued low-dimensional space; short vectors and dense     - [[Word2vec]] as word embeddings         - usually lowercase as pre-processing (tokenise)         - Continuous Bag-of-Words ([[CBOW]])         - [[Skip-gram]] - Continuous Bag-of-Words ([[CBOW]])   collapsed:: true     - Predict the target word wt based on the surrounding context words     -      - \u6709\u4e00\u4e2aembedding layer\u548c\u4e00\u4e2aoutput layer, context words\u4eec\u7ecf\u8fc7embedding layer\u5f97\u5230\u4ed6\u4eec\u7684embeddings\u7136\u540e\u52a0\u8d77\u6765\u7136\u540efed into output layer\u751f\u6210V\u5927\u5c0f\u7684\u5411\u91cf\u7ed9softmax, \u7ed9\u5b9atarget\u7684one hot\u5c31\u53ef\u4ee5\u548csoftmax\u7684\u7ed3\u679c\u8ba1\u7b97categorical [[cross-entropy]]\u4e86 - [[Skip-gram]]   collapsed:: true     - Predict the context words based on the target word wt     -      - \u4e5f\u6709\u4e24\u4e2aweight matrices, \u4e24\u4e2a\u90fd\u53ef\u4ee5\u7528\u4f5cembedding. \u4e0eCBOW\u4e0d\u540c\u7684\u662f, skip-gram\u8f93\u5165\u4e86target, \u8981\u9884\u6d4bcontext. \u6839\u636econtext\u7a97\u53e3\u7684\u5927\u5c0f, \u4f1a\u6709\u4e0d\u540c\u4e2a\u6570\u7684context \u8bcd. \u4f8b\u5982\u5982\u679c\u6709\u56db\u4e2acontext\u8bcd\u7684\u8bdd, \u5c31\u4f1a\u4e0e\u8fd9\u4e2atarget\u5f62\u6210\u56db\u4e2atraining samples, \u66f4\u65b0\u7f51\u7edc\u56db\u6b21. \u540c\u6837\u4e5f\u662f\u5bf9\u8f93\u51fa\u7684V\u8fdb\u884csoftmax, \u7136\u540e\u4e0e\u6b63\u786e\u7684context\u8ba1\u7b97categorical cross-entropy. \u5b9e\u9645\u8bad\u7ec3\u7684loss\u662f\u540c\u65f6\u4e00\u4e2abatch\u6709\u597d\u51e0\u53e5\u8bdd, \u6bcf\u53e5\u8bdd\u90fd\u9009\u62e9\u540c\u4e00\u4e2a\u4f4d\u7f6e\u7684\u628a\u8fd9\u56db\u4e2acontext\u7684log likelihood\u52a0\u8d77\u6765, \u5e0c\u671b\u4ed6\u53d8\u5927     - Intuition \u662foptimise target\u7684embedding\u548ccontext\u7ecf\u8fc7w\u2018\u7684embedding\u7684\u76f8\u4f3c\u5ea6     - Downside \u662f\u6bcf\u6b21\u7b97\u4e00\u4e2acontext\u7684\u6982\u7387\u90fd\u9700\u8981\u5bf9\u6574\u4e2aV\u8fdb\u884csoftmax, \u4f8b\u5982300dim\u7684embedding\u548c1w\u7684V, \u56e0\u4e3a\u6709\u4e24\u4e2aweights, \u5c31\u9700\u8981\u505a5M\u7684\u8ba1\u7b97     - \u89e3\u51b3\u65b9\u6cd5:       collapsed:: true         - lookup, \u76f4\u63a5index\u5230\u9700\u8981\u7684embedding, \u800c\u4e0d\u662f\u505a\u77e9\u9635\u4e58\u6cd5         - negative sampling: logistic regression for the real context word and k other noise words \u5373\u6700\u540e\u7684\u8f93\u51fa\u7ed3\u679c\u627e\u51fa\u90a31+k\u4e2a\u8bcd\u7684\u4f4d\u7f6e\u8fdb\u884csigmoid, \u4ec5\u5bf9\u4ed6\u4eec\u51e0\u4e2a\u8fdb\u884cbp, \u4ece\u800c\u53ea\u66f4\u65b0\u8fd9\u51e0\u4e2a\u8bcd\u7684embedding. noise\u8bcd\u7684\u9009\u62e9\u53ef\u4ee5\u662frandom\u7684\u4e5f\u53ef\u4ee5\u662f\u6839\u636e\u9891\u7387\u6765. \u5927\u6a21\u578b\u9700\u8981\u7684negative \u5c11\u4e00\u4e9b, \u5c0f\u6a21\u578b\u591a\u4e00\u4e9b         - \u5047\u59821\u4e2acontext\u914d5\u4e2anegative, \u6bcf\u6b21\u53ea\u9700\u8981\u66f4\u65b06\u4e2a embedding, \u5728output layer\u53ea\u9700\u8981\u66f4\u65b06300\u4e2aparameters, \u800c\u4e0d\u662f3M\u4e86, \u5f53\u7136embedding layer\u8fd8\u662f\u4e00\u6837\u7684 - \u4ecd\u7136\u9057\u7559\u7684\u95ee\u9898   collapsed:: true     - \u7f55\u89c1\u8bcd\u548c\u672a\u89c1\u8fc7\u7684\u8bcd     - morphological similarity \u5355\u590d\u6570(\u56e0\u4e3a\u6211\u4eec\u7528\u4e86\u4e00\u4e9bpre-processing     - \u8bcd\u8bed\u7684\u8bed\u5883\u610f\u4e49\u65e0\u6cd5\u5206\u8fa8     - Contextualised word embeddings \u53ef\u4ee5\u89e3\u51b3, \u8fd9\u88ab\u73b0\u4ee3\u7684\u8bed\u8a00\u6a21\u578b\u6240\u4f7f\u7528 - Byte Pair Encoding([[BPE]])   collapsed:: true     - Instead of manually specifying rules for lemmatisation or stemming, let\u2019s learn from data which character sequences occur together frequently     - BPE\u9996\u5148\u4f1a\u5f97\u5230base vocabulary \u5373\u6240\u6709\u5355\u4e2a\u7684\u5b57\u7b26, \u7136\u540e\u901a\u8fc7\u5728\u4e00\u8f6e\u8f6e\u8bad\u7ec3\u4e2d\u5f97\u5230merge\u6700\u5e38\u89c1\u7684\u7b26\u53f7pair\u4eec\u7684rule\u6765\u5f97\u5230\u5355\u8bcd\u4eec\u7684\u5171\u540c\u7684root\u4e4b\u7c7b\u7684\u5143\u7d20\u4f5c\u4e3a\u65b0\u7684\u8bcd\u6c47, \u76f4\u5230\u5f97\u5230\u60f3\u8981\u7684\u8bcd\u6c47\u91cf\u4e3a\u6b62, \u56e0\u800c\u53ef\u4ee5\u5904\u7406\u4e00\u4e9b\u4e0d\u8ba4\u8bc6\u7684\u5355\u8bcd, \u56e0\u4e3a\u6700\u5dee\u7684\u60c5\u51b5\u5c31\u662f\u628a\u4e00\u4e2a\u8bcd\u5206\u89e3\u6210\u4e86\u4e00\u4e2a\u4e2a\u5b57\u7b26.     - byte-level (\u53ef\u4ee5\u5904\u7406\u6240\u6709\u7684\u5b57\u7b26)\u7684 BPE\u88ab\u5e7f\u6cdb\u8fd0\u7528\u5728\u4e86[[GPT]]\u8fd8\u6709[[BERT]]\u6a21\u578b\u4e2d\u4f5c\u4e3atokeniser, \u7528\u4e8e\u628a\u8f93\u5165\u7684\u8bcd\u8bed\u5206\u89e3\u5230char\u4ee5\u540emerge\u6210\u8bcd\u6c47\u8868\u4e2d\u7684\u7684\u8bcd\u6c47, \u4f8b\u5982 lowest_\u88ab\u5206\u89e3\u6210\u4e00\u4e2a\u4e2a\u5b57\u6bcd\u540e\u4f1aapply\u8bad\u7ec3\u5f97\u5230\u7684merge rule merge\u6210low\u548cest_ \u4e24\u4e2a\u8bcd\u8bed (\u7528\u6765\u6807\u6ce8\u8fd9\u662f\u8bcd\u8bed\u7684\u7ed3\u5c3e, \u53ef\u4ee5\u7528\u6765\u91cd\u65b0\u7ec4\u5408), \u8fd9\u6837\u5b50\u5c31\u53ef\u4ee5\u88ab\u7f51\u7edc\u8fdb\u884c\u5904\u7406\u4e86 (embedding\u554a\u4ec0\u4e48\u7684) - [[Wordpieces]]   collapsed:: true     - \u4e0eBPE\u76f8\u4f3c, \u4f46\u662f\u9664\u4e86\u6bcf\u4e2a\u8bcd\u7684\u5f00\u5934char\u4ee5\u5916\u90fd\u4f1a\u5728\u524d\u9762##, \u9009\u62e9mergerule\u4e0d\u4e00\u6837, merge\u6210\u6700\u957f\u7684subword - [[BPE]]\u7684\u4f18\u52bf   collapsed:: true     - The advantage of BPE in NLP is that it can effectively handle out-of-vocabulary words and rare words by breaking them down into smaller subword units that are more likely to be in the vocabulary. This can improve the performance of NLP models that rely on word embeddings, such as neural machine translation, sentiment analysis, and named entity recognition.             - \u7591\u70b9 - skip-gram\u7684loss\u548c\u8bad\u7ec3\u65b9\u5f0f\u5230\u5e95\u662f\u600e\u4e48\u6837\u7684   collapsed:: true     - \u6682\u65f6\u89c9\u5f97\u5c31\u662f         - Model architectures           collapsed:: true             - \u96be\u70b9             - \u7591\u70b9         - ---         - Classification           collapsed:: true             - \u96be\u70b9               collapsed:: true - common NLP classification tasks   collapsed:: true     - 1. Hate speech detection       2. Sentiment analysis       3. Fact verification       4. Spam detection       5. Error detection       6. Natural Language Inference (NLI) - [[BoW]] (Bag of words) \u6765\u8868\u793a\u4e00\u53e5\u8bdd   collapsed:: true     - \u7528\u67d0\u4e9b\u8bcd\u8bed\u7684\u8bcd\u9891\u6765\u8868\u793a\u4e00\u53e5\u8bdd, \u6bcf\u4e2a\u8bcd\u8bed\u53ef\u4ee5\u770b\u4f5c\u662f\u4e00\u4e2afeature, \u53ef\u4ee5\u53ea\u9009\u4e00\u4e9b\u6709\u610f\u4e49\u7684\u8bcd\u8bed, \u4f8b\u5982\u8bf4 good bad     - Suppose we have a collection of three documents:     - Document 1: \"The cat in the hat.\"       Document 2: \"The dog chased the cat.\"       Document 3: \"The cat ran away from the dog.\"     - To apply the BoW model to this collection, we first create a vocabulary of all the unique words in the documents:     - Vocabulary: the, cat, in, hat, dog, chased, ran, away, from     - Next, we represent each document as a vector of word counts, where the element at index i represents the count of the ith word in the vocabulary in that document. For example, the BoW representation of Document 1 is:     - Document 1 BoW vector: [2, 1, 1, 1, 0, 0, 0, 0, 0]     - This means that Document 1 contains 2 occurrences of the word \"the,\" 1 occurrence of the words \"cat,\" \"in,\" and \"hat,\" and 0 occurrences of all the other words in the vocabulary.     - Similarly, the BoW vectors for Document 2 and Document 3 are:     - Document 2 BoW vector: [1, 1, 0, 0, 1, 1, 0, 0, 0]       Document 3 BoW vector: [1, 1, 0, 0, 1, 0, 1, 1, 1]     - We can now use these BoW vectors to perform various tasks, such as document classification or information retrieval, by comparing the similarity between vectors using measures such as cosine similarity or Euclidean distance. - [[Naive Bayes Classifier]]   collapsed:: true     -      -      - \u627e\u5230\u6700\u7b26\u5408\u6570\u636ex\u7684label y, \u4e5f\u7b49\u4e8e\u7ed9\u5b9ay\u51fa\u73b0x\u7684\u6982\u7387 \u4e58\u4e0ay\u672c\u8eab\u8fd9\u4e2alabel\u51fa\u73b0\u7684\u6982\u7387, \u5176\u4e2dP(x|y)\u6709\u4e2aindependence\u5047\u8bbe, \u53ef\u4ee5\u662f\u6240\u6709feature\u7684conditional prob\u7684\u4e58\u79ef     -      - \u5148\u9a8cp(y)=3/5 \u548c 2/5 \u5206\u522b\u662f\u6b63\u548c\u8d1f; P(good | +\uff09= 2/4 \u56db\u4e2a\u5212\u5206\u4e3a\u6b63\u4f8b\u7684feature\u8bcd\u91cc\u6709\u4e24\u4e2a\u662fgood     - Add-one smoothing: \u89e3\u51b3probability \u4e3a\u96f6\u7684\u95ee\u9898       collapsed:: true         -          - P(good | +\uff09= 2+1 / 4+3 = 3/7; 3\u662f\u6709\u4e09\u79cd\u4e0d\u540c\u7684\u8bcd     - \u5047\u8bbe\u6d4b\u8bd5\u4f8b\u4e2d\u8fd9\u4e09\u4e2a\u8bcd\u90fd\u51fa\u73b0\u4e86\u4e00\u6b21, \u6700\u540e\u4e3a\u6b63\u7684\u6982\u7387\u5c31\u662f3/5 * \u4e09\u4e2a\u8bcdcondition+\u7684\u6982\u7387\u79ef     - improvement       collapsed:: true         - 1. \u53e5\u5b50\u4e2d\u91cd\u590d\u51fa\u73b0\u67d0\u4e2a\u7279\u5f81\u8bcd, \u4e5f\u4e0d\u4f5c\u91cd\u590d\u5904\u7406, \u5f53\u4f5c\u662f\u4e00\u4e2a (Binary Naive Bayes, \u5373\u4fbf\u6709\u4e24\u4e2amovie, \u4e5f\u53ea\u4e58\u4e00\u904d\u5b83\u7684\u6982\u7387)         - 2. append \u2018NOT\u2019 after any logical negation until next punct. \u4f7f\u5f97\u5426\u5b9a\u7684\u542b\u4e49\u66f4\u6e05\u6670, \u4e0d\u7136\u6734\u7d20\u8d1d\u53f6\u65af\u53ea\u6839\u636e\u8bcd\u8bed\u672c\u8eab\u6765\u5224\u65ad, \u65e0\u6cd5\u770b\u5230\u524d\u9762\u7684\u5426\u5b9a\u542b\u4e49, \u4e5f\u65e0\u6cd5\u5f97\u5230context\u548cdependency     - \u95ee\u9898: context, new words - [[Logistic Regression]]   collapsed:: true     - \u5b66\u4e60\u7ed9\u6bcf\u4e2a\u8bcd\u8bed\u7684weight (How important an input feature is to the classification decision) \u548cbias, \u7ecf\u8fc7logistic function(sigmoid)\u6765\u8fdb\u884c\u5206\u7c7b, \u901a\u8fc7BCE loss\u6765\u5f97\u5230loss, \u901a\u8fc7gd\u66f4\u65b0\u53c2\u6570     - \u5982\u679c\u662f\u591a\u4e2a\u7c7b\u7684\u8bdd, \u53ef\u4ee5\u7528fc map\u5230\u591a\u4e2a\u7c7b, \u76f8\u5f53\u4e8e\u7ed9\u6bcf\u4e2a\u7c7b\u4e00\u6761weight, \u6700\u540e\u5f97\u5230\u7684\u7ed3\u679c\u8fdb\u884csoftmax \u8fd9\u4e2a\u53eb\u505amultinomial logistic regression - \u4e24\u79cdbaseline\u5206\u7c7b\u65b9\u6cd5\u7684\u91cd\u8981\u7ed3\u8bba   collapsed:: true     - LR: better dealing with correlated features, larger datasets     - NB: faster, good on small datasets     - \u5e2e\u52a9\u6211\u4eec\u77e5\u9053\u54ea\u4e9bfeature\u662f\u6709\u7528\u7684, \u5982\u4f55\u548cclass\u8054\u7cfb\u7684, \u5e2e\u52a9\u6211\u4eec\u7406\u89e3dataset, \u53ef\u4ee5\u4e0e\u5927\u6a21\u578b\u6bd4\u8f83\u6765\u7406\u89e3\u8fd9\u4e2a\u4efb\u52a1\u7684nature - \u4e3a\u4ec0\u4e48\u6211\u4eec\u9700\u8981\u66f4\u597d\u7684\u7406\u89e3\u6211\u4eec\u7684\u6570\u636e\u5462?   collapsed:: true     - To select and engineer features     - To identify patterns and relationships. This can help us to understand the underlying structure of our data and make more informed decisions about how to model and analyze it     - Preprocessing optimization: By understanding the data, we can identify common preprocessing steps and wisely adjust them     - Identification of data quality issues - Neural Networks ([[NN]]s)   collapsed:: true     - 2\u4e2a\u975e\u5e38\u975e\u5e38naive\u7684\u8868\u8fbe\u53e5\u5b50\u7684\u65b9\u5f0f       collapsed:: true         - \u6bcf\u4e2a\u8bcd\u90fd\u6709\u6bd4\u5982\u8bf43\u7ef4\u7684\u8868\u8fbe\u4e86, \u53e5\u5b50\u7684\u8868\u793a\u5c31\u7528\u6240\u6709\u8bcd\u7684\u6bcf\u4e00\u4e2a\u7ef4\u5ea6\u7684avg\u6765\u8868\u793a, \u5f62\u6210\u4e00\u4e2a\u4e09\u7ef4\u7684\u53e5\u8868\u8fbe, \u8868\u73b0\u8fd8\u53ef\u4ee5         - \u8fd8\u6709\u4e00\u79cd\u56fa\u5b9a\u4e0b\u6765\u53e5\u957f, \u7528\u6240\u6709\u7684\u8bcd\u5411\u91cf\u7684concat\u8868\u8fbe, \u8fd9\u6837\u5b50\u9996\u5148\u957f\u5ea6fix\u4e86, \u5176\u6b21\u8bcd\u8bed\u4f4d\u7f6e\u56fa\u5b9a\u4e86, \u975e\u5e38\u4e0d\u597d - Recurrent neural networks ([[RNN]]S)   collapsed:: true     - Usually the last hidden state is the input to the output layer     - \\(h_{t+1} = f(h_t,x_t) = tanh(Wh_t + Ux_t)\\), \\(y_t = W_{hy}h_t + B_y\\)\u8003\u8fc7\u7684\u516c\u5f0f     - \\(W \\in \\mathbb{R}^{H\\times H}, U\\in \\mathbb{R}^{E\\times H}\\)     -      - Vanishing gradient problem as tanh's derivatives are between 0 and 1 - [[CNN]]s   collapsed:: true     - Filter: width\u662fembedding dims, height\u662fwindow\u5927\u5c0f, \u5373\u4e00\u6b21\u8003\u8651\u51e0\u4e2a\u8bcd, \u901a\u5e382-5(bigram to 5-gram)     - stride: \u6bcf\u6b21\u79fb\u52a8filter\u7684\u591a\u5c11     -      - maxpooling\u5c31\u662f\u540c\u6837\u7528window\u5bf9\u8fd9\u5f97\u5230\u7684\u7ed3\u679c\u8fdb\u884c\u6c47\u805a, \u53e6\u5916\u6709\u591a\u5c11\u4e2afilters\u5c31\u6709\u591a\u5c11\u8fd9\u6837\u7684\u4e1c\u4e1c - RNNs vs CNNs   collapsed:: true     - RNNs perform better when you need to understand longer range dependencies, RNNs are typically suited for tasks that require understanding the temporal dependencies between the input features. well-suited for tasks involving variable-length sequences.       collapsed:: true         - Language modeling: predicting the probability distribution of the next word in a sequence given the previous words         - Speech recognition: recognizing spoken words or phrases from an audio signal         - Machine translation: translating a sentence from one language to another         - Named entity recognition: identifying and classifying named entities in text         - Sentiment analysis: classifying the sentiment of a text as positive, negative, or neutral         - Text generation: generating new text based on a given input or a specific style or topic     - CNNs can perform well if the task involves key phrase recognition       collapsed:: true       CNNs are typically suited for tasks that involve extracting local features from the input, such as identifying important n-grams or combinations of words that are indicative of a certain category or relationship. They are good at processing fixed-length inputs, making them well-suited for tasks that involve text classification or semantic role labeling.         - Text classification: classifying text into one or more predefined categories         - Semantic role labeling: identifying the semantic relationships between words in a sentence         - Question answering: answering natural language questions based on a given context or passage         - Named entity recognition: identifying and classifying named entities in text         - Relation extraction: identifying the relationships between entities in text         - \u4e3e\u4f8b: Determining if an article is Fake News based on the headline           collapsed:: true             - CNNs are well-suited for this type of classification task because they can capture local features or patterns within the text that are important for determining the category. In the case of a headline, local features could include specific words or combinations of words that are indicative of fake news, such as \"shocking new evidence\" or \"breaking news\". These features can be detected by the convolutional layers of the CNN, which scan the input text with a sliding window to extract local features. - [[De-biasing]]   collapsed:: true     - preventing a model learning from shallow heuristics     - \u6709\u4e9bfeature\u5f88\u6d45\u5f88\u660e\u663e\u5e26\u6709\u504f\u89c1\u5f88\u5bb9\u6613\u88ab\u9519\u8bef\u5229\u7528, \u6211\u4eec\u4e0d\u5e0c\u671b\u8fd9\u4e9b\u6d45\u5c42\u7684\u4fe1\u606f\u88abmodel\u6293\u5230\u800c\u4e0d\u53bb\u63a2\u7d22\u6df1\u5c42\u6b21\u7684\u4e1c\u897f, \u6bd4\u5982\u8bf4sentence length, \u4e00\u5207\u963b\u6b62\u6211\u4eecgeneralise model\u7684\u4e1c\u897f     - Possible strategies       collapsed:: true         - Augment with more data to balance bias         - filter data         - make model predict different from predictions based on the bias         - prevent model finding the bias     - bi is the prob of each class given the bias; pi is the prob from our model     - [[Product of Experts]]       collapsed:: true         - \u8bad\u7ec3\u7684\u65f6\u5019, \u6211\u4eec\u8ba9\u6211\u4eec\u7684\u6a21\u578b\u53bb\u5b66\u4e60bias\u4e4b\u5916\u7684\u4e1c\u897f, \u901a\u8fc7\u589e\u52a0bias\u7684\u7c7b\u7684prob, \u53ef\u4ee5\u9f13\u52b1\u81ea\u5df1\u7684\u6a21\u578blearn everything around that bias         - \\(\\hat{p}_i = softmax(log(p_i)+log(b_i))\\)         - \u8fd9\u4e2abias\u7684prob\u53ef\u4ee5\u7531\u6bd4\u5982: \u6839\u636e\u67d0\u4e2abias\u7684feature\u6765\u5f97\u5230, \u7528\u4e00\u4e2a\u7b80\u5355\u7684\u6a21\u578b\u5b66\u4e60, \u4ec5\u7528\u4e00\u4e2asample\u4e2d\u7684\u90e8\u5206\u6570\u636e, \u51cf\u5c11\u6570\u636e\u96c6\u4e2dsample\u6570\u91cf     - Weight the loss of examples based on the performance of our biased model       collapsed:: true         - \u7528 1-bi\u7ed9\u6b63\u786e\u7c7b\u7684\u9884\u6d4bprob \u6765weight loss; \u5982\u679cbias\u5bf9\u8fd9\u4e2a\u7ed3\u679c\u975e\u5e38\u81ea\u4fe1, 1-1=0\u4f1a\u5927\u5927\u51cf\u5c11\u7531\u8fd9\u79cd\u7b80\u5355\u7684sample\u5e26\u6765\u7684\u6743\u91cd\u66f4\u65b0; \u8fd9\u4f1a\u9f13\u52b1\u6a21\u578b\u66f4\u591a\u7684\u53bb\u770b\u90a3\u4e9bbias model \u89c9\u5f97\u82e6\u96be\u7684, \u53d1\u73b0\u4e0d\u4e86\u7684\u6df1\u5c42\u6b21\u7684feature. give more weight to examples that are more difficult for the biased model to predict accurately, with the aim of reducing the impact of any biases present in the original model.     - \u9002\u7528\u573a\u666f: out-of-distribution test set, generalised for \u66f4\u591a\u5206\u5e03\u5916\u6570\u636e     - \u4e0d\u9002\u5408: geder bias, \u76f4\u63a5hide gender info\u5c31\u597d - Micro averaged F1 = acc             - \u7591\u70b9         -         - [[Language Modeling]]           collapsed:: true             - \u96be\u70b9               collapsed:: true - \u4ec0\u4e48\u662f[[LM]]   collapsed:: true     - Language modeling involves assigning probabilities to sequences of words.     - Predicting the next word in a sequence of words     - Predicting a masked word in a sentence     - \u7b80\u800c\u8a00\u4e4b\u5c31\u662f\u9884\u6d4b\u548c\u751f\u6210\u8bcd, \u5176\u672c\u8eab\u4e5f\u80fd\u591f\u8ba1\u7b97\u4e00\u4e2a\u53e5\u5b50\u5b58\u5728\u7684\u53ef\u80fd\u6027 - [[N-gram]]   collapsed:: true     - \u662f\u6700\u57fa\u7840\u7684LM, \u901a\u8fc7\u8bad\u7ec3\u96c6\u4e2d\u7684\u7edf\u8ba1\u6570\u636e, \u6765\u8fdb\u884c\u751f\u6210, \u9009\u62e9\u6700\u6709\u53ef\u80fd\u7684(\u6216\u8005\u6309\u6982\u7387)\u7684\u4e0b\u4e00\u4e2a\u8bcd\u8bed, \u53ef\u4ee5\u7528CE, perplexity\u6765\u8861\u91cf\u5176\u597d\u574f, \u5373\u5728\u6d4b\u8bd5\u96c6\u4e2d\u9002\u5e94\u7a0b\u5ea6. \u7531\u4e8en\u7684\u9650\u5236, \u6ca1\u6cd5\u627e\u5230\u957f\u8ddd\u79bb\u7684dependency. (NN LM\u4eec\u53ef\u4ee5\u89e3\u51b3)     - P(w|h), w\u5728given \u524d\u9762\u7684\u8bcd\u8bed\u4eech, \u5373history\u7684\u6982\u7387, n-gram\u5c31\u662f\u53ea\u8003\u8651w\u548ch\u4e00\u5171n\u4e2a\u8bcd, \u8fd9\u4e2a\u6982\u7387\u53ef\u4ee5\u7531\u7edf\u8ba1\u5f97\u5230, \u5373abc\u4e00\u8d77\u51fa\u73b0\u7684\u6570\u91cf\u9664\u4ee5ab\u4e00\u8d77\u51fa\u73b0\u7684\u6570\u91cf     - \\(P(w_n | w_1^{n-1}) \\approx P(w_n | w_{n-N+1}^{n-1})\\) (assume they are likely) - \u6574\u53e5\u8bdd\u7684prob\u53ef\u4ee5\u5199\u4f5c\u6240\u6709n-gram\u7684\u6982\u7387\u4e58\u79ef, \u5728log space\u53ef\u4ee5\u8868\u8fbe\u6210\u52a0\u6cd5   collapsed:: true     -  - [[Perplexity]] \u56f0\u60d1\u5ea6   collapsed:: true     - It\u2019s the inverse probability of a text, normalized by the # of words, measure of the surprise in a LM when seeing new text \u60ca\u559c\u7a0b\u5ea6\u8d8a\u4f4e, \u8d8a\u719f\u6089. \u6d4b\u8bd5\u96c6\u4e2d\u8d8a\u4f4e\u8d8a\u597d. \u662f\u4e2aintrinsic evaluation     - \u6700\u5c0f\u4e3a1, \u5982\u679c\u5b8c\u5168\u968f\u673a\u7684\u8bdd\u6700\u5927\u5c31\u662f|V|     -        id:: 640a81b3-9d4c-4701-a2bd-942addba3574 - Cross Entropy   collapsed:: true     - \u95ee\u9898\u5728\u4e8e, \u6211\u4eec\u6ca1\u529e\u6cd5\u77e5\u9053\u771f\u5b9e\u7684\u5e94\u8be5\u7684prob, \u4e0b\u9762\u8fd9\u4e2a\u5c31\u662f\u4e00\u53e5\u8bdd\u7684CE, normalised by N, \u5373\u8bcd\u8bed\u6570\u91cf     -      - \u771f\u5b9e\u5904\u7406\u8fc7\u7a0b\u4e2d \u662f\u6709\u4e2awindow size\u7684, context\u53ea\u4f1a\u53d6\u4e00\u90e8\u5206, \u8fd8\u53ef\u4ee5\u51b3\u5b9a\u6709\u6ca1\u6709strided sliding window       collapsed:: true         -  - Converting Cross Entropy Loss to Perplexity   collapsed:: true     - \\(Perplexity(M) = e^H\\) - \u867d\u7136LM\u662f\u751f\u6210, \u4f46\u662f\u4e5f\u53ef\u4ee5\u5e2e\u52a9\u5206\u7c7b, \u4f8b\u5982\u8bf4\u4e8b\u5148\u6982\u62ec\u6587\u672c, \u7ea0\u6b63\u9519\u8bef, \u751a\u81f3\u76f4\u63a5\u8ba9\u4ed6\u8fdb\u884c\u5206\u7c7b. GPT\u4e4b\u7c7b\u7684\u6a21\u578b\u8003\u8651\u7684\u53ef\u4e0d\u662f\u5355\u7eaf\u6982\u7387, \u8fd8\u6709\u8bed\u5883\u610f\u4e49\u7b49\u7b49. \u4e5f\u5c31\u53ef\u4ee5\u7528\u8ba9\u4ed6\u4eecperform classification, \u505a\u9009\u62e9\u9898, NLI\u4e4b\u7c7b\u7684\u4efb\u52a1\u6765\u8bc4\u4f30 - \u5982\u4f55\u89e3\u51b3sparsity\u7684\u95ee\u9898   collapsed:: true     - Use Neural-based language models     - Add-1 Smoothing       collapsed:: true         - \u5bf9\u4e8e\u90a3\u4e9bsparse statistics\u7684\u8bcd, steal probability mass from more frequently words; \u53ef\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd, \u4f46\u662f\u4f1a\u4ece\u9ad8prob\u5904\u5077\u8d70\u5f88\u591a, \u7ed9unseen\u7684\u592a\u591a\u4e86         -      - Back-off       collapsed:: true         - back-off and see how many occurrences there are of \u9000\u4e00\u6b65\u770b\u5c11\u4e00\u4e2a\u8bcd\u7684\u8be5\u7ec4\u5408, \u5982\u679c\u518d\u6ca1\u6709, \u518d\u5c11\u770b\u4e00\u4e2a\u8bcd, \u76f4\u5230\u53ea\u5269\u4e0b\u4e00\u4e2a\u81ea\u5df1         -      - Interpolation       collapsed:: true         - \u63d2\u503c\u6cd5, \u7ed9n-gram\u53d8\u6210 \u51e0\u4e2a\u6bd4n\u7b49\u548c\u5c0f\u7684n-gram\u7684\u52a0\u6743\u7ec4\u5408         -              - \u7591\u70b9         - ---         - Neural language models           collapsed:: true             - \u96be\u70b9               collapsed:: true - \u4f18\u52bf\u548c\u8fdb\u6b65   collapsed:: true     - \u907f\u514d\u4e86n-gram\u7684sparsity\u95ee\u9898     - \u80fd\u591fcontextual word representations - Feed-forward LM ([[FFLM]])   collapsed:: true     -      - 4-gram\u7684\u8bdd\u5c31\u662f3\u4e2acontext words\u7ecf\u8fc7embedding, concat\u8d77\u6765\u4f5c\u4e3acontext\u7528tanh\u6fc0\u6d3b\u540e\u7528\u4e00\u4e2aFC\u6765\u8fdb\u884c\u5206\u7c7b\u9884\u6d4b\u7b2c\u56db\u4e2a\u8bcd. \u6548\u679c\u6bd4smoothed 3-gram LM\u597d, \u4f46\u662f\u8fd8\u5f97\u770bRNN - [[RNN]]s for language modeling   collapsed:: true     - \\(h_{t+1} = f(h_t,x_t) = tanh(Wh_t + Ux_t)\\),  \\(y_t = W_{hy}h_t + B_y\\)     - \\(W \\in \\mathbb{R}^{H\\times H}, U\\in \\mathbb{R}^{E\\times H}\\)     - ht\u662f\u6700\u540e\u4e00\u4e2ahidden state, \u53ef\u4ee5\u4f5c\u4e3a\u6574\u53e5\u8bdd\u7684context, \u8f93\u51fa\u7ed9\u4e00\u4e2a\u8f93\u51fa\u5c42, \u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1     - Vanishing gradient       collapsed:: true         - Activation functions: sigmoid and tanh cause the gradients to become very small as they are backpropagated through time         - Time step size: too large, gradient becomes unstable         - Initialization of the weights         - \u53ef\u4ee5\u7528 gradient clipping \u5e2e\u52a9\u628agradient\u9650\u5236\u5728\u4e00\u4e2a\u56fa\u5b9a\u533a\u95f4\u5185, \u800c\u4e0d\u6539\u53d8\u65b9\u5411     - Many-to-many       collapsed:: true         - \u6bcf\u4e2a\u65f6\u95f4t\u90fd\u4f1a\u7ed9\u51fa\u4e00\u4e2aoutput\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd, \u4e0e\u4e0b\u4e2a\u65f6\u95f4\u7684\u6b63\u786e\u8bcd\u8fdb\u884cCE     - Teacher forcing       collapsed:: true         - \u5728\u8bad\u7ec3\u7684\u65f6\u5019\u4e0d\u7528\u4e0a\u4e00\u4e2a\u65f6\u523b\u7684\u9884\u6d4b\u8bcd\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u65f6\u523b\u7684\u8f93\u5165, \u800c\u662f\u4f7f\u7528\u771f\u5b9e\u7684\u6807\u7b7e\u4f5c\u4e3a\u6307\u5bfc\u8f93\u5165, \u8ba9\u8bad\u7ec3\u66f4\u52a0\u7a33\u5b9a     - Weight tying \u5982\u4f55\u51cf\u5c11weights       collapsed:: true         - \u628ahidden state\u770b\u4f5c\u662fembedding, \u7528\u540c\u4e00\u4e2aembedding matrix\u6765map\u56deV; \u56e0\u6b64\u4f7f\u7528\u7684embedding dim\u548chidden state dim\u76f8\u540c, \u4e0b\u56fe\u4e2d\u7684U \u4e3a H x H, E\u4e3aH x V         -  - [[Bi-directional RNN]]s   collapsed:: true     - \u5982\u4f55\u7528\u4e8elanguage modeling: The hidden states from both directions are then concatenated to obtain a final hidden state, which is used to make the prediction. \u7528\u53cc\u5411\u4fe1\u606f\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd.     - \u5e94\u7528\u573a\u666f: \u7531\u4e8e\u83b7\u5f97\u4e86\u53cc\u5411\u4fe1\u606f, \u53ef\u4ee5detect grammatical errors, \u4e5f\u53ef\u4ee5\u5c06\u4e24\u4e2a\u65b9\u5411\u7684\u6700\u7ec8hidden state\u62fc\u63a5\u8d77\u6765, \u4f5c\u4e3a\u6574\u53e5\u8bdd\u7684hidden state (\u957f\u5ea6\u4e3a2H)     - \u4f18\u52bf: capture both past and future context of a sequence, can better handle noisy or missing information in the input sequence - [[Multi-layered RNN]]s   collapsed:: true     - each layer can learn to represent a different level of abstraction in the data. lower layers learn local dependencies, higher levels learn long-term dependencies - [[LSTM]]   collapsed:: true     -      -      - How do LSTMs help with Vanishing gradients       collapsed:: true         - The gradients through the cell states are hard to vanish           Additive formula means no repeated multiplication           forget gate allows model to learn when to preserve gradients - [[GRU]]s   collapsed:: true     - GRU is quicker to compute and has fewer parameters. no cell state, only history             - \u7591\u70b9         - Machine Translation           collapsed:: true             - \u96be\u70b9               collapsed:: true - Statistical Machine Translation   collapsed:: true     - \u6709\u591a\u4e2asub-models     - \u9700\u8981\u4e24\u95e8\u8bed\u8a00\u7684corpus, \u5bf9\u5e94\u7684\u53e5\u5b50\u5bf9\u4eec     - p(t|s) = p(s|t)p(t) - Pipeline of Statistical Machine Translation   collapsed:: true     - Alignment model       collapsed:: true         - \u5bfb\u627e\u76ee\u6807\u8bed\u8a00\u5e93\u4e2d, \u5bf9\u5e94\u7684phrase pairs     - Translation model       collapsed:: true         - \u5bf9\u5bf9\u5e94\u7684phrase pair\u8fdb\u884c\u6982\u7387\u8bc4\u4f30, \u6bcf\u4e2apair\u7ed9\u51fa\u53ef\u80fd\u6027, \u627e\u7684\u662fp(s|t) \u7ed9\u5b9a\u4e00\u4e2a\u53ef\u80fd\u7684target, source\u662f\u8fd9\u6837\u8fd9\u53ef\u80fd\u6027     - Language model       collapsed:: true         - \u5bf9\u7ed9\u51fa\u6765\u7684\u53e5\u5b50, \u8fdb\u884c\u76ee\u6807\u8bed\u8a00\u5185\u5b58\u5728\u53ef\u80fd\u6027\u8bc4\u4f30 \u4e5f\u5c31\u662f p(t) - Downsides of statistical MT   collapsed:: true     - Sentence alignment: \u591a\u79cd\u7ffb\u8bd1\u53ef\u80fd, \u4e5f\u53ef\u80fd\u88ab\u5939\u65ad     - Word alignment: \u4e0d\u4e00\u5b9a\u6709\u5bf9\u5e94\u7684\u8bcd\u8bed     - Statistical anomalies: \u7edf\u8ba1\u4e0a\u7684\u8fc7\u591a\u642d\u914d, \u53ef\u80fd\u4e3b\u5bfc\u7ffb\u8bd1     - \u4e60\u8bed, \u4e0d\u5b58\u5728\u7684\u8bcd == - Neural Machine Translation   collapsed:: true     - en-de used in seq2seq tasks (summarisation, Q&amp;A)     - X-&gt;encoder-&gt;latent encoding-&gt;decoder-&gt;Y;     - encoder \u662f\u4e2a\u7f16\u7801\u5668, decoder\u5c31\u662f\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b     - BiRNN \u901a\u5e38\u662f\u4e2a\u4e0d\u9519\u7684\u9009\u62e9, \u53ef\u4ee5\u8003\u8651\u5230\u5386\u53f2\u548c\u672a\u6765, \u5229\u7528\u4e0a\u53ef\u4ee5\u7528hidden states for each word or the concat of two last HS ([h_i; h\u2019_0])     - BiRNN\u8868\u793a\u6574\u53e5\u8bdd\u4e5f\u53ef\u4ee5\u5229\u7528[h_i; h\u2019_0] \u6216\u8005avg of \u6240\u6709\u8bcd\u4f4d, \u8f93\u51fa\u7ed9decoder RNN \u4f5c\u4e3ahidden state(\u4e0d\u662f0\u4e86), \u7136\u540e\u7ed9\u4e00\u4e2a[SOS]\u4f5c\u4e3a\u521d\u59cbinput\u5c31\u53ef\u4ee5\u5f00\u59cbauto-regressive\u4e86, \u8bad\u7ec3\u7684\u65f6\u5019\u540c\u6837\u53ef\u4ee5\u91c7\u7528teacher forcing (\u6309\u7167\u6bd4\u4f8b\u6765\u6bd4\u598250%)     - Teacher forcing       collapsed:: true         - Using incorrect predictions as input can cause accumulation of errors, can be hard to optimise, so teacher forcing         - \u4f46\u662f\u6709[[Exposure Bias]] \u95ee\u9898, \u6a21\u578b\u53ea\u4f1a\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u65f6\u7684\u6b63\u786e\u5f15\u5bfcinput, \u800c\u4e0d\u4f1a\u6839\u636e\u81ea\u5df1\u7684\u9884\u6d4b\u6765\u8fdb\u884c\u4e86     -      - BiRNN\u95ee\u9898\u548c\u7f3a\u70b9       collapsed:: true         - d\u7ef4\u65e0\u6cd5\u5f88\u597d\u4fdd\u5b58\u5386\u53f2\u4fe1\u606f, decoder\u4e2d\u4f1a\u52a0\u5165\u5230\u751f\u6210\u7684\u8bcd\u8bed\u7684\u4fe1\u606f, \u4e5f\u4f1a\u8ba9encoder\u6765\u7684context c\u6d88\u5931, \u4f9d\u7136\u6709vanishing gradient problem         - \u8fd9\u4e5f\u5f15\u51fa\u4e86attention, \u4e0d\u518d\u53ea\u4f9d\u9760c, \u800c\u662f\u80fd\u591f\u76f4\u63a5\u770b\u5230\u8f93\u5165\u7684\u6240\u6709\u4fe1\u606f     - Attention (additive MLP)       collapsed:: true         - \u5bf9\u6bcf\u4e00\u4e2adecoder step \u90fd\u4f1a\u7528\u4e0a\u4e00\u6b65\u7684state\u4e0e\u6240\u6709\u7684encoder hidden state \u8fdb\u884cattention, \u5f97\u5230\u6bcf\u4e2ahidden state\u7684\u6743\u91cd, \u52a0\u6743\u6c42\u548c\u5f97\u5230\u4e00\u4e2a\u8fd9\u4e00\u6b65\u7279\u6709\u7684context c         - \u6743\u91cd\u662f\u7531\u8fd9\u4e2aencoder\u7684ht\u4e0edecoder\u4e2d\u7684s\u7684\u76f8\u5173\u6027\u5b9a\u4e49\u7684, \u76f8\u5173\u6027\u7531\u4e24\u4e2a\u6743\u91cdmatrix\u4e0e\u4ed6\u4eec\u5206\u522b\u76f8\u4e58\u7684\u548c\u7684tanh\u5f97\u5230, v\u628ad\u7ef4\u7f29\u653e\u52301, \u53d8\u6210\u4e00\u4e2aenergy score. \u4e0b\u9762\u7684a\u4f1a\u5f97\u5230i\u4e2a\u503c, softmax\u5b8c\u4e86\u4ee5\u540e\u5c31\u662f\u6bcf\u4e2ahi\u7684weight\u4e86         -          - Decoder \u4f1a\u7528\u5230st-1, ct, yt-1, \u5373hidden state, context, input\u6765\u8ba1\u7b97\u65b0\u7684st, \u8ba1\u7b97y^t\u65f6, \u4f1a\u7528\u5230st\u548cct     - BLEU: \u6700\u53d7\u6b22\u8fce\u7684evaluation metric, modified precision (MP)       collapsed:: true         - MP = total unique overlap/total MT ngrams         - total unique overlap\u662f\u63d0\u70bc\u51fangram\u4ee5\u540e, \u6bcf\u4e2agram\u5728\u4e24\u4e2areference\u4e2d\u627ematch, \u54ea\u4e2amatch\u591a\u8fd9\u4e2angram\u7684count\u5c31\u7b97\u54ea\u4e2a\u6570         -          - BLEU\u5219\u662f1-n\u4e2agram\u7684MP\u7684\u4e58\u79ef\u7684Brevity penalty\u7ed3\u679c, \u60e9\u7f5a\u77ed\u53e5\u5b50, \u5e0c\u671b\u83b7\u5f97\u5dee\u4e0d\u591a\u957f\u5ea6\u7684\u53e5\u5b50         -      - \u5176\u4ed6\u7684\u7ffb\u8bd1\u4efb\u52a1metrics:       collapsed:: true         - Chr-F is an F beta -score based metric over character n-grams. This metric balances character precision and character recall         - TER (number of minimum edits) is performed at the word level, and the \u201cedits\u201d can be a: Shift, Insertion, Substitution and Deletion.         - ROUGE-L: F-Score of the longest common subsequence (LCS), /ref\u5c31\u662frecall, hyp\u5c31\u662fprecision         - METEOR: \u9002\u7528\u4e8e\u6982\u62ec\u548c\u6807\u9898, \u6709\u591a\u5c11chunks\u662fmatch\u7684, \u8003\u8651\u4e86\u5f62\u6001\u53d8\u5316         - BERT score: \u4e0d\u518d\u4f7f\u7528ngram, \u4f7f\u7528BERT-based contextual  embedding, \u6bd4\u8f83\u4ed6\u4eec\u7684embedding \u7684cosine similarity, \u6bcf\u4e2a\u8bcd\u90fd\u6709\u4e00\u4e2aemb, \u4e24\u4e24\u8ba1\u7b97, \u7528\u6700\u597d\u7684\u5339\u914d. \u7f3a\u70b9\u5728\u4e8e\u4f9d\u8d56\u4e8ebert, \u4e0d\u540c\u7684model\u7ed9\u51fa\u4e0d\u540c\u7ed3\u679c     - Inference       collapsed:: true         - \u7531\u4e8e\u4e0d\u518d\u6709label, \u9700\u8981auto-regressive, \u5c31\u5f97\u8981\u9009\u62e9\u7ed9\u4e0b\u4e00\u4e2ainput\u7684\u8bcd\u662f\u54ea\u4e00\u4e2a         - Greedy decoding: \u76f4\u63a5\u53ea\u7528\u6700\u9ad8\u6982\u7387\u7684, \u53ea\u4fdd\u7559\u4e00\u6761\u7ebf         - Beam search: \u4fdd\u7559k\u6761\u7ebf, \u53d6\u4e58\u8d77\u6765\u603b\u6982\u7387\u6700\u9ad8\u7684\u90a3k\u6761\u7ebf         - Temperature sampling: divide logits by T, A high temperature &gt;1 value increases the randomness of the sampling process by making the less probable words more likely to be selected. Thus more diverse.  controlling the trade-off between creativity and coherence in generated text from NLP models, such as language models. \u7528\u9664\u4ee5T\u6765smooth softmax, \u5f97\u5230\u60f3\u8981\u7684flat\u7684\u6982\u7387\u5206\u5e03, \u7528\u8fd9\u4e2a\u65b0\u7684flat\u7684\u6982\u7387\u5206\u5e03sample\u8bcd, \u800c\u4e0d\u662f\u53d6\u6982\u7387\u6700\u9ad8\u7684\u90a3\u51e0\u4e2a. \u8fd9\u91cc\u662f\u91c7\u6837!\u4e0d\u662f\u53d6\u6700\u5927!     - Data Augmentation       collapsed:: true         - Backtranslation         - Synonym replacement         - Group similar length sentences together in the batch            Train your model on simpler/smaller sequence lengths first             - \u7591\u70b9         - Transformers           collapsed:: true             - \u96be\u70b9               collapsed:: true - Encoder, decoder and EN-DE   collapsed:: true     - Encoder: \u6ce8\u91cd\u7f16\u7801\u63d0\u53d6\u4e0a\u4e0b\u6587\u7279\u5f81\u7528\u4e8e\u4e0b\u6e38, \u6216\u9884\u6d4b\u4e2d\u95f4\u8bcd: classification, MLM, NER; BERT, RoBERTa     - Decoder: \u6ce8\u91cd\u76f4\u63a5\u7528\u5df2\u6709\u8f93\u5165\u548c\u6a21\u578b\u751f\u6210\u9884\u6d4b\u540e\u9762\u7684\u8bcd: LM, text gen; GPT     - EN-DE: seq2seq\u573a\u666f, \u9700\u8981\u5bf9source\u8fdb\u884c\u7279\u6b8a\u7406\u89e3\u7684\u573a\u666f: translation, summarisation, Q&amp;A; Transformer, BART - \u7ed3\u6784:   collapsed:: true     -  - Self-attention (scaled dot product attention)   collapsed:: true     - \u6bcf\u4e2a\u8bcd\u90fd\u548c\u5176\u4ed6\u8bcdattention, \u627e\u5230\u5bf9\u5e94attention\u5206\u6570, \u6839\u636e\u8fd9\u4e2a\u5206\u6570\u6765weight\u6bcf\u4e2a\u8bcd\u8bed\u5bf9\u5e94\u7684V\u7684\u503c, \u6c42\u52a0\u6743\u5e73\u5747\u5f97\u5230\u8fd9\u4e2a\u8bcd\u6700\u7ec8\u7684value     - QKV\u4e09\u4e2a\u65b0\u4e1c\u897f, \u7531\u4e09\u4e2aW project\u8bcd\u7684embedding\u5f97\u5230, \u4eceembedding dim D(\u4e5f\u901a\u5e38\u662fmodel dim, n\u4e2aheads concat\u8d77\u6765\u7684\u603b\u957f\u5ea6) map\u5230 \u5355\u4e2ahead\u7684dim d_h     - \u6bcf\u5bf9Q\u548cK\u7684\u70b9\u79ef\u7ed3\u679c\u4e3a\u6807\u91cf, \u8868\u793a\u76f8\u4f3c\u5ea6, \u4e00\u5171\u6709SxS\u4e2a, \u7528\u6839\u53f7d_h normalise\u540e\u7684\u7ed3\u679c\u662f\u6ce8\u610f\u529b\u5206\u6570, \u5bf9\u6bcf\u884c\u505asoftmax\u4ee5\u540e\u5c31\u662fweight, \u7ef4\u5ea6\u4e0d\u53d8. \u53e5\u4e2d\u8bcd\u7684V\u7684\u52a0\u6743\u5e73\u5747\u5c31\u662f\u505aSA\u7684\u8bcd\u7684value,  \u6700\u540e\u662fS x d_h = Z, S\u4e2a\u8bcd\u7684values     -  - Multi-head attention   collapsed:: true     - Intuitively multiple attention heads allows for attending to parts of the sequence differently (e.g. some heads are responsible for longer-term dependencies, others for shorter-term dependencies)     - \u8f93\u5165\u8fdb\u6765\u6bcf\u4e2a\u8bcd\u7684embedding\u662fD, h\u4e2aheads, h\u7ec4QKV projection matrices\u628a\u4ed6\u4eec\u53d8\u6210\u4e86d_h, head dim = d_h = model dim(D) / h, \u7528\u6765\u505aSA. \u6bcf\u4e2a\u8bcd\u7684h\u4e2a\u5934\u7684d_h\u4f1aconcat\u8d77\u6765\u91cd\u65b0\u5f97\u5230D\u4f5c\u4e3a\u8fd9\u4e2a\u8bcd\u7684encoding, \u7ecf\u8fc7\u4e00\u4e2aFC\u8fd8\u662fD, \u4fdd\u8bc1\u4e86D\u4e0d\u53d8, \u53ef\u4ee5\u5728addnorm\u4ee5\u540e\u8fdb\u884c\u518d\u4e00\u6b21\u7684MHA\u4e86 - Layer normalisation (NORM)   collapsed:: true     - \u5bf9\u6bcf\u53e5\u8bdd\u7684\u6bcf\u4e2a\u8bcd\u7684\u6574\u6761\u7279\u5f81\u5411\u91cf\u8fdb\u884cnorm, \u5373\u5bf9\u6bcf\u4e2a\u8bcd\u7684D\u7ef4\u5411\u91cf\u8fdb\u884cnorm     - BN\u7684\u8bdd\u5219\u662f\u5bf9\u6240\u6709\u53e5\u5b50\u4e2d\u76f8\u540c\u8bcd\u8bed\u4f4d\u7f6e\u7684\u8bcd\u8bed\u7684\u76f8\u540c\u7ef4\u5ea6\u4f4d\u7f6e\u7684\u6570\u5b57\u4eec\u8fdb\u884cnorm     - Why gamma and beta have d dims?       collapsed:: true         - they allow for different scaling and shifting of each feature dimension. In NLP tasks, different feature dimensions can have different magnitudes and ranges, which can make it difficult to normalize them effectively with a single scalar factor. - Residual Connection (ADD)   collapsed:: true     - allows the current layer to focus on learning the difference between the two outputs, rather than learning an entirely new transformation - Position Wise Feedforward Network   collapsed:: true     - MLP, \u540c\u6837\u7684weight\u4f1a\u7528\u4e8e\u6bcf\u4e00\u4e2a\u53e5\u5b50\u4e2d\u7684\u8bcd\u8bed - Positional Encodings   collapsed:: true     - Transformers are position invariant by default, \u5bf9\u4e8e\u4e0d\u540c\u8bcd\u8bed\u7684\u987a\u5e8f\u6ca1\u6709\u611f\u77e5, not inherently sequential, \u9700\u8981positional encoding \u6765inject position information into embeddings, \u6211\u4eec\u76f4\u63a5\u5728\u8bcdembedding\u4e0a\u52a0PE. dim\u8f6e\u6d41\u7528sin cos, \u503c\u7684\u5927\u5c0f\u53d7pos\u5f71\u54cd     - Intuitively, front words are 010101; back words effected with diff patterns     -  - Decoder   collapsed:: true     - Masked multi-head self-attention       collapsed:: true         - \u9884\u6d4b\u65f6, encoder encode\u4e86source, \u4f5c\u4e3adecoder\u7684\u53c2\u8003, \u6211\u4eec\u7ed9decoder\u4e00\u4e2aSOS, \u6bcf\u6b21\u9884\u6d4b\u7684\u65b0\u8bcd, \u548c\u524d\u9762\u7684\u8bcd\u62fc\u8d77\u6765\u4f5c\u4e3a\u65b0\u7684\u8f93\u5165\u6765\u9884\u6d4b\u65b0\u7684\u8bcd auto-regressive \u76f4\u5230\u8fbe\u5230\u9650\u5236\u6216\u662fEOS         - \u8bad\u7ec3\u65f6\u9650\u5236attention\u4e0d\u770b\u5230\u672a\u6765\u7684\u8bcd, \u6211\u4eecfeed\u662f\u6574\u4e2asequence, \u4f46\u6bcf\u4e00\u4e2a\u65f6\u95f4\u70b9\u90fd\u53ea\u80fd\u770b\u5230\u73b0\u5728\u548c\u8fc7\u53bb\u7684, transformers\u7684\u8bad\u7ec3\u662f\u767e\u5206\u767e\u7684teacher forcing. mask set values in the upper triangular to be -inf, \u6765\u8ba9softmax\u4e0d\u5173\u6ce8     - cross attention       collapsed:: true         - \u4e3a\u4e86\u8ba9decoder\u6bcf\u4e00\u6b65\u90fd\u9700\u8981\u77e5\u9053\u8981\u5173\u6ce8\u54ea\u4e9bencoded tokens, perform attention to all encoded tokens in the last layer of the encoder, using Q from current decoder layer, and K, V from encoder. \u5373\u5728\u641e\u6e05\u695atarget sequence\u91cc\u9762\u81ea\u5df1\u7684\u5173\u7cfb\u4ee5\u540e, \u518d\u548csource \u63a2\u7d22\u4e00\u4e0b\u5173\u7cfb, \u83b7\u5f97\u65b0\u7684values \u8fd9\u91cc\u7684cross attention matrix T x S.     -             - \u7591\u70b9         - ---         - Pre-training models           collapsed:: true             - \u96be\u70b9               collapsed:: true - \u5168\u5c40\u7406\u89e3   collapsed:: true     - fine-tune\u662f\u9488\u5bf9\u67d0\u4e2aspecific \u7684task\u7684, fine-tune \u6574\u4e2a\u6a21\u578b\u800c\u975e\u51bb\u7ed3\u5728\u5927\u8bad\u7ec3\u96c6\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d     - Contextual word embedding \u9700\u8981\u628a\u6574\u53e5\u8bdd\u7ed9\u5230\u6a21\u578b\u6765\u5f97\u5230\u4e00\u4e2a\u8bcd\u7684embedding     - zero/one/few shot learning \u662f\u4ee5prompt\u7684\u65b9\u5f0f\u5f15\u5bfc\u6a21\u578b, \u4e0d\u66f4\u65b0\u53c2\u6570 - Tokenisation and word embedding   collapsed:: true     - Byte Pair Encoding &amp; Wordpieces       collapsed:: true         - 1.2\u4e2d\u5168\u90e8\u6709\u63d0\u5230     - Contextual word representations       collapsed:: true         - take the context into account when constructing word representations         - ELMo: Embeddings from Language Models           collapsed:: true             - \u4f7f\u7528\u4e24\u4e2a\u4ece\u5de6\u5230\u53f3 \u4ece\u53f3\u5230\u5de6\u5f97\u5230\u7684hidden state concat\u8d77\u6765; \u4e09\u5c42LSTM\u7684\u4e09\u4e2a\u8f93\u51fa             - - pre-training encoder models   collapsed:: true     - Masked Language Modeling       collapsed:: true         - \u7528[MASK] \u7b26\u53f7\u4ee3\u8868\u6765\u906e\u853d\u6389\u90e8\u5206\u8bcd\u6c47, \u8ba9BERT\u751f\u6210\u6574\u53e5\u8bdd\u7684encodings, \u53d6\u51fa\u5bf9\u5e94mask\u4f4d\u7f6e\u7684encoding \u7528mlp\u8fdb\u884cV\u4e2d\u5206\u7c7b, \u4e0e\u771f\u5b9elabel\u6c42loss BP         - 80% [MASK], 10% random, 10% original         - Random: \u53ea\u6709mask\u4f1a\u8ba9\u6a21\u578b\u53ea\u5b66\u4e60mask\u7684\u90e8\u5206, \u4e0d\u5b66\u4e60\u90a3\u4e9b\u4e0dmask\u7684\u4e0d\u6b63\u786e\u7684\u90e8\u5206         - Original: \u9632\u6b62random\u7684\u4e0d\u6b63\u786e\u7684\u6837\u4f8b\u8ba9\u6a21\u578b\u8ba4\u4e3a\u6ca1\u6709mask\u7684\u5c31\u662f\u4e0d\u6b63\u786e\u7684, \u8ba9\u6a21\u578b\u4e5f\u80fd\u5904\u7406\u90a3\u4e9b\u6b63\u786e\u7684\u8bcd\u8bed     - Another approach: predict next sentence (label yes or no) \u6ca1\u7528     - \u4f7f\u7528\u65b9\u6cd5:       collapsed:: true         - BERT\u4f1a\u5728tokenise\u7684\u65f6\u5019\u52a0\u4e00\u4e2aCLS \u4e13\u95e8\u7528\u6765\u8868\u793a\u7c7b\u522b, SEP\u6765\u5206\u5f00\u53e5\u5b50\u6765\u5904\u7406\u591a\u53e5, \u53ef\u4ee5\u5728\u6bcf\u4e2atoken\u540e\u63a5out layer, QA\u4e5f\u53ef\u4ee5\u5b9e\u73b0by label in candidate answer span     - Some ideas: mask a span, distil model with similar performance, sparse attention     - Parameter-efficient fine-tuning \u5982\u4f55\u6709\u6548\u8bad\u7ec3\u5e76\u5f97\u5230\u6700\u5e7f\u6cdb\u7684\u5e94\u7528       collapsed:: true         - Prompt tuning:  include task specific prompt and fine-tune only their embeddings for that particular task, froze remaining         - Prefix tuning: include these trainable task-specific \u201ctokens\u201d into all layers of the transformer.         - Adapters: insert specific trainable modules and freeze rest         - BitFit: Keep most of the model parameters frozen, fine-tune only the biases.     - Keys to good performance       collapsed:: true         - model pre-training, large models, Loads of data, Fast computation, A difficult learning task - Pre-training encoder-decoder models   collapsed:: true     - \u65e0\u6cd5\u4f7f\u7528MLM, \u56e0\u4e3aoutput\u7684\u662f\u4e00\u6574\u4e2asequence, \u800c\u4e0d\u662f\u4e00\u5bf9\u4e00\u7684\u8f93\u51fa, \u6ca1\u529e\u6cd5\u5bf9\u5e94\u4f4d\u7f6e\u53bb\u5206\u7c7b.     - 3\u79cd\u8bad\u7ec3\u65b9\u5f0f:       collapsed:: true         - Prefix language modeling         - Sentence permutation deshuffling         - BERT-style token masking     - Corrupt original sentences in ways and optimise the model to recover them. \u6bd4\u5982span mask\u6389, \u9884\u6d4b\u51faspan\u662f\u4ec0\u4e48     - Instructional training: train with natural language instructions as inputs, and annotated answers as target outputs. - Pre-training decoder models   collapsed:: true     - train on unlabeled text, optimising p(xn|h)     - Fine-tuning: Supervised training for particular input-output pairs.     - No gradient update       collapsed:: true         - Zero-shot: \u7ed9\u51fa\u81ea\u7136\u8bed\u8a00\u6307\u4ee4, \u662fLLM\u7684\u4f18\u52bf         - One-shot: \u7ed9\u51fa\u6307\u4ee4\u5e76\u7ed9\u51fa\u4e00\u4e2a\u4f8b\u5b50         - Few-shot: \u7ed9\u51fa\u6307\u4ee4\u5e76\u7ed9\u51fa\u5f88\u591a\u4f8b\u5b50, \u7ed9\u4f8b\u5b50\u80fd\u591f\u6307\u5bfc\u5176\u56de\u7b54\u9700\u8981\u7684\u7b54\u6848     - Chain-of-thought       collapsed:: true         - Show examples of reasoning and do reasoning (lets think step by step \u4f5c\u4e3aprompt\u6765\u5f15\u51faCOT)     - Retrieval-based language models: \u4ece\u6570\u636e\u5e93\u4e2d\u83b7\u53d6factual knowledge     - Limitations of instruction fine-tuning       collapsed:: true         - data is expensive as manually created         - creative generation have no right answer         - penalise token-level mistakes equally, but some are worse     - [[RLHF]]: Reinforcement learning from human feedback       collapsed:: true         - train our language model to maximize this a human reward, using reinforcement learning         - to reduce expensive direct human participants, model human preferences as a separate (NLP) problem, ask for pairwise comparisons         - InstructGPT and ChatGPT \u5c31\u7528\u4e86\u8fd9\u79cd\u65b9\u5f0f, \u975e\u5e38\u6d41\u884c \u4e14\u6548\u679c\u5f88\u597d             - \u7591\u70b9         - ---         - Structured Prediction           collapsed:: true             - \u96be\u70b9             - \u7591\u70b9         - Tagging           collapsed:: true             - \u96be\u70b9             - \u7591\u70b9         - Parsing           collapsed:: true             - \u96be\u70b9             - \u7591\u70b9 - ## Coursework   collapsed:: true     - Deadline: 3.7       SCHEDULED: &lt;2023-03-07 Tue&gt;     - 3.4         - 1. \u76f4\u63a5\u6d4b\u4e00\u4e2auncased and cased           2. \u7ed9\u6700\u4f73\u7ec4\u5408\u627e\u5230\u6700\u4f73\u7684learning rate\u548cbatch size (\u4f7f\u7528train val, \u518d\u5728test\u4e0a\u9762\u5f97\u5230\u7ed3\u679c           3. \u6d4b\u8bd5learning rate scheduler           4. \u6d4b\u8bd5\u4e0d\u540c\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7684\u533a\u522b           5. \u6d4b\u8bd5bagging \u548censemble           6. length of the input sequence           7. To what extent does model performance depend on the data categories? E.g. Observations for homeless vs poor-families     - Task:         - \u5b9e\u73b0\u4e00\u4e2a\u7528\u4e8e\u5224\u65ad\u6587\u672c\u6709\u6ca1\u6709\u5c48\u5c0a\u4f18\u8d8a\u9ad8\u4eba\u4e00\u7b49(patronising or condescending) \u7684\u542b\u4e49\u7684\u4e8c\u5206\u7c7b\u6a21\u578b             - transformer-based model             - F1 score: over 0.48 on dev set; over 0.49 on test set             - dev set evaluation will be a public test on LabTS             - test set \u7684label is private, \u6253\u5206\u7684\u65f6\u5019\u624d\u80fd\u770b         - Report, \u7528\u4e8e\u6253\u5206, pdf             - answer the questions in the Marking Scheme section     - Submission:         - PDF of your report         - SHA1 key for your GitLab repository             - \u2013 Dev set predictions as dev.txt               \u2013 Test set predictions as test.txt             - also contain the code, but not be marked     - Data and evaluation:         - use the dontpatronizeme_pcl.tsv         - practice split\u4e2d\u5206\u597d\u4e86train\u548cdev, dev\u6211\u4eec\u4f5c\u4e3atest\u7528     - Marking scheme         - 1) Data analysis of the training data (15 marks): a written description of the training data         - 2) Modelling (40 marks): implementation of a transformer model         - 3) Analysis (15 marks):Analysis questions to be answered         - 4) Written report (30 marks): awarded for the quality of your written report     - Ideas:         - data augmentation         - \u5e73\u8861\u6570\u636e         - prompting         - {0,1}   = No PCL         - {2,3,4} = PCL     - Transformer:     - Links:         - RoBERTa #[[Hugging Face]]         - LUKE#[[Hugging Face]]         - DeBERTa-v2#[[Hugging Face]]         - An Algorithm for Routing Vectors in Sequences | Papers With Code         - ALBERT#[[Hugging Face]]         - Models - Hugging Face#[[Hugging Face]]         - Processing the data - Hugging Face Course#[[Hugging Face]]         - Hugging Face \u7684 Transformers \u5e93\u5feb\u901f\u5165\u95e8\uff08\u4e00\uff09\uff1a\u5f00\u7bb1\u5373\u7528\u7684 pipelines - \u5c0f\u6607\u7684\u535a\u5ba2         - Transformer \u6a21\u578b\u901a\u4fd7\u5c0f\u4f20 - \u5c0f\u6607\u7684\u535a\u5ba2         - Hugging Face \u7684 Transformers \u5e93\u5feb\u901f\u5165\u95e8\uff08\u56db\uff09\uff1a\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b - \u5c0f\u6607\u7684\u535a\u5ba2         - - ## Info   collapsed:: true     - 7:3     - \u661f\u671f\u4e00 16:00 - 18:00, \u661f\u671f\u56db 11:00 - 13:00     - 3 hours lectures + 1 hour tut/lab     - CW: 1.30 - 3.7 - ## Syllabus   collapsed:: true     -          - - ## Links     - Scientia     - Ed \u2014 Digital Learning Platform     - textbook #\u4e66     - COMP70016: Natural Language Processing | Department of Computing | Imperial College London     - ImperialNLP/NLPLabs-2023 \u00b7 GitHub     - Tensors \u2014 PyTorch Tutorials 1.13.1+cu117 documentation #tutorial     - NLP-progress #GitHub #\u79d1\u6280\u8d44\u6e90 #Academic     - Browse the State-of-the-Art in Machine Learning | Papers With Code #\u79d1\u6280\u8d44\u6e90 #Academic     - SpaCy #NLP #Academic #\u79d1\u6280\u8d44\u6e90     - Stanza #NLP #GitHub #Academic #\u79d1\u6280\u8d44\u6e90     - huggingface     - exBERT     - GitHub - bhoov/exbert: A Visual Analysis Tool to Explore Learned Representations in Transformers Models"},{"location":"nlp/Natural%20Language%20Processing/#_2","title":"Natural Language Processing","text":"<pre><code>- Transformers\n  collapsed:: true\n</code></pre>"},{"location":"nlp/Natural%20Language%20Processing/#_3","title":"Natural Language Processing","text":"<pre><code>- [[Layer Normalisation]] #card\n  id:: 64419f8e-794a-4de5-a628-4345a46dfaa4\n</code></pre>"},{"location":"nlp/Natural%20Language%20Processing/#_4","title":"Natural Language Processing","text":"<ul> <li>Other tricks   collapsed:: true</li> </ul>"},{"location":"nlp/Natural%20Language%20Processing/#-bert-predictlabel","title":"- BERT\u7684\u7b2c\u4e8c\u4e2a\u8bad\u7ec3\u76ee\u6807, \u5224\u65ad\u4e24\u53e5\u8bdd\u662f\u4e0d\u662f\u6309\u987a\u5e8f\u51fa\u73b0\u5728\u4e00\u4e2a\u6e90\u6587\u672c\u7684, predict\u7684\u662f\u4e00\u4e2alabel \u4f46\u662f\u8fd9\u4e2a\u88ab\u8bc1\u660e\u6ca1\u5565\u7528","text":""}]}